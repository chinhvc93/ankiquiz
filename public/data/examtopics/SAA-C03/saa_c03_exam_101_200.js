var SAA_C03_Exam_101_200 = {
  "msg": "Quiz Questions",
  "data": [
    {
      "question_id": "#101",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A solutions architect is designing a VPC with public and private subnets. The VPC and subnets use IPv4 CIDR blocks. There is one public subnet and one private subnet in each of three Availability Zones (AZs) for high availability. An internet gateway is used to provide internet access for the public subnets. The private subnets require access to the internet to allow Amazon EC2 instances to download software updates.<br>What should the solutions architect do to enable Internet access for the private subnets?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#101",
          "answers": [
            {
              "choice": "<p>A. Create three NAT gateways, one for each public subnet in each AZ. Create a private route table for each AZ that forwards non-VPC traffic to the NAT gateway in its AZ.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create three NAT instances, one for each private subnet in each AZ. Create a private route table for each AZ that forwards non-VPC traffic to the NAT instance in its AZ.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a second internet gateway on one of the private subnets. Update the route table for the private subnets that forward non-VPC traffic to the private internet gateway.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an egress-only internet gateway on one of the public subnets. Update the route table for the private subnets that forward non-VPC traffic to the egress-only Internet gateway.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 713622,
          "date": "Tue 08 Nov 2022 10:10",
          "username": "\t\t\t\tGil80\t\t\t",
          "content": "NAT Instances - OUTDATED BUT CAN STILL APPEAR IN THE EXAM!<br>However, given that A provides the newer option of NAT Gateway, then A is the correct answer.<br><br>B would be correct if NAT Gateway wasn't an option.",
          "upvote_count": "7",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 850275,
          "date": "Sat 25 Mar 2023 17:13",
          "username": "\t\t\t\tRudraman\t\t\t",
          "content": "NAT Gateway - AWS-managed NAT, higher bandwidth, high availability, no administration",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 826210,
          "date": "Wed 01 Mar 2023 20:17",
          "username": "\t\t\t\tRODCCN\t\t\t",
          "content": "You should create 3 NAT gateways, but not in the public subnet. So, even NAT instance is already deprecated, is the right answer in this case, since it's relate to create in a private subnet, not public.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 822022,
          "date": "Sun 26 Feb 2023 03:48",
          "username": "\t\t\t\tBen2008\t\t\t",
          "content": "Refer: <br>https://docs.aws.amazon.com/vpc/latest/userguide/nat-gateway-scenarios.html#public-nat-gateway-overview<br>Should be A. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 764456,
          "date": "Tue 03 Jan 2023 11:47",
          "username": "\t\t\t\terik29\t\t\t",
          "content": "aaaaaa",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 759210,
          "date": "Wed 28 Dec 2022 01:42",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "Networking 101, A is only right option",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 752728,
          "date": "Wed 21 Dec 2022 21:22",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The correct answer is option A. <br><br>To enable Internet access for the private subnets, the solutions architect should create three NAT gateways, one for each public subnet in each Availability Zone (AZ). NAT gateways allow private instances to initiate outbound traffic to the Internet but do not allow inbound traffic from the Internet to reach the private instances.<br><br>The solutions architect should then create a private route table for each AZ that forwards non-VPC traffic to the NAT gateway in its AZ. This will allow instances in the private subnets to access the Internet through the NAT gateways in the public subnets.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 749216,
          "date": "Sun 18 Dec 2022 22:53",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option A<br>NAT gateway needs to be configured within each VPC's in Public Subnet.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 713048,
          "date": "Mon 07 Nov 2022 14:02",
          "username": "\t\t\t\tDeplake\t\t\t",
          "content": "Should be B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 712908,
          "date": "Mon 07 Nov 2022 09:06",
          "username": "\t\t\t\tNigma\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/35679-exam-aws-certified-solutions-architect-associate-saa-c02/",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 703383,
          "date": "Mon 24 Oct 2022 23:57",
          "username": "\t\t\t\tdave9994\t\t\t",
          "content": "B should be the answer. https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Sir, you didn't even read the link you posted !! There it is clearly stated that when you need access to Internet from a private subnet you place the NAT gateway in a PUBLIC subnet.</li><li>B is NAT Instances, which is outdated. The link you provided refers to NAT Gateways (the newer approach) - which means, A is the right answer.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 724713,
          "date": "Tue 22 Nov 2022 22:55",
          "username": "\t\t\t\tyalvar\t\t\t",
          "content": "Sir, you didn't even read the link you posted !! There it is clearly stated that when you need access to Internet from a private subnet you place the NAT gateway in a PUBLIC subnet.",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 713623,
          "date": "Tue 08 Nov 2022 10:12",
          "username": "\t\t\t\tGil80\t\t\t",
          "content": "B is NAT Instances, which is outdated. The link you provided refers to NAT Gateways (the newer approach) - which means, A is the right answer.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 699820,
          "date": "Thu 20 Oct 2022 12:56",
          "username": "\t\t\t\tEvangelia\t\t\t",
          "content": "aaaaaaa",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        }
      ]
    },
    {
      "question_id": "#102",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company wants to migrate an on-premises data center to AWS. The data center hosts an SFTP server that stores its data on an NFS-based file system. The server holds 200 GB of data that needs to be transferred. The server must be hosted on an Amazon EC2 instance that uses an Amazon Elastic File System (Amazon EFS) file system.<br>Which combination of steps should a solutions architect take to automate this task? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: AB</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#102",
          "answers": [
            {
              "choice": "<p>A. Launch the EC2 instance into the same Availability Zone as the EFS file system.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Install an AWS DataSync agent in the on-premises data center.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a secondary Amazon Elastic Block Store (Amazon EBS) volume on the EC2 instance for the data.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Manually use an operating system copy command to push the data to the EC2 instance.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Use AWS DataSync to create a suitable location configuration for the on-premises SFTP server.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 698297,
          "date": "Tue 18 Oct 2022 16:36",
          "username": "\t\t\t\t123jhl0\t\t\t",
          "content": "**A**. Launch the EC2 instance into the same Availability Zone as the EFS file system.<br>Makes sense to have the instance in the same AZ the EFS storage is.<br>**B**. Install an AWS DataSync agent in the on-premises data center.<br>The DataSync with move the data to the EFS, which already uses the EC2 instance (see the info provided). No more things are required...C.  Create a secondary Amazon Elastic Block Store (Amazon EBS) volume on the EC2 instance for the data.<br>This secondary EBS volume isn't required... the data should be move on to EFS...D.  Manually use an operating system copy command to push the data to the EC2 instance.<br>Potentially possible (instead of A), BUT the \\\"automate this task\\\" premise goes against any \\\"manually\\\" action. So, we should keep A. E.  Use AWS DataSync to create a suitable location configuration for the on-premises SFTP server.<br>I don't get the relationship between DataSync and the configuration for SFTP \\\"on-prem\\\"! Nonsense.<br>So, anwers are A&B<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>CORRECT ANSWER: B&amp;E<br>Steps4 &amp;5<br>https://aws.amazon.com/datasync/getting-started/?nc1=h_ls</li><li>will A,B work without E?</li><li>Can someone explain why A is correct?<br>EFS is spread across Availability Zones in a region, as per https://aws.amazon.com/blogs/gametech/gearbox-entertainment-goes-remote-with-aws-and-perforce/<br>My question then is whether it makes sense to launch EC2 instances in the *same Availability Zone as the EFS file system* ?</li><li>However, launching the EC2 instance in the same AZ as the EFS file system can provide some performance benefits, such as reduced network latency and improved throughput. Therefore, it may be a best practice to launch the EC2 instance in the same AZ as the EFS file system if performance is a concern.</li><li>Yes exactly, that's why A doesn't make sense. I voted for B and E. </li><li>E is correct<br>https://aws.amazon.com/blogs/storage/migrating-storage-with-aws-datasync/</li></ul>",
          "upvote_count": "27",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 812386,
          "date": "Fri 17 Feb 2023 22:14",
          "username": "\t\t\t\tLalo\t\t\t",
          "content": "CORRECT ANSWER: B&E<br>Steps4 &5<br>https://aws.amazon.com/datasync/getting-started/?nc1=h_ls",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 743871,
          "date": "Tue 13 Dec 2022 11:12",
          "username": "\t\t\t\tRBSK\t\t\t",
          "content": "will A,B work without E?",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 724149,
          "date": "Tue 22 Nov 2022 08:29",
          "username": "\t\t\t\tattila9778\t\t\t",
          "content": "Can someone explain why A is correct?<br>EFS is spread across Availability Zones in a region, as per https://aws.amazon.com/blogs/gametech/gearbox-entertainment-goes-remote-with-aws-and-perforce/<br>My question then is whether it makes sense to launch EC2 instances in the *same Availability Zone as the EFS file system* ?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>However, launching the EC2 instance in the same AZ as the EFS file system can provide some performance benefits, such as reduced network latency and improved throughput. Therefore, it may be a best practice to launch the EC2 instance in the same AZ as the EFS file system if performance is a concern.</li><li>Yes exactly, that's why A doesn't make sense. I voted for B and E. </li></ul>",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 846800,
          "date": "Wed 22 Mar 2023 09:24",
          "username": "\t\t\t\tlovelazur\t\t\t",
          "content": "However, launching the EC2 instance in the same AZ as the EFS file system can provide some performance benefits, such as reduced network latency and improved throughput. Therefore, it may be a best practice to launch the EC2 instance in the same AZ as the EFS file system if performance is a concern.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 782203,
          "date": "Fri 20 Jan 2023 13:15",
          "username": "\t\t\t\tBlueVolcano1\t\t\t",
          "content": "Yes exactly, that's why A doesn't make sense. I voted for B and E. ",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 720169,
          "date": "Thu 17 Nov 2022 04:28",
          "username": "\t\t\t\tCizzla7049\t\t\t",
          "content": "E is correct<br>https://aws.amazon.com/blogs/storage/migrating-storage-with-aws-datasync/",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 838057,
          "date": "Mon 13 Mar 2023 17:09",
          "username": "\t\t\t\tCapJackSparrow\t\t\t",
          "content": "\\\"location\\\" keyword. No EC2 needed.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: BE"
        },
        {
          "id": 838056,
          "date": "Mon 13 Mar 2023 17:07",
          "username": "\t\t\t\tCapJackSparrow\t\t\t",
          "content": "Every DataSync \\\"job\\\" has TWO \\\"locations\\\" FROM and WHERE TO. EC2 is not needed for the Datasync \\\"job\\\" just an agent on prem and a \\\"location\\\" to where the data is going. So I'm inclined to go with B and E. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 817195,
          "date": "Tue 21 Feb 2023 22:35",
          "username": "\t\t\t\tbdp123\t\t\t",
          "content": "Use AWS Transfer family for SFTP<br>https://aws.amazon.com/datasync/faqs/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 814179,
          "date": "Sun 19 Feb 2023 15:41",
          "username": "\t\t\t\tvherman\t\t\t",
          "content": "BE is correct. <br>I did not select A because EC2 instance is not necessary to have in order to automate data transfer",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: BE"
        },
        {
          "id": 812465,
          "date": "Fri 17 Feb 2023 23:48",
          "username": "\t\t\t\tK0nAn\t\t\t",
          "content": "Since EFS will be used in all AZ zones ,so A does not make sense ,BE makes sense for me",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BE"
        },
        {
          "id": 801355,
          "date": "Tue 07 Feb 2023 20:49",
          "username": "\t\t\t\tbdp123\t\t\t",
          "content": "A and B isall that is needed",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 798783,
          "date": "Sun 05 Feb 2023 12:38",
          "username": "\t\t\t\tCaoMengde09\t\t\t",
          "content": "I need to pay attention more to the ambiguous wording in those kind of questions.<br><br>You cannot transfer data from OnPrem to AWS without installing the AWS DataSync Agent inside the OnPrem server. The AWS DataSync Agent acts like a VM that cache and send data to AWS (in this case the EFS store). Without the AWS DataSync Agent just forget of any data transfer. So E.  Answer for me is just a distractor. B.  is the right one. And since C&D are ruled out A answer is the optimal architecture for data Availability.<br><br>A&B Are the correct answers",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 791676,
          "date": "Sun 29 Jan 2023 14:44",
          "username": "\t\t\t\tVickysss\t\t\t",
          "content": "This can be a bit confusing but i believe the ab choice is correct. The company need to migrate a workload on AWS. The workload consists in having computation and storage power in the cloud (which lead you to choice A). Also, the company needs to migrate the existing data part into EFS (using DataSync). Which such a combination (without off course considering the technicality in details) the company will be able to run the workload on AWS.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 791333,
          "date": "Sun 29 Jan 2023 03:43",
          "username": "\t\t\t\tChiggaBoy\t\t\t",
          "content": "B and E, A might not work with E",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BE"
        },
        {
          "id": 788422,
          "date": "Thu 26 Jan 2023 06:48",
          "username": "\t\t\t\tkdinesh95\t\t\t",
          "content": "AB dudes",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 783817,
          "date": "Sun 22 Jan 2023 00:28",
          "username": "\t\t\t\tbullrem\t\t\t",
          "content": "Option E, using AWS DataSync to create a suitable location configuration for the on-premises SFTP server, is not a correct solution because DataSync is used to transfer data between on-premises locations and other cloud storage, it's not designed to work with SFTP servers.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>DataSync allows you to configure a source storage location (NFS or SMB share) on-premises, and a destination in AWS storage services (Amazon S3 or Amazon EFS). It uses a purpose-built network protocol and scale-out architecture to accelerate the transfer of data to AWS</li></ul>",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 796593,
          "date": "Fri 03 Feb 2023 02:20",
          "username": "\t\t\t\tAjithKumar3\t\t\t",
          "content": "DataSync allows you to configure a source storage location (NFS or SMB share) on-premises, and a destination in AWS storage services (Amazon S3 or Amazon EFS). It uses a purpose-built network protocol and scale-out architecture to accelerate the transfer of data to AWS",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 782210,
          "date": "Fri 20 Jan 2023 13:28",
          "username": "\t\t\t\tBlueVolcano1\t\t\t",
          "content": "It's B and E.  Set up DataSync and the configure it to use the on-premise SMB server as a location: <br>https://docs.aws.amazon.com/datasync/latest/userguide/create-smb-location.html<br><br>A doesn't make sense as EFS is Multi-AZ.<br>From the docs: \\\"Amazon EC2 and other AWS compute instances running in multiple Availability Zones within the same AWS Region can access the file system, so that many users can access and share a common data source.\\\"<br>https://docs.aws.amazon.com/efs/latest/ug/how-it-works.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>How you insure the transfer without installing the AWS DataSync. All those configurations are bound to fail without the AGENT.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BE"
        },
        {
          "id": 798784,
          "date": "Sun 05 Feb 2023 12:40",
          "username": "\t\t\t\tCaoMengde09\t\t\t",
          "content": "How you insure the transfer without installing the AWS DataSync. All those configurations are bound to fail without the AGENT.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 778363,
          "date": "Tue 17 Jan 2023 00:10",
          "username": "\t\t\t\tremand\t\t\t",
          "content": "AB are the answers.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 777039,
          "date": "Sun 15 Jan 2023 21:27",
          "username": "\t\t\t\tEllo2023\t\t\t",
          "content": "A and B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 775106,
          "date": "Sat 14 Jan 2023 06:29",
          "username": "\t\t\t\tgoodmail\t\t\t",
          "content": "If B is right, how DataSync agent works without the step mentioned in E?",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BE"
        },
        {
          "id": 774550,
          "date": "Fri 13 Jan 2023 14:58",
          "username": "\t\t\t\tMyxa\t\t\t",
          "content": "B.  Install an AWS DataSync agent in the on-premises data center.E.  Use AWS DataSync to create a suitable location configuration for the on-premises SFTP server.<br><br>A is not necessary for automating the data transfer process.<br>B & E are related to the automation.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: BE"
        }
      ]
    },
    {
      "question_id": "#103",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has an AWS Glue extract, transform, and load (ETL) job that runs every day at the same time. The job processes XML data that is in an Amazon S3 bucket. New data is added to the S3 bucket every day. A solutions architect notices that AWS Glue is processing all the data during each run.<br>What should the solutions architect do to prevent AWS Glue from reprocessing old data?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#103",
          "answers": [
            {
              "choice": "<p>A. Edit the job to use job bookmarks.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Edit the job to delete data after the data is processed.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Edit the job by setting the NumberOfWorkers field to 1.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use a FindMatches machine learning (ML) transform.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 698300,
          "date": "Tue 18 Oct 2022 16:39",
          "username": "\t\t\t\t123jhl0\t\t\t",
          "content": "This is the purpose of bookmarks: \\\"AWS Glue tracks data that has already been processed during a previous run of an ETL job by persisting state information from the job run. This persisted state information is called a job bookmark. Job bookmarks help AWS Glue maintain state information and prevent the reprocessing of old data.\\\"<br>https://docs.aws.amazon.com/glue/latest/dg/monitor-continuations.html",
          "upvote_count": "24",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 763671,
          "date": "Mon 02 Jan 2023 12:28",
          "username": "\t\t\t\tgustavtd\t\t\t",
          "content": "Delete files in S3 freely is not good. so B is not correct,",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 759221,
          "date": "Wed 28 Dec 2022 01:55",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 759147,
          "date": "Wed 28 Dec 2022 00:13",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A.  Edit the job to use job bookmarks.<br><br>Job bookmarks in AWS Glue allow the ETL job to track the data that has been processed and to skip data that has already been processed. This can prevent AWS Glue from reprocessing old data and can improve the performance of the ETL job by only processing new data. To use job bookmarks, the solutions architect can edit the job and set the \\\"Use job bookmark\\\" option to \\\"True\\\". The ETL job will then use the job bookmark to track the data that has been processed and skip data that has already been processed in subsequent runs.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 749224,
          "date": "Sun 18 Dec 2022 22:59",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 743151,
          "date": "Mon 12 Dec 2022 19:10",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "It's obviously A.  Bookmarks serve this purpose",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 723813,
          "date": "Mon 21 Nov 2022 19:06",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "A is correct",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 698034,
          "date": "Tue 18 Oct 2022 10:20",
          "username": "\t\t\t\tLeGloupier\t\t\t",
          "content": "A<br>https://docs.aws.amazon.com/glue/latest/dg/monitor-continuations.html",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        }
      ]
    },
    {
      "question_id": "#104",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A solutions architect must design a highly available infrastructure for a website. The website is powered by Windows web servers that run on Amazon EC2 instances. The solutions architect must implement a solution that can mitigate a large-scale DDoS attack that originates from thousands of IP addresses. Downtime is not acceptable for the website.<br>Which actions should the solutions architect take to protect the website from such an attack? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: AC</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#104",
          "answers": [
            {
              "choice": "<p>A. Use AWS Shield Advanced to stop the DDoS attack.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Configure Amazon GuardDuty to automatically block the attackers.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Configure the website to use Amazon CloudFront for both static and dynamic content.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use an AWS Lambda function to automatically add attacker IP addresses to VPC network ACLs.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Use EC2 Spot Instances in an Auto Scaling group with a target tracking scaling policy that is set to 80% CPU utilization.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 696435,
          "date": "Sun 16 Oct 2022 20:32",
          "username": "\t\t\t\talvarez100\t\t\t",
          "content": "I think it is AC, reason is they require a solution that is highly available. AWS Shield can handle the DDoS attacks. To make the solution HA you can use cloud front. AC seems to be the best answer imo. <br>AB seem like redundant answers. How do those answers make the solution HA?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>A - AWS Shield Advanced<br>C - (protecting this option) IMO: AWS Shield Advanced has to be attached. But it can not be attached directly to EC2 instances. <br>According to the docs: https://aws.amazon.com/shield/ <br>It requires to be attached to services such as CloudFront, Route 53, Global Accelerator, ELB or (in the most direct way using) Elastic IP (attached to the EC2 instance)</li></ul>",
          "upvote_count": "19",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 724169,
          "date": "Tue 22 Nov 2022 09:19",
          "username": "\t\t\t\tattila9778\t\t\t",
          "content": "A - AWS Shield Advanced<br>C - (protecting this option) IMO: AWS Shield Advanced has to be attached. But it can not be attached directly to EC2 instances. <br>According to the docs: https://aws.amazon.com/shield/ <br>It requires to be attached to services such as CloudFront, Route 53, Global Accelerator, ELB or (in the most direct way using) Elastic IP (attached to the EC2 instance)",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 814152,
          "date": "Sun 19 Feb 2023 15:11",
          "username": "\t\t\t\tKhushna\t\t\t",
          "content": "DDos is better with shield and Cloudfront alsoprovide protection for ddos",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 759150,
          "date": "Wed 28 Dec 2022 00:20",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A.  Use AWS Shield Advanced to stop the DDoS attack.<br><br>It provides always-on protection for Amazon EC2 instances, Elastic Load Balancers, and Amazon Route 53 resources. By using AWS Shield Advanced, the solutions architect can help protect the website from large-scale DDoS attacks.<br><br>Option C.  Configure the website to use Amazon CloudFront for both static and dynamic content.<br><br>CloudFront is a content delivery network (CDN) that integrates with other Amazon Web Services products, such as Amazon S3 and Amazon EC2, to deliver content to users with low latency and high data transfer speeds. By using CloudFront, the solutions architect can distribute the website's content across multiple edge locations, which can help absorb the impact of a DDoS attack and reduce the risk of downtime for the website.",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 756111,
          "date": "Mon 26 Dec 2022 01:47",
          "username": "\t\t\t\tCloudForFun\t\t\t",
          "content": "AC<br>\\\"AWS Shield Advanced is available globally on all Amazon CloudFront, AWS Global Accelerator, and Amazon Route 53 edge locations worldwide. You can protect your web applications hosted anywhere in the world by deploying Amazon CloudFront in front of your application. Your origin servers can be Amazon Simple Storage Service (S3), Amazon EC2, Elastic Load Balancing, or a custom server outside of AWS.\\\"<br>https://aws.amazon.com/shield/faqs/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 749233,
          "date": "Sun 18 Dec 2022 23:13",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Aand C as your will need to configure Cloudfront to activate AWS Advance Shield",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 745983,
          "date": "Thu 15 Dec 2022 12:13",
          "username": "\t\t\t\tishitamodi4\t\t\t",
          "content": "AC, AWS Shield Advanced is available globally on all Amazon CloudFront, AWS Global Accelerator, and Amazon Route 53 edge locations worldwide",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 743446,
          "date": "Tue 13 Dec 2022 01:31",
          "username": "\t\t\t\t333666999\t\t\t",
          "content": "c not b. b is wrong because it's not malicious activity, just annoying activity",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 727236,
          "date": "Sat 26 Nov 2022 04:28",
          "username": "\t\t\t\tNewptone\t\t\t",
          "content": "I thought it was AB.  But after I read the docs, I vote for AC. <br><br>Amazon GuardDuty is a threat detection service, it can NOT take action directly, it needs to work with Lambda.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 723811,
          "date": "Mon 21 Nov 2022 19:03",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "A and C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 718844,
          "date": "Tue 15 Nov 2022 15:24",
          "username": "\t\t\t\trjam\t\t\t",
          "content": "AWS Shield can handle the DDoS attacks<br>Amazon CloudFront supports DDoS protection, integration with Shield, AWS Web Application Firewall",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 714283,
          "date": "Wed 09 Nov 2022 05:20",
          "username": "\t\t\t\ttubtab\t\t\t",
          "content": "correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 712326,
          "date": "Sun 06 Nov 2022 14:30",
          "username": "\t\t\t\tAz900500\t\t\t",
          "content": "I believe it's A & E ; the questions speaks to two things.<br>1. That can mitigate large DDOS attack -(Ans A )<br>2. A solutions architect must design a highly available infrastructure for a website; Downtime is not acceptable ( Ans E)<br>SoAns is AE<br><br>I guess we focus only on the DDOS attack aspect of the question<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>You need extra overhead to set up for E option. Target Tracking doesn't happen automatically when Auto Scaling is set up</li><li>So, spot instances mean HA for you?</li><li>spot instances aren't always going to be highly available enough for certain situations. its AC</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 753007,
          "date": "Thu 22 Dec 2022 07:17",
          "username": "\t\t\t\ta070112\t\t\t",
          "content": "You need extra overhead to set up for E option. Target Tracking doesn't happen automatically when Auto Scaling is set up",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 727436,
          "date": "Sat 26 Nov 2022 11:20",
          "username": "\t\t\t\tocbn3wby\t\t\t",
          "content": "So, spot instances mean HA for you?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 720175,
          "date": "Thu 17 Nov 2022 04:37",
          "username": "\t\t\t\tCizzla7049\t\t\t",
          "content": "spot instances aren't always going to be highly available enough for certain situations. its AC",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 709601,
          "date": "Wed 02 Nov 2022 04:31",
          "username": "\t\t\t\tmm_\t\t\t",
          "content": "Amazon GuardDuty has Threat response and remediation automation.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>No, GuardDuty's role is detect. not block.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 754890,
          "date": "Sat 24 Dec 2022 14:22",
          "username": "\t\t\t\tberks\t\t\t",
          "content": "No, GuardDuty's role is detect. not block.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 707431,
          "date": "Sat 29 Oct 2022 21:35",
          "username": "\t\t\t\tdokaedu\t\t\t",
          "content": "A : handle DDoS<br>E: Use EC2 Spot Instances in an Auto Scaling group with a target tracking scaling policy that is set to 80% CPU utilization.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>spot instance are not reliable, they are for worlds which can tolerate downtime. So the Answer should be A &amp; C</li><li>*workloads</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 725812,
          "date": "Thu 24 Nov 2022 13:06",
          "username": "\t\t\t\tVic_d_gr8\t\t\t",
          "content": "spot instance are not reliable, they are for worlds which can tolerate downtime. So the Answer should be A & C<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>*workloads</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 725813,
          "date": "Thu 24 Nov 2022 13:07",
          "username": "\t\t\t\tVic_d_gr8\t\t\t",
          "content": "*workloads",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 704801,
          "date": "Wed 26 Oct 2022 17:17",
          "username": "\t\t\t\tSix_Fingered_Jose\t\t\t",
          "content": "CF doesn't help with preventing downtime with dynamic content, it improves latency yes but doesn't really help with this case imo<br><br>question is asking for ways to PROTECT the server and prevent downtime,<br>and if you read this, guardduty makes sense.<br><br>https://aws.amazon.com/guardduty/<br>> Gain insight of compromised credentials, unusual data access in Amazon S3, API calls from known malicious IP addresses, and more.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 703390,
          "date": "Tue 25 Oct 2022 00:09",
          "username": "\t\t\t\tdave9994\t\t\t",
          "content": "The question is about \\\"Protect\\\", not remediation. So, A and C are the possible answers.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 699182,
          "date": "Wed 19 Oct 2022 18:13",
          "username": "\t\t\t\twhosawsome\t\t\t",
          "content": "GuardDuty allows you to take automated action to remedy an attack",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AB"
        }
      ]
    },
    {
      "question_id": "#105",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is preparing to deploy a new serverless workload. A solutions architect must use the principle of least privilege to configure permissions that will be used to run an AWS Lambda function. An Amazon EventBridge (Amazon CloudWatch Events) rule will invoke the function.<br>Which solution meets these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#105",
          "answers": [
            {
              "choice": "<p>A. Add an execution role to the function with lambda:InvokeFunction as the action and * as the principal.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Add an execution role to the function with lambda:InvokeFunction as the action and Service: lambda.amazonaws.com as the principal.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Add a resource-based policy to the function with lambda:* as the action and Service: events.amazonaws.com as the principal.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Add a resource-based policy to the function with lambda:InvokeFunction as the action and Service: events.amazonaws.com as the principal.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 698315,
          "date": "Tue 18 Oct 2022 17:18",
          "username": "\t\t\t\t123jhl0\t\t\t",
          "content": "Best way to check it... The question is taken from the example shown here in the documentation:<br>https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-use-resource-based.html#eb-lambda-permissions",
          "upvote_count": "20",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 752760,
          "date": "Wed 21 Dec 2022 22:15",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The correct solution is D.  Add a resource-based policy to the function with lambda:InvokeFunction as the action and Service: events.amazonaws.com as the principal.<br><br>The principle of least privilege requires that permissions are granted only to the minimum necessary to perform a task. In this case, the Lambda function needs to be able to be invoked by Amazon EventBridge (Amazon CloudWatch Events). To meet these requirements, you can add a resource-based policy to the function that allows the InvokeFunction action to be performed by the Service: events.amazonaws.com principal. This will allow Amazon EventBridge to invoke the function, but will not grant any additional permissions to the function.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Why other options are wrong<br><br>Option A is incorrect because it grants the lambda:InvokeFunction action to any principal (*), which would allow any entity to invoke the function and goes beyond the minimum permissions needed.<br><br>Option B is incorrect because it grants the lambda:InvokeFunction action to the Service: lambda.amazonaws.com principal, which would allow any Lambda function to invoke the function and goes beyond the minimum permissions needed.<br><br>Option C is incorrect because it grants the lambda:* action to the Service: events.amazonaws.com principal, which would allow Amazon EventBridge to perform any action on the function and goes beyond the minimum permissions needed.</li></ul>",
          "upvote_count": "6",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 752763,
          "date": "Wed 21 Dec 2022 22:18",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Why other options are wrong<br><br>Option A is incorrect because it grants the lambda:InvokeFunction action to any principal (*), which would allow any entity to invoke the function and goes beyond the minimum permissions needed.<br><br>Option B is incorrect because it grants the lambda:InvokeFunction action to the Service: lambda.amazonaws.com principal, which would allow any Lambda function to invoke the function and goes beyond the minimum permissions needed.<br><br>Option C is incorrect because it grants the lambda:* action to the Service: events.amazonaws.com principal, which would allow Amazon EventBridge to perform any action on the function and goes beyond the minimum permissions needed.",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 801360,
          "date": "Tue 07 Feb 2023 20:52",
          "username": "\t\t\t\tbdp123\t\t\t",
          "content": "https://docs.aws.amazon.com/eventbridge/latest/userguide/resource-based-policies-eventbridge.html#lambda-permissions",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 763674,
          "date": "Mon 02 Jan 2023 12:33",
          "username": "\t\t\t\tgustavtd\t\t\t",
          "content": "The definition scope of D is the smallest, so is it",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 759229,
          "date": "Wed 28 Dec 2022 02:11",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "events.amazonaws.com is principal for eventbridge",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 749235,
          "date": "Sun 18 Dec 2022 23:16",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 744091,
          "date": "Tue 13 Dec 2022 14:50",
          "username": "\t\t\t\twly_al\t\t\t",
          "content": "least privilege meant the role cannot be \\\"*\\\". answer B only mention lambda. so the answer was D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 727440,
          "date": "Sat 26 Nov 2022 11:26",
          "username": "\t\t\t\tocbn3wby\t\t\t",
          "content": "My answer was D, as this is the most specific answer. <br>And then there's this guy's answer (123jhl0) which provides more details.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        }
      ]
    },
    {
      "question_id": "#106",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is preparing to store confidential data in Amazon S3. For compliance reasons, the data must be encrypted at rest. Encryption key usage must be logged for auditing purposes. Keys must be rotated every year.<br>Which solution meets these requirements and is the MOST operationally efficient?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#106",
          "answers": [
            {
              "choice": "<p>A. Server-side encryption with customer-provided keys (SSE-C)<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Server-side encryption with Amazon S3 managed keys (SSE-S3)<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Server-side encryption with AWS KMS keys (SSE-KMS) with manual rotation<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Server-side encryption with AWS KMS keys (SSE-KMS) with automatic rotation<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 698323,
          "date": "Tue 18 Oct 2022 17:31",
          "username": "\t\t\t\t123jhl0\t\t\t",
          "content": "The MOST operationally efficient one is D. <br>Automating the key rotation is the most efficient.<br>Just to confirm, the A and B options don't allow automate the rotation as explained here: https://aws.amazon.com/kms/faqs/#:~:text=You%20can%20choose%20to%20have%20AWS%20KMS%20automatically%20rotate%20KMS,KMS%20custom%20key%20store%20feature<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>In addition you cannot log key usage in B, for A I am not certain</li><li>Thank you for the explanation.</li></ul>",
          "upvote_count": "12",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 749808,
          "date": "Mon 19 Dec 2022 13:25",
          "username": "\t\t\t\tvadiminski_a\t\t\t",
          "content": "In addition you cannot log key usage in B, for A I am not certain",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 727441,
          "date": "Sat 26 Nov 2022 11:27",
          "username": "\t\t\t\tocbn3wby\t\t\t",
          "content": "Thank you for the explanation.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 769938,
          "date": "Mon 09 Jan 2023 02:08",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "Server-side encryption with AWS KMS keys (SSE-KMS) with automatic rotation meets the requirements and is the most operationally efficient solution. This option allows you to use AWS KMS to automatically rotate the keys every year, which simplifies key management. In addition, key usage is logged for auditing purposes, and the data is encrypted at rest to meet compliance requirements.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 762546,
          "date": "Sat 31 Dec 2022 09:12",
          "username": "\t\t\t\tZerotn3\t\t\t",
          "content": "mazon API Gateway is a fully managed service that makes it easy to create, publish, maintain, monitor, and secure APIs at any scale. You can use API Gateway to create a REST API that exposes the location data as an API endpoint, allowing you to access the data from your analytics platform.<br><br>AWS Lambda is a serverless compute service that lets you run code in response to events or HTTP requests. You can use Lambda to write the code that retrieves the location data from your data store and returns it to API Gateway as a response to API requests. This allows you to scale the API to handle a large number of requests without the need to provision or manage any infrastructure.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 752770,
          "date": "Wed 21 Dec 2022 22:26",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The most operationally efficient solution that meets the requirements listed would be option D: Server-side encryption with AWS KMS keys (SSE-KMS) with automatic rotation.<br><br>SSE-KMS allows you to use keys that are managed by the AWS Key Management Service (KMS) to encrypt your data at rest. KMS is a fully managed service that makes it easy to create and control the encryption keys used to encrypt your data. With automatic key rotation enabled, KMS will automatically create a new key for you on a regular basis, typically every year, and use it to encrypt your data. This simplifies the key rotation process and reduces the operational burden on your team.<br><br>In addition, SSE-KMS provides logging of key usage through AWS CloudTrail, which can be used for auditing purposes.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Why other options are wrong<br><br>Option A: Server-side encryption with customer-provided keys (SSE-C) would require you to manage the encryption keys yourself, which can be more operationally burdensome. <br><br>Option B: Server-side encryption with Amazon S3 managed keys (SSE-S3) does not allow for key rotation or logging of the key usage. <br><br>Option C: Server-side encryption with AWS KMS keys (SSE-KMS) with manual rotation would require you to manually initiate the key rotation process, which can be more operationally burdensome compared to automatic rotation.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 752771,
          "date": "Wed 21 Dec 2022 22:27",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Why other options are wrong<br><br>Option A: Server-side encryption with customer-provided keys (SSE-C) would require you to manage the encryption keys yourself, which can be more operationally burdensome. <br><br>Option B: Server-side encryption with Amazon S3 managed keys (SSE-S3) does not allow for key rotation or logging of the key usage. <br><br>Option C: Server-side encryption with AWS KMS keys (SSE-KMS) with manual rotation would require you to manually initiate the key rotation process, which can be more operationally burdensome compared to automatic rotation.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 749236,
          "date": "Sun 18 Dec 2022 23:18",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 744783,
          "date": "Wed 14 Dec 2022 07:30",
          "username": "\t\t\t\tBerny\t\t\t",
          "content": "You can choose to have AWS KMS automatically rotate KMS keys every year, provided that those keys were generated within AWS KMS HSMs. Automatic key rotation is not supported for imported keys, asymmetric keys, or keys generated in a CloudHSM cluster using the AWS KMS custom key store feature. If you choose to import keys to AWS KMS or asymmetric keys or use a custom key store, you can manually rotate them by creating a new KMS key and mapping an existing key alias from the old KMS key to the new KMS key.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 740685,
          "date": "Sat 10 Dec 2022 05:58",
          "username": "\t\t\t\tPavelTech\t\t\t",
          "content": "Can anybody correct me if I'm wrong, KMS does not offer automatic rotations but SSE-KMS only allows automatic rotation once in 3 years thus if we want rotation every year we need to rotate it manually?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>You're wrong :) \\\"All AWS managed keys are automatically rotated every year. You cannot change this rotation schedule.\\\" https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#customer-cmk</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 750780,
          "date": "Tue 20 Dec 2022 12:23",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "You're wrong :) \\\"All AWS managed keys are automatically rotated every year. You cannot change this rotation schedule.\\\" https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#customer-cmk",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 714477,
          "date": "Wed 09 Nov 2022 11:35",
          "username": "\t\t\t\tPS_R\t\t\t",
          "content": "Agree Also, SSE-S3 cannot be audited.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        }
      ]
    },
    {
      "question_id": "#107",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A bicycle sharing company is developing a multi-tier architecture to track the location of its bicycles during peak operating hours. The company wants to use these data points in its existing analytics platform. A solutions architect must determine the most viable multi-tier option to support this architecture. The data points must be accessible from the REST API.<br>Which action meets these requirements for storing and retrieving location data?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#107",
          "answers": [
            {
              "choice": "<p>A. Use Amazon Athena with Amazon S3.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use Amazon API Gateway with AWS Lambda.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use Amazon QuickSight with Amazon Redshift.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use Amazon API Gateway with Amazon Kinesis Data Analytics.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 699031,
          "date": "Wed 19 Oct 2022 14:34",
          "username": "\t\t\t\tArielSchivo\t\t\t",
          "content": "API Gateway is needed to get the data so option A and C are out.<br>The company wants to use these data points in its existing analytics platform so there is no need to add Kynesis. Option D is also out.<br>This leaves us with option B as the correct one.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>i dont understand the use of a lambda function here, maybe if there would be need to transform the data, can you explain?</li><li>AWS Lambda is a serverless compute service that can be used to run code in response to specific events, such as changes to data in an Amazon S3 bucket or updates to a DynamoDB table. It could be used to process the location data, but it doesn't provide storage solution. Therefore, it would not be the best option for storing and retrieving location data in this scenario.</li></ul>",
          "upvote_count": "43",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 787911,
          "date": "Wed 25 Jan 2023 17:50",
          "username": "\t\t\t\tces26015\t\t\t",
          "content": "i dont understand the use of a lambda function here, maybe if there would be need to transform the data, can you explain?",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 784634,
          "date": "Sun 22 Jan 2023 20:39",
          "username": "\t\t\t\tbullrem\t\t\t",
          "content": "AWS Lambda is a serverless compute service that can be used to run code in response to specific events, such as changes to data in an Amazon S3 bucket or updates to a DynamoDB table. It could be used to process the location data, but it doesn't provide storage solution. Therefore, it would not be the best option for storing and retrieving location data in this scenario.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 704806,
          "date": "Wed 26 Oct 2022 17:24",
          "username": "\t\t\t\tSix_Fingered_Jose\t\t\t",
          "content": "I dont understand why you will vote B?<br>how are you going to store data with just lambda?<br>> Which action meets these requirements for storing and retrieving location data<br><br>In this use case there will obviously be a ton of data and you want to get real-time location data of the bicycles, and to analyze all these info kinesis is the one that makes most sense here.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>But KDA also cannot store data.</li><li>Lambda isn't storing the data themselves. It's triggering the data store to the company's \\\"existing data analytics platform\\\"</li><li>Real-time analytics on Kinesis Data Streams &amp; Firehose using SQL, not store db ...</li><li>I vote D because company HAS its analitcs Platform, Why pay?. Kinesis is for analys not for storing. Can you explain? Thanks</li><li>Weird Q as they already have their own data analysis platform<br>Hopefully i dont see this question in the exam lol</li><li>B Lambda and API</li><li>it can store according to the doc<br>There is no way to lambda to store data which is part of the requirements</li></ul>",
          "upvote_count": "23",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 802812,
          "date": "Thu 09 Feb 2023 05:03",
          "username": "\t\t\t\tJiyuKim\t\t\t",
          "content": "But KDA also cannot store data.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 753012,
          "date": "Thu 22 Dec 2022 07:22",
          "username": "\t\t\t\ta070112\t\t\t",
          "content": "Lambda isn't storing the data themselves. It's triggering the data store to the company's \\\"existing data analytics platform\\\"",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 745707,
          "date": "Thu 15 Dec 2022 06:07",
          "username": "\t\t\t\tkmliuy73\t\t\t",
          "content": "Real-time analytics on Kinesis Data Streams & Firehose using SQL, not store db ...",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 709962,
          "date": "Wed 02 Nov 2022 18:36",
          "username": "\t\t\t\trob74\t\t\t",
          "content": "I vote D because company HAS its analitcs Platform, Why pay?. Kinesis is for analys not for storing. Can you explain? Thanks<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Weird Q as they already have their own data analysis platform<br>Hopefully i dont see this question in the exam lol</li><li>B Lambda and API</li><li>it can store according to the doc<br>There is no way to lambda to store data which is part of the requirements</li></ul>",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 710390,
          "date": "Thu 03 Nov 2022 10:44",
          "username": "\t\t\t\tOnimole\t\t\t",
          "content": "Weird Q as they already have their own data analysis platform<br>Hopefully i dont see this question in the exam lol",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 709963,
          "date": "Wed 02 Nov 2022 18:36",
          "username": "\t\t\t\trob74\t\t\t",
          "content": "B Lambda and API",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 710388,
          "date": "Thu 03 Nov 2022 10:39",
          "username": "\t\t\t\tOnimole\t\t\t",
          "content": "it can store according to the doc<br>There is no way to lambda to store data which is part of the requirements",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 847058,
          "date": "Wed 22 Mar 2023 13:36",
          "username": "\t\t\t\tMssP\t\t\t",
          "content": "It could be B or D but I prefer B because you take into account \\\"The company wants to use these data points in its existing analytics platform\\\". No need Kinesis for analytics and more expensive than Lambda.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Besides this, look at the question: Which action meets these requirements for storing and retrieving location data?Ask for storing and retrieving, not analisys.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 847062,
          "date": "Wed 22 Mar 2023 13:38",
          "username": "\t\t\t\tMssP\t\t\t",
          "content": "Besides this, look at the question: Which action meets these requirements for storing and retrieving location data?Ask for storing and retrieving, not analisys.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 840697,
          "date": "Thu 16 Mar 2023 09:41",
          "username": "\t\t\t\tHemanthgowda1932\t\t\t",
          "content": "D is correct Answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 839231,
          "date": "Tue 14 Mar 2023 21:09",
          "username": "\t\t\t\trhm\t\t\t",
          "content": "D is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 836140,
          "date": "Sat 11 Mar 2023 15:30",
          "username": "\t\t\t\tmell1222\t\t\t",
          "content": "Use AWS IoT to collect and publish location data from the bicycles to an MQTT topic. The bicycles could be equipped with GPS sensors that send data to AWS IoT, which would then publish the data to an MQTT topic.<br>Set up an Amazon Kinesis Data Firehose delivery stream to ingest the data from the MQTT topic and store it in an Amazon S3 bucket. This would allow for easy storage and retrieval of the location data.<br>Use Amazon API Gateway to create a REST API that would allow the analytics platform to access the location data stored in the S3 bucket.<br>Set up AWS Lambda functions to process and transform the location data as required. This could involve filtering or aggregating the data to reduce the amount of data that needs to be stored, or transforming the data into a format that is easier for the analytics platform to consume.<br>Use Amazon CloudWatch to monitor and troubleshoot the various components of the multi-tier architecture. This would allow for easy identification and resolution of any issues that may arise.<br>KDA for storage and retrival",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 817200,
          "date": "Tue 21 Feb 2023 22:39",
          "username": "\t\t\t\tbdp123\t\t\t",
          "content": "https://aws.amazon.com/solutions/implementations/aws-streaming-data-solution-for-amazon-kinesis/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 811892,
          "date": "Fri 17 Feb 2023 14:13",
          "username": "\t\t\t\tAlhaz\t\t\t",
          "content": "\\\" existing analytics platform\\\"and hence no need of any other analytics kinesis",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 801371,
          "date": "Tue 07 Feb 2023 21:02",
          "username": "\t\t\t\tbdp123\t\t\t",
          "content": "This AWS CloudFormation template deploys a reference architecture that includes the following:<br>An Amazon API Gateway REST API acts as a proxy to Amazon Kinesis Data Streams, adding either an individual data record or a list of data records.<br>An Amazon Cognito user pool is used to control who can invoke REST API methods.<br>Kinesis Data Streams to store the incoming streaming data.<br>An AWS Lambda function processes the records from the data stream.<br>https://aws.amazon.com/solutions/implementations/streaming-data-solution-for-amazon-kinesis/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 798793,
          "date": "Sun 05 Feb 2023 13:01",
          "username": "\t\t\t\tCaoMengde09\t\t\t",
          "content": "Let's read again this key sentence : \\\"The company wants to use these data points in its existing analytics platform\\\"<br><br>So we have already an existing Analytics Platforms which means here that we should only support the architecture not propose a new analytics paltform from scratch. So AWS API Gateway and Lambda are more than enough to bring the data to the client's EXISTING ANALYTICS PLATFORM.<br><br>Also AWS Kinesis Data Analytics cannot work without already a provisioned AWS Kinesis Data Stream Cluster. So D.  Is far from enough to support the architecture",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 796248,
          "date": "Thu 02 Feb 2023 17:52",
          "username": "\t\t\t\tMaxx\t\t\t",
          "content": "I believe option D is more closer ,Kenisis Data Analytics can provide bicycle location as more data points to company's existing analytics platform through API Gateway , but not getting the question last statement for Storing and retrieving Local Data.<br>More closer Option Is D from all.Kindly comments in reply for this post .",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 796247,
          "date": "Thu 02 Feb 2023 17:50",
          "username": "\t\t\t\tMaxx\t\t\t",
          "content": "I believe option D is more closer ,Kenisis Data Analytics can provide bicycle location as more data points to company's existing analytics platform through API Gateway , but not getting the question last statement for Storing and retrieving Local Data ?!<br>But better option from all is Option D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 791301,
          "date": "Sun 29 Jan 2023 02:40",
          "username": "\t\t\t\tkerl\t\t\t",
          "content": "I select B, Question asking about solutioning a \\\"multi-tier\\\" application to call their existing Analytic Platform via Rest API. So i will use API-Gateway as the front for user to call, and lambda to call the exiting analytic platform n return back to the user.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 790578,
          "date": "Sat 28 Jan 2023 13:20",
          "username": "\t\t\t\tLionelSid\t\t\t",
          "content": "Amazon API Gateway can be used to create a REST API to access the data. Kinesis Data Analytics can be used to process and analyze the location data in real-time and stream it to an existing analytics platform. This solution would provide a way to handle high-volume location data streams and perform real-time analytics on them.<br><br>Amazon Athena with Amazon S3 could be used for ad-hoc querying and analyzing data stored in S3. But it would not be a real-time solution.<br><br>Amazon QuickSight with Amazon Redshift could also be used for analyzing data stored in Redshift but again it wouldn't be a real-time solution.<br><br>Amazon API Gateway with AWS Lambda could be used to create a REST API but it would not provide a way to process and analyze the location data in real-time and stream it to an existing analytics platform.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 784633,
          "date": "Sun 22 Jan 2023 20:38",
          "username": "\t\t\t\tbullrem\t\t\t",
          "content": "D.  Use Amazon API Gateway with Amazon Kinesis Data Analytics meets these requirements as it allows the company to store and retrieve location data in real-time and process it using Kinesis Data Analytics. The data can then be accessed from the REST API using Amazon API Gateway. This option allows the company to have real-time access to the location data and use it for analytics.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 781785,
          "date": "Fri 20 Jan 2023 03:09",
          "username": "\t\t\t\tkerl\t\t\t",
          "content": "https://aws.amazon.com/blogs/publicsector/creating-a-serverless-gps-monitoring-and-alerting-solution/<br>Can the above solution provide clear answer of choosing Lambda(B) or Kinesis Data Analytics? I go for B based on the blog form aws.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 778415,
          "date": "Tue 17 Jan 2023 01:44",
          "username": "\t\t\t\tremand\t\t\t",
          "content": "Only storage and retrieval is needed. Analytics platform is already available.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        }
      ]
    },
    {
      "question_id": "#108",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has an automobile sales website that stores its listings in a database on Amazon RDS. When an automobile is sold, the listing needs to be removed from the website and the data must be sent to multiple target systems.<br>Which design should a solutions architect recommend?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#108",
          "answers": [
            {
              "choice": "<p>A. Create an AWS Lambda function triggered when the database on Amazon RDS is updated to send the information to an Amazon Simple Queue Service (Amazon SQS) queue for the targets to consume.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an AWS Lambda function triggered when the database on Amazon RDS is updated to send the information to an Amazon Simple Queue Service (Amazon SQS) FIFO queue for the targets to consume.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Subscribe to an RDS event notification and send an Amazon Simple Queue Service (Amazon SQS) queue fanned out to multiple Amazon Simple Notification Service (Amazon SNS) topics. Use AWS Lambda functions to update the targets.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Subscribe to an RDS event notification and send an Amazon Simple Notification Service (Amazon SNS) topic fanned out to multiple Amazon Simple Queue Service (Amazon SQS) queues. Use AWS Lambda functions to update the targets.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 723536,
          "date": "Mon 21 Nov 2022 14:21",
          "username": "\t\t\t\tromko\t\t\t",
          "content": "Interesting point that Amazon RDS event notification doesn't support any notification when data inside DB is updated.<br>https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.overview.html<br>So subscription to RDS eventsdoesn't give any value for Fanout = SNS => SQS<br><br>B is out because FIFO is not required here.<br><br>A is left as correct answer<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Romko, you are right pal. Nice research. <br><br>There is RDS Fanout to SNS, but not specifically for DB level events (write, reads, etc). <br>It can fan out events at instance level (turn on, restart, update), cluster level (added to cluster, removed from cluster, etc). But not at DB level. <br><br>More detailed event list here:<br>https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.Messages.html<br><br>Correct answer is A. </li><li>D is connect <br>RDS event notification by RDS stream or advance audit DML so it is possible</li><li>Please provide reference for this claim: \\\" event notification by RDS stream or advance audit DML\\\"</li><li>The key is \\\"Fanned out\\\" due to \\\"Multiple target systems\\\" need to update</li><li>Amazon RDS uses the SNS to provide notification when an Amazon event occurs<br>https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.html</li></ul>",
          "upvote_count": "44",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 728021,
          "date": "Sun 27 Nov 2022 08:53",
          "username": "\t\t\t\tocbn3wby\t\t\t",
          "content": "Romko, you are right pal. Nice research. <br><br>There is RDS Fanout to SNS, but not specifically for DB level events (write, reads, etc). <br>It can fan out events at instance level (turn on, restart, update), cluster level (added to cluster, removed from cluster, etc). But not at DB level. <br><br>More detailed event list here:<br>https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.Messages.html<br><br>Correct answer is A. ",
          "upvote_count": "13",
          "selected_answers": ""
        },
        {
          "id": 743340,
          "date": "Mon 12 Dec 2022 22:11",
          "username": "\t\t\t\tJiang_aws1\t\t\t",
          "content": "D is connect <br>RDS event notification by RDS stream or advance audit DML so it is possible<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Please provide reference for this claim: \\\" event notification by RDS stream or advance audit DML\\\"</li><li>The key is \\\"Fanned out\\\" due to \\\"Multiple target systems\\\" need to update</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 750797,
          "date": "Tue 20 Dec 2022 12:40",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "Please provide reference for this claim: \\\" event notification by RDS stream or advance audit DML\\\"",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 743342,
          "date": "Mon 12 Dec 2022 22:13",
          "username": "\t\t\t\tJiang_aws1\t\t\t",
          "content": "The key is \\\"Fanned out\\\" due to \\\"Multiple target systems\\\" need to update",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 725830,
          "date": "Thu 24 Nov 2022 13:35",
          "username": "\t\t\t\tVic_d_gr8\t\t\t",
          "content": "Amazon RDS uses the SNS to provide notification when an Amazon event occurs<br>https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 766049,
          "date": "Wed 04 Jan 2023 21:15",
          "username": "\t\t\t\tksolovyov\t\t\t",
          "content": "RDS events only provide operational events such as DB instance events, DB parameter group events, DB security group events, and DB snapshot events. What we need in the scenario is to capture data-modifying events (INSERT, DELETE, UPDATE) which can be achieved thru native functions or stored procedures.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I agree with it requiring a native function or stored procedure, but can they in turn invoke a Lambda function? I have only seen this being possible with Aurora, but not RDS - and I'm not able to find anything googling for it either. I guess it has to be possible, since there's no other option that fits either.<br><br>https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Integrating.Lambda.html</li><li>To add to that though, A also states to only use SQS (no SNS to SQS fan-out), which doesn't seem right as the message needs to go to multiple targets?</li></ul>",
          "upvote_count": "5",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 782326,
          "date": "Fri 20 Jan 2023 15:15",
          "username": "\t\t\t\tBlueVolcano1\t\t\t",
          "content": "I agree with it requiring a native function or stored procedure, but can they in turn invoke a Lambda function? I have only seen this being possible with Aurora, but not RDS - and I'm not able to find anything googling for it either. I guess it has to be possible, since there's no other option that fits either.<br><br>https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Integrating.Lambda.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>To add to that though, A also states to only use SQS (no SNS to SQS fan-out), which doesn't seem right as the message needs to go to multiple targets?</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 782329,
          "date": "Fri 20 Jan 2023 16:49",
          "username": "\t\t\t\tBlueVolcano1\t\t\t",
          "content": "To add to that though, A also states to only use SQS (no SNS to SQS fan-out), which doesn't seem right as the message needs to go to multiple targets?",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 844355,
          "date": "Mon 20 Mar 2023 01:05",
          "username": "\t\t\t\tasoli\t\t\t",
          "content": "it says different target systems. So, we need a multiple sqs queue. <br>With only 1 SQS, we cannot have multiple deliveries to different targets. So, it needs SNS fan out to multiple SQS and then each sqs has a target system to consume<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>when I read others' answers, I changed my mind. The answer is A. <br>as mentioned, the updating data does not triggering any event in RDS. So, D is not correct.<br>https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.overview.html</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 844359,
          "date": "Mon 20 Mar 2023 01:09",
          "username": "\t\t\t\tasoli\t\t\t",
          "content": "when I read others' answers, I changed my mind. The answer is A. <br>as mentioned, the updating data does not triggering any event in RDS. So, D is not correct.<br>https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.overview.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 838098,
          "date": "Mon 13 Mar 2023 18:12",
          "username": "\t\t\t\tCapJackSparrow\t\t\t",
          "content": "I mean, it is not saying any changes have been made to the DB. .. the condition \\\"sold\\\" becomes true, which triggers the event. I'm inclined to go with D on this with that logic... Although I can not say for sure.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 834553,
          "date": "Fri 10 Mar 2023 02:56",
          "username": "\t\t\t\ttaehyeki\t\t\t",
          "content": "https://docs.aws.amazon.com/ko_kr/lambda/latest/dg/services-rds.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 822083,
          "date": "Sun 26 Feb 2023 06:29",
          "username": "\t\t\t\tTofu13\t\t\t",
          "content": "Question to who thinks A is correct:<br>How is sending data to\\\"multiple target systems\\\" possible with a single SQS?",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 801378,
          "date": "Tue 07 Feb 2023 21:06",
          "username": "\t\t\t\tbdp123\t\t\t",
          "content": "https://docs.aws.amazon.com/lambda/latest/dg/services-rds.html<br>https://docs.aws.amazon.com/lambda/latest/dg/with-sns.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 801015,
          "date": "Tue 07 Feb 2023 15:48",
          "username": "\t\t\t\tUnluckyDucky\t\t\t",
          "content": "Weird question... <br>Answer A - only one system will be able to poll the SQS message not multiple which doesn't meet the requirements.<br>Answer D - Amazon RDS event notifications don't provide Insert/Update/Delete notifications, they only provide notifications for the instance itself.<br><br>Not really sure what's the right answer here... the key imho is that it requires processing by multiple systems therefore fanout makes most sense.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 783478,
          "date": "Sat 21 Jan 2023 16:28",
          "username": "\t\t\t\tdexpos\t\t\t",
          "content": "this question is tricky. I can undestand from the links provided that the RDS event notification can not be used for the modification of a data in the DB but the SQS alone is not enough to update several systems.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>what is your take, what is your answer dexpos, please advise?</li><li>for me is D</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 794337,
          "date": "Tue 31 Jan 2023 16:21",
          "username": "\t\t\t\ttinkeringpuncturing\t\t\t",
          "content": "what is your take, what is your answer dexpos, please advise?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>for me is D</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 794535,
          "date": "Tue 31 Jan 2023 20:21",
          "username": "\t\t\t\tdexpos\t\t\t",
          "content": "for me is D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 778419,
          "date": "Tue 17 Jan 2023 01:52",
          "username": "\t\t\t\tremand\t\t\t",
          "content": "Multiple Target system could consume from its own SQS via SNS fanout make sense",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 773981,
          "date": "Fri 13 Jan 2023 02:22",
          "username": "\t\t\t\tmj61\t\t\t",
          "content": "A<br> Lambda function could be triggered by an RDS event notification, which would be set up to trigger when a new record is inserted, updated, or deleted in the database, sending the data to an SQS queue for the targets to consume. SQS can provide an asynchronous messaging service that allows the targets to process the data at their own pace and can buffer the data in case of high traffic.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 763585,
          "date": "Mon 02 Jan 2023 08:54",
          "username": "\t\t\t\taba2s\t\t\t",
          "content": "RDS events only provide operational events such as DB instance events, DB parameter group events, DB security group events, and DB snapshot events. What we need in the scenario is to capture data-modifying events (INSERT,DELETE,UPDATE) which can be achieved thru native functions or stored procedures. So C and D is out. <br>order doesn't matter here, so B is also out. I vote for A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 762554,
          "date": "Sat 31 Dec 2022 09:20",
          "username": "\t\t\t\tZerotn3\t\t\t",
          "content": "To design a solution that sends data from an Amazon RDS database to multiple target systems when an automobile is sold, you can use a combination of Amazon Simple Notification Service (Amazon SNS) and Amazon Simple Queue Service (Amazon SQS).",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 758309,
          "date": "Tue 27 Dec 2022 10:25",
          "username": "\t\t\t\tarseyam\t\t\t",
          "content": "You can create a procedure in your RDS database to invoke a Lambda function<br>https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/PostgreSQL-Lambda.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 754783,
          "date": "Sat 24 Dec 2022 10:15",
          "username": "\t\t\t\tNV305\t\t\t",
          "content": "Since RDS sends notification to SNS. IT HAS TO BE D.  :)<br><br>https://docs.aws.amazon.com/lambda/latest/dg/services-rds.html<br>https://docs.aws.amazon.com/lambda/latest/dg/with-sns.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>D says, \\\"AWS Lambda functions to update the targets.\\\" However, the question only asks that the data be sent to target systems. A doesn't update targets.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 760335,
          "date": "Wed 28 Dec 2022 22:01",
          "username": "\t\t\t\tFNJ1111\t\t\t",
          "content": "D says, \\\"AWS Lambda functions to update the targets.\\\" However, the question only asks that the data be sent to target systems. A doesn't update targets.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 754689,
          "date": "Sat 24 Dec 2022 05:01",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "D is the right option.<br>RDS event notifications can be configured toSNS topic.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 752795,
          "date": "Wed 21 Dec 2022 22:52",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The correct design for this scenario would be Option D: Subscribe to an RDS event notification and send an Amazon Simple Notification Service (Amazon SNS) topic fanned out to multiple Amazon Simple Queue Service (Amazon SQS) queues. Use AWS Lambda functions to update the targets.<br><br>In this design, Amazon RDS can be configured to send a notification to an Amazon SNS topic when a specific event occurs, such as an update to the database. The Amazon SNS topic can then be configured to fan out the notification to multiple Amazon SQS queues, which allows the targets to consume the data asynchronously. AWS Lambda functions can be triggered by the messages in the Amazon SQS queues to process the data and update the target systems.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A is incorrect because it does not include the use of Amazon SNS to fan out the data to multiple targets. <br>Option B is incorrect because it uses a FIFO (first-in, first-out) queue, which is not necessary for this scenario. <br>Option C is incorrect because it uses an Amazon SQS queue as the initial event notification, rather than Amazon SNS.</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 752796,
          "date": "Wed 21 Dec 2022 22:53",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A is incorrect because it does not include the use of Amazon SNS to fan out the data to multiple targets. <br>Option B is incorrect because it uses a FIFO (first-in, first-out) queue, which is not necessary for this scenario. <br>Option C is incorrect because it uses an Amazon SQS queue as the initial event notification, rather than Amazon SNS.",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#109",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company needs to store data in Amazon S3 and must prevent the data from being changed. The company wants new objects that are uploaded to Amazon S3 to remain unchangeable for a nonspecific amount of time until the company decides to modify the objects. Only specific users in the company's AWS account can have the ability 10 delete the objects.<br>What should a solutions architect do to meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#109",
          "answers": [
            {
              "choice": "<p>A. Create an S3 Glacier vault. Apply a write-once, read-many (WORM) vault lock policy to the objects.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an S3 bucket with S3 Object Lock enabled. Enable versioning. Set a retention period of 100 years. Use governance mode as the S3 bucket's default retention mode for new objects.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an S3 bucket. Use AWS CloudTrail to track any S3 API events that modify the objects. Upon notification, restore the modified objects from any backup versions that the company has.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an S3 bucket with S3 Object Lock enabled. Enable versioning. Add a legal hold to the objects. Add the s3:PutObjectLegalHold permission to the IAM policies of users who need to delete the objects.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 698347,
          "date": "Tue 18 Oct 2022 18:20",
          "username": "\t\t\t\t123jhl0\t\t\t",
          "content": "A - No as \\\"specific users can delete\\\"<br>B - No as \\\"nonspecific amount of time\\\"<br>C - No as \\\"prevent the data from being change\\\"<br>D - The answer: \\\"The Object Lock legal hold operation enables you to place a legal hold on an object version. Like setting a retention period, a legal hold prevents an object version from being overwritten or deleted. However, a legal hold doesn't have an associated retention period and remains in effect until removed.\\\" https://docs.aws.amazon.com/AmazonS3/latest/userguide/batch-ops-legal-hold.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>The Object Lock legal hold operation enables you to place a legal hold on an object version. Like setting a retention period, a legal hold prevents an object version from being overwritten or deleted. However, a legal hold doesn't have an associated retention period and remains in effect until removed.<br><br>Correct</li></ul>",
          "upvote_count": "19",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 757957,
          "date": "Tue 27 Dec 2022 00:59",
          "username": "\t\t\t\tPassNow1234\t\t\t",
          "content": "The Object Lock legal hold operation enables you to place a legal hold on an object version. Like setting a retention period, a legal hold prevents an object version from being overwritten or deleted. However, a legal hold doesn't have an associated retention period and remains in effect until removed.<br><br>Correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 696722,
          "date": "Mon 17 Oct 2022 03:06",
          "username": "\t\t\t\tChunsli\t\t\t",
          "content": "typo -- 10 delete the objects => TO delete the objects",
          "upvote_count": "11",
          "selected_answers": ""
        },
        {
          "id": 824924,
          "date": "Tue 28 Feb 2023 15:50",
          "username": "\t\t\t\tKZM\t\t\t",
          "content": "The correct answer is D. ",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 824606,
          "date": "Tue 28 Feb 2023 11:12",
          "username": "\t\t\t\tWherecanIstart\t\t\t",
          "content": "Option B specifies a retention period of 100 years which contradicts what the question asked for.....<br>\\\"The company wants new objects that are uploaded to Amazon S3 to remain unchangeable for a nonspecific amount of time until the company decides to modify the objects\\\"<br>Setting the retention period of 100 years is specific and the company wants new data/objects to remain unchanged for nonspecific amount of time.<br><br>Correct answer is D<br><br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/batch-ops-legal-hold.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 812299,
          "date": "Fri 17 Feb 2023 20:50",
          "username": "\t\t\t\tbdp123\t\t\t",
          "content": "\\\"The Object Lock legal hold operation enables you to place a legal hold on an object version. Like setting a retention period, a legal hold prevents an object version from being overwritten or deleted. However, a legal hold doesn't have an associated retention period and remains in effect until removed.\\\" https://docs.aws.amazon.com/AmazonS3/latest/userguide/batch-ops-legal-hold.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 806023,
          "date": "Sun 12 Feb 2023 07:23",
          "username": "\t\t\t\tYelizaveta\t\t\t",
          "content": "retention period of 100 Years prevents the object to be deleted bevor the retention period expires, so it's not a good fit.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 772839,
          "date": "Wed 11 Jan 2023 20:34",
          "username": "\t\t\t\tnadir_kh\t\t\t",
          "content": "it is B. <br>Once a legal hold is enabled, regardless of the object's retention date or retention mode, the object version cannot be deleted until the legal hold is removed.<br><br>Question says: \\\"Specific users must have ability to delete objects\\\"",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 767475,
          "date": "Fri 06 Jan 2023 10:43",
          "username": "\t\t\t\tJohn_Zhuang\t\t\t",
          "content": "While S3 bucket governance mode does allow certain users with permissions to alter retention/delete objects, the 100 years in Option B makes it invalid. <br><br>Correct answer is option D.  <br>\\\"With Object Lock you can also place a legal hold on an object version. Like a retention period, a legal hold prevents an object version from being overwritten or deleted. However, a legal hold doesn't have an associated retention period and remains in effect until removed. \\\"<br>https://aws.amazon.com/s3/features/object-lock/<br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html#object-lock-legal-holds",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 763604,
          "date": "Mon 02 Jan 2023 09:53",
          "username": "\t\t\t\taba2s\t\t\t",
          "content": "With Object Lock, you can also place a legal hold on an object version. Like a retention period, a legal hold prevents an object version from being overwritten or deleted. However, a legal hold doesn't have an associated retention period and remains in effect until removed. Legal holds can be freely placed and removed by any user who has thes3:PutObjectLegalHoldpermission.<br>B - No as \\\"nonspecific amount of time\\\" otherwise B will meet the requirement with legal hold attached.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 760341,
          "date": "Wed 28 Dec 2022 22:10",
          "username": "\t\t\t\tFNJ1111\t\t\t",
          "content": "Wouldn't D require s3:GetBucketObjectLockConfiguration IAM permission? If so, D is incomplete and wouldn't meet the requirement.<br>(from the link shared above)",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 757260,
          "date": "Mon 26 Dec 2022 09:38",
          "username": "\t\t\t\tSilvestr\t\t\t",
          "content": "Correct answer : B<br>Retention mode - Governance:<br> Most users can't overwrite or delete an object version or alter its lock settings<br> Some users have special permissions to change the retention or delete the object",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 752801,
          "date": "Wed 21 Dec 2022 23:05",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "To meet the requirements specified in the question, the solution architect should choose Option B: Create an S3 bucket with S3 Object Lock enabled. Enable versioning. Set a retention period of 100 years. Use governance mode as the S3 bucket's default retention mode for new objects.<br><br>S3 Object Lock is a feature of Amazon S3 that allows you to apply a retention period to objects in your bucket, during which time the objects cannot be deleted or overwritten. By enabling versioning on the bucket, you can ensure that all versions of an object are retained, including any deletions or overwrites. By setting a retention period of 100 years, you can ensure that the objects remain unchangeable for a long time.<br><br>By using governance mode as the default retention mode for new objects, you can ensure that the retention period is applied to all new objects that are uploaded to the bucket. This will prevent the objects from being deleted or overwritten until the retention period expires.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Why other options are wrong<br>Option A (creating an S3 Glacier vault and applying a WORM vault lock policy) would not meet the requirement to prevent the objects from being changed, because S3 Glacier is a storage class for long-term data archival and does not support read-write operations. <br><br>Option C (using CloudTrail to track API events and restoring modified objects from backup versions) would not prevent the objects from being changed in the first place.<br><br>Option D (adding a legal hold and the s3:PutObjectLegalHold permission to IAM policies) would not meet the requirement to prevent the objects from being changed for a nonspecific amount of time.</li><li>Legal holds are used to prevent objects that are subject to legal or compliance requirements from being deleted or overwritten, even if their retention period has expired. While legal holds can be useful for preventing the accidental deletion of important objects, they do not prevent the objects from being changed. S3 Object Lock can be used to prevent objects from being deleted or overwritten for a specified retention period, but a legal hold does not provide this capability.<br><br>In addition, the s3:PutObjectLegalHold permission allows users to place a legal hold on an object, but it does not prevent the object from being changed. To prevent the objects from being changed for a nonspecific amount of time, the solution architect should use S3 Object Lock and set a longer retention period on the objects.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 752802,
          "date": "Wed 21 Dec 2022 23:06",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Why other options are wrong<br>Option A (creating an S3 Glacier vault and applying a WORM vault lock policy) would not meet the requirement to prevent the objects from being changed, because S3 Glacier is a storage class for long-term data archival and does not support read-write operations. <br><br>Option C (using CloudTrail to track API events and restoring modified objects from backup versions) would not prevent the objects from being changed in the first place.<br><br>Option D (adding a legal hold and the s3:PutObjectLegalHold permission to IAM policies) would not meet the requirement to prevent the objects from being changed for a nonspecific amount of time.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Legal holds are used to prevent objects that are subject to legal or compliance requirements from being deleted or overwritten, even if their retention period has expired. While legal holds can be useful for preventing the accidental deletion of important objects, they do not prevent the objects from being changed. S3 Object Lock can be used to prevent objects from being deleted or overwritten for a specified retention period, but a legal hold does not provide this capability.<br><br>In addition, the s3:PutObjectLegalHold permission allows users to place a legal hold on an object, but it does not prevent the object from being changed. To prevent the objects from being changed for a nonspecific amount of time, the solution architect should use S3 Object Lock and set a longer retention period on the objects.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 752804,
          "date": "Wed 21 Dec 2022 23:08",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Legal holds are used to prevent objects that are subject to legal or compliance requirements from being deleted or overwritten, even if their retention period has expired. While legal holds can be useful for preventing the accidental deletion of important objects, they do not prevent the objects from being changed. S3 Object Lock can be used to prevent objects from being deleted or overwritten for a specified retention period, but a legal hold does not provide this capability.<br><br>In addition, the s3:PutObjectLegalHold permission allows users to place a legal hold on an object, but it does not prevent the object from being changed. To prevent the objects from being changed for a nonspecific amount of time, the solution architect should use S3 Object Lock and set a longer retention period on the objects.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 749252,
          "date": "Sun 18 Dec 2022 23:37",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 741962,
          "date": "Sun 11 Dec 2022 19:08",
          "username": "\t\t\t\tsanyoc\t\t\t",
          "content": "\\\"The Object Lock legal hold operation enables you to place a legal hold on an object version. Like setting a retention period, a legal hold prevents an object version from being overwritten or deleted. However, a legal hold doesn't have an associated retention period and remains in effect until removed.\\\"",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 736756,
          "date": "Tue 06 Dec 2022 12:25",
          "username": "\t\t\t\tadelegend\t\t\t",
          "content": "Answer is D, the key here is that no specific retention period was set by the company and this is exactly what differentiates Legal hold from Governance<br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 733909,
          "date": "Fri 02 Dec 2022 17:26",
          "username": "\t\t\t\tAWSExam2021\t\t\t",
          "content": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html<br><br>With Object Lock you can also place a legal hold on an object version. Like a retention period, a legal hold prevents an object version from being overwritten or deleted. However, a legal hold doesn't have an associated retention period and remains in effect until removed. Legal holds can be freely placed and removed by any user who has the s3:PutObjectLegalHold permission.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 728824,
          "date": "Mon 28 Nov 2022 07:08",
          "username": "\t\t\t\tCizzla7049\t\t\t",
          "content": "Answer is 100% B. <br><br>Governance mode<br>You should use the Governance mode if you want to protect objects from being deleted by most users during a pre-defined retention period, but at the same time want some users with special permissions to have the flexibility to alter the retention settings or delete the objects.<br><br>Legal Hold works as an infinite retention period. Once applied it is not possible to delete any object until the hold is released manually. The hold can only be removed by users with special permissions.<br><br>A retention period specifies a fixed period of time during which an object remains locked. During this period, your object is WORM-protected and can't be overwritten or deleted. You apply a retention period either in number of days or number of years with the minimum being 1-day and no maximum limit.<br>A legal hold provides the same protection as a retention period, but it has no expiration date. Instead, a legal hold remains in place until you explicitly remove it.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Legal Hold works as an infinite retention period, which is being asked for \\\"to remain unchangeable for a nonspecific amount of time \\\"</li><li>You think 100 years of retention period is \\\"nonspecific amount of time\\\"?</li><li>Legal hold, no one can delete objects. Governance, those with special permissions can delete</li><li>s3:PutObjectLegalHold permission allows users to remove the legal hold on the objects, So they can delete even if legal hold is there.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 750812,
          "date": "Tue 20 Dec 2022 12:57",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "Legal Hold works as an infinite retention period, which is being asked for \\\"to remain unchangeable for a nonspecific amount of time \\\"",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 753020,
          "date": "Thu 22 Dec 2022 07:33",
          "username": "\t\t\t\ta070112\t\t\t",
          "content": "You think 100 years of retention period is \\\"nonspecific amount of time\\\"?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 728828,
          "date": "Mon 28 Nov 2022 07:11",
          "username": "\t\t\t\tCizzla7049\t\t\t",
          "content": "Legal hold, no one can delete objects. Governance, those with special permissions can delete<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>s3:PutObjectLegalHold permission allows users to remove the legal hold on the objects, So they can delete even if legal hold is there.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 731871,
          "date": "Wed 30 Nov 2022 19:51",
          "username": "\t\t\t\tmj98\t\t\t",
          "content": "s3:PutObjectLegalHold permission allows users to remove the legal hold on the objects, So they can delete even if legal hold is there.",
          "upvote_count": "4",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#110",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A social media company allows users to upload images to its website. The website runs on Amazon EC2 instances. During upload requests, the website resizes the images to a standard size and stores the resized images in Amazon S3. Users are experiencing slow upload requests to the website.<br>The company needs to reduce coupling within the application and improve website performance. A solutions architect must design the most operationally efficient process for image uploads.<br>Which combination of actions should the solutions architect take to meet these requirements? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: CD</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#110",
          "answers": [
            {
              "choice": "<p>A. Configure the application to upload images to S3 Glacier.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Configure the web server to upload the original images to Amazon S3.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Configure the application to upload images directly from each user's browser to Amazon S3 through the use of a presigned URL<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure S3 Event Notifications to invoke an AWS Lambda function when an image is uploaded. Use the function to resize the image.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. Create an Amazon EventBridge (Amazon CloudWatch Events) rule that invokes an AWS Lambda function on a schedule to resize uploaded images.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 752807,
          "date": "Wed 21 Dec 2022 23:16",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "To meet the requirements of reducing coupling within the application and improving website performance, the solutions architect should consider taking the following actions:<br>C.  Configure the application to upload images directly from each user's browser to Amazon S3 through the use of a pre-signed URL. This will allow the application to upload images directly to S3 without having to go through the web server, which can reduce the load on the web server and improve performance.<br>D.  Configure S3 Event Notifications to invoke an AWS Lambda function when an image is uploaded. Use the function to resize the image. This will allow the application to resize images asynchronously, rather than having to do it synchronously during the upload request, which can improve performance.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Why other options are wrong<br>Option A, Configuring the application to upload images to S3 Glacier, is not relevant to improving the performance of image uploads. <br><br>Option B, Configuring the webserver to upload the original images to Amazon S3, is not a recommended solution as it would not reduce coupling within the application or improve performance. <br><br>Option E, Creating an Amazon EventBridge (Amazon CloudWatch Events) rule that invokes an AWS Lambda function on a schedule to resize uploaded images, is not a recommended solution as it would not be able to resize images in a timely manner and would not improve performance.</li><li>Here it means to decouple the processes, so that the web server don't have to do the resizing, so it doesn't slow down. The customers access the web server, so the web server have to be involved in the process, and how the others already wrote, the pre-signed URL is not the right solution because, of the explanation you can read in the other comments. <br><br>And additional! \\\"Configure the application to upload images directly from EACH USER'S BROWSER to Amazon S3 through the use of a pre-signed URL\\\"<br><br>I am not an expert, but I can't imagine that you can store an image that an user uploads in his browser etc.</li></ul>",
          "upvote_count": "18",
          "selected_answers": "Selected Answer: CD"
        },
        {
          "id": 752808,
          "date": "Wed 21 Dec 2022 23:16",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Why other options are wrong<br>Option A, Configuring the application to upload images to S3 Glacier, is not relevant to improving the performance of image uploads. <br><br>Option B, Configuring the webserver to upload the original images to Amazon S3, is not a recommended solution as it would not reduce coupling within the application or improve performance. <br><br>Option E, Creating an Amazon EventBridge (Amazon CloudWatch Events) rule that invokes an AWS Lambda function on a schedule to resize uploaded images, is not a recommended solution as it would not be able to resize images in a timely manner and would not improve performance.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Here it means to decouple the processes, so that the web server don't have to do the resizing, so it doesn't slow down. The customers access the web server, so the web server have to be involved in the process, and how the others already wrote, the pre-signed URL is not the right solution because, of the explanation you can read in the other comments. <br><br>And additional! \\\"Configure the application to upload images directly from EACH USER'S BROWSER to Amazon S3 through the use of a pre-signed URL\\\"<br><br>I am not an expert, but I can't imagine that you can store an image that an user uploads in his browser etc.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 806028,
          "date": "Sun 12 Feb 2023 07:37",
          "username": "\t\t\t\tYelizaveta\t\t\t",
          "content": "Here it means to decouple the processes, so that the web server don't have to do the resizing, so it doesn't slow down. The customers access the web server, so the web server have to be involved in the process, and how the others already wrote, the pre-signed URL is not the right solution because, of the explanation you can read in the other comments. <br><br>And additional! \\\"Configure the application to upload images directly from EACH USER'S BROWSER to Amazon S3 through the use of a pre-signed URL\\\"<br><br>I am not an expert, but I can't imagine that you can store an image that an user uploads in his browser etc.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 852223,
          "date": "Mon 27 Mar 2023 17:35",
          "username": "\t\t\t\tkampatra\t\t\t",
          "content": "to me : BD",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 842547,
          "date": "Sat 18 Mar 2023 08:35",
          "username": "\t\t\t\tSuketuKohli\t\t\t",
          "content": "you can use a presigned URL to optionally share objects or allow your customers/users to upload objects to buckets without AWS security credentials or permissions. https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-presigned-url.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 842500,
          "date": "Sat 18 Mar 2023 06:56",
          "username": "\t\t\t\tGrace83\t\t\t",
          "content": "B + D looks right to me.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 835527,
          "date": "Sat 11 Mar 2023 00:26",
          "username": "\t\t\t\tfkie4\t\t\t",
          "content": "Why would anyone vote C? signed URL is for temporary access. also, look at the vote here: https://www.examtopics.com/discussions/amazon/view/82971-exam-aws-certified-solutions-architect-associate-saa-c02/",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 835498,
          "date": "Fri 10 Mar 2023 23:30",
          "username": "\t\t\t\tavengu\t\t\t",
          "content": "B+D looks correct as creating & using presigned url is not operationally efficient",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 794073,
          "date": "Tue 31 Jan 2023 12:41",
          "username": "\t\t\t\taakashkumar1999\t\t\t",
          "content": "B+D MAKES SENSE",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 788467,
          "date": "Thu 26 Jan 2023 08:07",
          "username": "\t\t\t\tkdinesh95\t\t\t",
          "content": "no presigned url full fills",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 778466,
          "date": "Tue 17 Jan 2023 03:13",
          "username": "\t\t\t\tremand\t\t\t",
          "content": "CD - pre-signed URL makes sense",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: CD"
        },
        {
          "id": 778079,
          "date": "Mon 16 Jan 2023 19:07",
          "username": "\t\t\t\tJiggs007\t\t\t",
          "content": "B + D more sense for me.<br>Event notifications  Trigger workflows that use Amazon Simple Notification Service (Amazon SNS), Amazon Simple Queue Service (Amazon SQS), and AWS Lambda when a change is made to your S3 resources.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>agree, pre-signed URL doesnt seem like a good choice \\\"When you create a presigned URL for your object, you must provide your security credentials and then specify a bucket name, an object key, an HTTP method (GET to download the object), and an expiration date and time. The presigned URLs are valid only for the specified duration.\\\"</li></ul>",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 787929,
          "date": "Wed 25 Jan 2023 18:09",
          "username": "\t\t\t\tces26015\t\t\t",
          "content": "agree, pre-signed URL doesnt seem like a good choice \\\"When you create a presigned URL for your object, you must provide your security credentials and then specify a bucket name, an object key, an HTTP method (GET to download the object), and an expiration date and time. The presigned URLs are valid only for the specified duration.\\\"",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 777071,
          "date": "Sun 15 Jan 2023 22:15",
          "username": "\t\t\t\tEllo2023\t\t\t",
          "content": "B and D<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>If the webserver handle also the upload that would increase the TIGHT COUPLING of UPLOADING and STORING and PROCESSING. If users uploads directly to S3 the APP would focuson resizing the final image and storing it</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 798838,
          "date": "Sun 05 Feb 2023 14:04",
          "username": "\t\t\t\tCaoMengde09\t\t\t",
          "content": "If the webserver handle also the upload that would increase the TIGHT COUPLING of UPLOADING and STORING and PROCESSING. If users uploads directly to S3 the APP would focuson resizing the final image and storing it",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 775161,
          "date": "Sat 14 Jan 2023 08:24",
          "username": "\t\t\t\tgoodmail\t\t\t",
          "content": "There is no point to use presigned URL for that case.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 773994,
          "date": "Fri 13 Jan 2023 02:38",
          "username": "\t\t\t\tmj61\t\t\t",
          "content": "A,C, E are not as efficient or operationally efficient as the B and D:A.  Configuring the application to upload images to S3 Glacier would not reduce the coupling within the application and would not improve website performance.C.  Uploading images directly from the user's browser to S3 would not reduce the coupling within the application and could increase the load on the application server.E.  Creating an Amazon EventBridge rule that invokes an AWS Lambda function on a schedule to resize uploaded images would not be real-time and would not work well with a large number of image uploads.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 762728,
          "date": "Sat 31 Dec 2022 18:03",
          "username": "\t\t\t\tMindvision\t\t\t",
          "content": "B + D correct. <br>C incorrect<br>The presigned URLs are valid only for the specified duration. <br>When you create a presigned URL for your object, you must provide your security credentials and then specify a bucket name, an object key, an HTTP method (GET to download the object), and an expiration date and time.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 762559,
          "date": "Sat 31 Dec 2022 09:33",
          "username": "\t\t\t\tZerotn3\t\t\t",
          "content": "As chat gpt support",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: CD"
        },
        {
          "id": 757944,
          "date": "Tue 27 Dec 2022 00:42",
          "username": "\t\t\t\tSoluAWS\t\t\t",
          "content": "D is the best option as the user does not need to wait he/she will get the instant response that the image is uploaded. Once the image gets uploaded triggering the lambda function after that to resize the image (and might delete the original image and keep the resized image). This is the most efficient solution. <br>and D is the first half of the logic.<br><br>So, B & D. ",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 757193,
          "date": "Mon 26 Dec 2022 08:56",
          "username": "\t\t\t\tNV305\t\t\t",
          "content": "shld be c & d",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: CD"
        }
      ]
    },
    {
      "question_id": "#111",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company recently migrated a message processing system to AWS. The system receives messages into an ActiveMQ queue running on an Amazon EC2 instance. Messages are processed by a consumer application running on Amazon EC2. The consumer application processes the messages and writes results to a MySQL database running on Amazon EC2. The company wants this application to be highly available with low operational complexity.<br>Which architecture offers the HIGHEST availability?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#111",
          "answers": [
            {
              "choice": "<p>A. Add a second ActiveMQ server to another Availability Zone. Add an additional consumer EC2 instance in another Availability Zone. Replicate the MySQL database to another Availability Zone.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use Amazon MQ with active/standby brokers configured across two Availability Zones. Add an additional consumer EC2 instance in another Availability Zone. Replicate the MySQL database to another Availability Zone.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use Amazon MQ with active/standby brokers configured across two Availability Zones. Add an additional consumer EC2 instance in another Availability Zone. Use Amazon RDS for MySQL with Multi-AZ enabled.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use Amazon MQ with active/standby brokers configured across two Availability Zones. Add an Auto Scaling group for the consumer EC2 instances across two Availability Zones. Use Amazon RDS for MySQL with Multi-AZ enabled.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 699048,
          "date": "Wed 19 Oct 2022 14:59",
          "username": "\t\t\t\t123jhl0\t\t\t",
          "content": "Answer is D as the \\\"HIGHEST available\\\" and less \\\"operational complex\\\"<br>The \\\"Amazon RDS for MySQL with Multi-AZ enabled\\\" option excludes A and B<br>The \\\"Auto Scaling group\\\" is more available and reduces operational complexity in case of incidents (as remediation it is automated) than just adding one more instance. This excludes C. <br><br>C and D to choose from based on <br>D over C sinceis configured",
          "upvote_count": "10",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 841750,
          "date": "Fri 17 Mar 2023 08:49",
          "username": "\t\t\t\tErbug\t\t\t",
          "content": "you can find some details about Amazon MQ active/standby broker for high availability https://docs.aws.amazon.com/amazon-mq/latest/developer-guide/active-standby-broker-deployment.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 788509,
          "date": "Thu 26 Jan 2023 09:05",
          "username": "\t\t\t\tAbdel42\t\t\t",
          "content": "D as the Auto Scaling group offer the highest availability between all solutions",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 752813,
          "date": "Wed 21 Dec 2022 23:21",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option D offers the highest availability because it addresses all potential points of failure in the system:<br><br>Amazon MQ with active/standby brokers configured across two Availability Zones ensures that the message queue is available even if one Availability Zone experiences an outage.<br><br>An Auto Scaling group for the consumer EC2 instances across two Availability Zones ensures that the consumer application is able to continue processing messages even if one Availability Zone experiences an outage.<br><br>Amazon RDS for MySQL with Multi-AZ enabled ensures that the database is available even if one Availability Zone experiences an outage.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A addresses some potential points of failure, but it does not address the potential for the consumer application to become unavailable due to an Availability Zone outage. <br><br>Option B addresses some potential points of failure, but it does not address the potential for the database to become unavailable due to an Availability Zone outage. <br><br>Option C addresses some potential points of failure, but it does not address the potential for the consumer application to become unavailable due to an Availability Zone outage.</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 752814,
          "date": "Wed 21 Dec 2022 23:22",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A addresses some potential points of failure, but it does not address the potential for the consumer application to become unavailable due to an Availability Zone outage. <br><br>Option B addresses some potential points of failure, but it does not address the potential for the database to become unavailable due to an Availability Zone outage. <br><br>Option C addresses some potential points of failure, but it does not address the potential for the consumer application to become unavailable due to an Availability Zone outage.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 749163,
          "date": "Sun 18 Dec 2022 21:30",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option D",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 723856,
          "date": "Mon 21 Nov 2022 20:38",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "D is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 705475,
          "date": "Thu 27 Oct 2022 13:19",
          "username": "\t\t\t\tUWSFish\t\t\t",
          "content": "I don't know about D. Active/Standby adds to fault tolerance but does nothing for HA. <br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Fault tolerance goes up a level from HA.  Active Standby contributes to HA. </li><li>Amazon RDS &gt; MySQL, hence A and B are eliminated</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 734787,
          "date": "Sun 04 Dec 2022 02:51",
          "username": "\t\t\t\tWajif\t\t\t",
          "content": "Fault tolerance goes up a level from HA.  Active Standby contributes to HA. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 714911,
          "date": "Thu 10 Nov 2022 05:01",
          "username": "\t\t\t\tnullvoiddeath\t\t\t",
          "content": "Amazon RDS > MySQL, hence A and B are eliminated",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 704821,
          "date": "Wed 26 Oct 2022 17:32",
          "username": "\t\t\t\tSix_Fingered_Jose\t\t\t",
          "content": "agree with D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        }
      ]
    },
    {
      "question_id": "#112",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company hosts a containerized web application on a fleet of on-premises servers that process incoming requests. The number of requests is growing quickly. The on-premises servers cannot handle the increased number of requests. The company wants to move the application to AWS with minimum code changes and minimum development effort.<br>Which solution will meet these requirements with the LEAST operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#112",
          "answers": [
            {
              "choice": "<p>A. Use AWS Fargate on Amazon Elastic Container Service (Amazon ECS) to run the containerized web application with Service Auto Scaling. Use an Application Load Balancer to distribute the incoming requests.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use two Amazon EC2 instances to host the containerized web application. Use an Application Load Balancer to distribute the incoming requests.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use AWS Lambda with a new code that uses one of the supported languages. Create multiple Lambda functions to support the load. Use Amazon API Gateway as an entry point to the Lambda functions.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use a high performance computing (HPC) solution such as AWS ParallelCluster to establish an HPC cluster that can process the incoming requests at the appropriate scale.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 699063,
          "date": "Wed 19 Oct 2022 15:38",
          "username": "\t\t\t\t123jhl0\t\t\t",
          "content": "Less operational overhead means A: Fargate (no EC2), move the containers on ECS, autoscaling for growth and ALB to balance consumption.<br>B - requires configure EC2<br>C - requires add code (developpers)<br>D - seems like the most complex approach, like re-architecting the app to take advantage of an HPC platform.",
          "upvote_count": "9",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 838825,
          "date": "Tue 14 Mar 2023 13:32",
          "username": "\t\t\t\tairraid2010\t\t\t",
          "content": "AWS Fargate is a technology that you can use with Amazon ECS to run containers without having to manage servers on clusters of Amazon EC2 instances. With Fargate, you no longer have to provision, configure, or scale of virtual machines to run containers. <br><br>https://docs.aws.amazon.com/AmazonECS/latest/userguide/what-is-fargate.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 802802,
          "date": "Thu 09 Feb 2023 04:43",
          "username": "\t\t\t\tChalamalli\t\t\t",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 759152,
          "date": "Wed 28 Dec 2022 00:25",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The best solution to meet the requirements with the least operational overhead is Option A: Use AWS Fargate on Amazon Elastic Container Service (Amazon ECS) to run the containerized web application with Service Auto Scaling. Use an Application Load Balancer to distribute the incoming requests.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 749167,
          "date": "Sun 18 Dec 2022 21:35",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option A has minimum operational overhead and almost no application code changes.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 723858,
          "date": "Mon 21 Nov 2022 20:40",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 704823,
          "date": "Wed 26 Oct 2022 17:34",
          "username": "\t\t\t\tSix_Fingered_Jose\t\t\t",
          "content": "Agreed with A,<br>lambda will work too but requires more operational overhead (more chores)<br><br>with A, you are just moving from an on-prem container to AWS container",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        }
      ]
    },
    {
      "question_id": "#113",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company uses 50 TB of data for reporting. The company wants to move this data from on premises to AWS. A custom application in the company's data center runs a weekly data transformation job. The company plans to pause the application until the data transfer is complete and needs to begin the transfer process as soon as possible.<br>The data center does not have any available network bandwidth for additional workloads. A solutions architect must transfer the data and must configure the transformation job to continue to run in the AWS Cloud.<br>Which solution will meet these requirements with the LEAST operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#113",
          "answers": [
            {
              "choice": "<p>A. Use AWS DataSync to move the data. Create a custom transformation job by using AWS Glue.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Order an AWS Snowcone device to move the data. Deploy the transformation application to the device.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Order an AWS Snowball Edge Storage Optimized device. Copy the data to the device. Create a custom transformation job by using AWS Glue.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Order an AWS Snowball Edge Storage Optimized device that includes Amazon EC2 compute. Copy the data to the device. Create a new EC2 instance on AWS to run the transformation application.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 699059,
          "date": "Wed 19 Oct 2022 15:29",
          "username": "\t\t\t\t123jhl0\t\t\t",
          "content": "A.  Use AWS DataSync to move the data. Create a custom transformation job by using AWS Glue. - No BW available for DataSync, so \\\"asap\\\" will be weeks/months (?)B.  Order an AWS Snowcone device to move the data. Deploy the transformation application to the device. - Snowcone will just store 14TB (SSD configuration).<br>**C**. Order an AWS Snowball Edge Storage Optimized device. Copy the data to the device. Create a custom transformation job by using AWS Glue. - SnowBall can store 80TB (ok), takes around 1 week to move the device (faster than A), and AWS Glue allows to do ETL jobs. This is the answer.D.  Order an AWS Snowball Edge Storage Optimized device that includes Amazon EC2 compute. Copy the data to the device. Create a new EC2 instance on AWS to run the transformation application. - Same as C, but the ETL job requires the deployment/configuration/maintenance of an EC2 instance, while Glue is serverless. This means D has more operational overhead than C. <br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I disagree on D.  transformation job is already in place.so, all you have to do is deploy and run on ec2. <br>C takes more effort to build Glue process, like reinventing the wheel . this is unnecessary</li></ul>",
          "upvote_count": "27",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 795533,
          "date": "Wed 01 Feb 2023 22:26",
          "username": "\t\t\t\tremand\t\t\t",
          "content": "I disagree on D.  transformation job is already in place.so, all you have to do is deploy and run on ec2. <br>C takes more effort to build Glue process, like reinventing the wheel . this is unnecessary",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 775166,
          "date": "Sat 14 Jan 2023 08:46",
          "username": "\t\t\t\tgoodmail\t\t\t",
          "content": "Why C? This answer misses the part between SnowBall and AWS Glue. <br>D at least provides a full-step solution that copies data in snowball device, and installs the custom application in device's EC2 to do the transformation job.",
          "upvote_count": "6",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 848620,
          "date": "Thu 23 Mar 2023 20:47",
          "username": "\t\t\t\tBang3R\t\t\t",
          "content": "C has less operational overhead than D.  Managing EC2 has higher operational overhead than serverless AWS Glue",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 830822,
          "date": "Mon 06 Mar 2023 14:07",
          "username": "\t\t\t\tStuMoz\t\t\t",
          "content": "I was originally going to vote for C, however it is D because of 2 reasons. 1) AWS love to promote their own products, so Glue is most likely and 2) because Glue presents the least operational overhead moving forward as it is serverless unlike an EC2 instance which requires patching, feeding and watering",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 830463,
          "date": "Mon 06 Mar 2023 01:49",
          "username": "\t\t\t\tDody\t\t\t",
          "content": "Using the EC2 instance created on the Snowball Edge for the transformation job will do it once , However the solution architectmust configure the transformation job to continue to run in the AWS Cloud so it's AWS Glue",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 820118,
          "date": "Fri 24 Feb 2023 05:51",
          "username": "\t\t\t\tAlmeroSenior\t\t\t",
          "content": "Lets not forget that even a compute optimized Snowball cannot run Glue . Basically a NAS with S3 and EC2 is what you get so cant be C ( unless you run storage on prem and Glue in cloud with a dx/vpn )",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 818757,
          "date": "Thu 23 Feb 2023 04:38",
          "username": "\t\t\t\thabibi03336\t\t\t",
          "content": "Is it possible to use AWS Glue service on snowball edge?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 795535,
          "date": "Wed 01 Feb 2023 22:27",
          "username": "\t\t\t\tremand\t\t\t",
          "content": "perfect fit is D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 791756,
          "date": "Sun 29 Jan 2023 16:09",
          "username": "\t\t\t\tG3\t\t\t",
          "content": ".... and the AI maven says : <br><br>A solution that would meet these requirements with the least operational overhead is to use AWS Snowball Edge. Snowball Edge is a data transfer device that can transfer large amounts of data into and out of the AWS cloud with minimal network bandwidth requirements. Additionally, Snowball Edge can run custom scripts on the device, so the transformation job can be configured to continue running during the transfer. Once the transfer is complete, the data can be loaded into an AWS storage service such as Amazon S3. This solution would minimize operational overhead by allowing for a parallel transfer and processing of data, rather than requiring the application to be paused.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 763644,
          "date": "Mon 02 Jan 2023 11:30",
          "username": "\t\t\t\taba2s\t\t\t",
          "content": "Option B is incorrect. Although you can use AWS DataSync to automate and accelerate data transfer from on-premises to AWS storage services, it's not capable of replicating existing applications running on your server.<br>Option B is incorrect as AWS Snowcone supports data collection and data processing using AWS compute services but supports only 8 TB of HDD-based hard disk. It's not the best option for transferring 50 TB of data, as it will require multiple iterations of offline data transfer. <br>I will go for C as it seem to have less operational overhead.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 759547,
          "date": "Wed 28 Dec 2022 09:04",
          "username": "\t\t\t\tNV305\t\t\t",
          "content": "c only<br>Glue is serverless. This means D has more operational overhead than C. ",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 754019,
          "date": "Fri 23 Dec 2022 09:13",
          "username": "\t\t\t\tDavidNamy\t\t\t",
          "content": "Option C involves using AWS Lambda to process the photos and storing the photos in Amazon S3, which can handle a large amount of data and scale to meet the needs of the growing user base. Retaining DynamoDB to store the metadata allows the application to continue to use a fast and highly available database for this purpose.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 754013,
          "date": "Fri 23 Dec 2022 09:09",
          "username": "\t\t\t\tDavidNamy\t\t\t",
          "content": "Option D, ordering an AWS Snowball Edge Storage Optimized device that includes Amazon EC2 compute, is the most efficient solution because it allows you to both transfer the data and run the transformation application on the same device, reducing the operational overhead required.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 752823,
          "date": "Wed 21 Dec 2022 23:36",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The solution that will meet these requirements with the least operational overhead is Option D: (Order an AWS Snowball Edge Storage Optimized device that includes Amazon EC2 compute. Copy the data to the device. Create a new EC2 instance on AWS to run the transformation application.)<br><br>AWS Snowball Edge Storage Optimized devices are used to transfer large amounts of data quickly and securely to and from the cloud. They come with onboard storage and compute capabilities, which allows you to perform data processing tasks on the device itself before transferring the data to the cloud. This means that you can copy the data to the device and then use the device's computing capabilities to run the transformation application directly on the device, without having to pause the application or transfer it to the cloud.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A, using AWS DataSync to move the data and creating a custom transformation job using AWS Glue, would require more operational overhead as it involves setting up and configuring multiple services. <br><br>Option B, ordering an AWS Snowcone device and deploying the transformation applied to the device, would also involve setting up and configuring multiple services and may not have sufficient computing capabilities to run the transformation application. <br><br>Option C, ordering an AWS Snowball Edge Storage Optimized device and creating a custom transformation job using AWS Glue, would involve setting up and configuring multiple services and would not have the onboard compute capabilities to run the transformation application directly on the device.</li></ul>",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 752824,
          "date": "Wed 21 Dec 2022 23:37",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A, using AWS DataSync to move the data and creating a custom transformation job using AWS Glue, would require more operational overhead as it involves setting up and configuring multiple services. <br><br>Option B, ordering an AWS Snowcone device and deploying the transformation applied to the device, would also involve setting up and configuring multiple services and may not have sufficient computing capabilities to run the transformation application. <br><br>Option C, ordering an AWS Snowball Edge Storage Optimized device and creating a custom transformation job using AWS Glue, would involve setting up and configuring multiple services and would not have the onboard compute capabilities to run the transformation application directly on the device.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 749171,
          "date": "Sun 18 Dec 2022 21:42",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option D is right as there is a need to copy and transfer the customer job also along with Data. Option C may not work as it requires custom job that needs to be re-written. So fastest and least operational overhead for migration is D only.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 743346,
          "date": "Mon 12 Dec 2022 22:17",
          "username": "\t\t\t\t[Removed]\t\t\t",
          "content": "A, B are obviously to be crossed out as others have mentioned. <br><br>I choose D as they have a custom application that runs data transformation so it would be simplest to just install it on Snowball Edge which comes with an EC2.<br><br>They have a custom transformation application, hence I think using AWS Glue is not a good choice.You would need to tweak AWS Glue to do the job like their custom application ( more operational overhead).",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 731883,
          "date": "Wed 30 Nov 2022 20:03",
          "username": "\t\t\t\tmj98\t\t\t",
          "content": "I would say D because they have a custom application?",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        }
      ]
    },
    {
      "question_id": "#114",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has created an image analysis application in which users can upload photos and add photo frames to their images. The users upload images and metadata to indicate which photo frames they want to add to their images. The application uses a single Amazon EC2 instance and Amazon DynamoDB to store the metadata.<br>The application is becoming more popular, and the number of users is increasing. The company expects the number of concurrent users to vary significantly depending on the time of day and day of week. The company must ensure that the application can scale to meet the needs of the growing user base.<br>Which solution meats these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#114",
          "answers": [
            {
              "choice": "<p>A. Use AWS Lambda to process the photos. Store the photos and metadata in DynamoDB. <br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use Amazon Kinesis Data Firehose to process the photos and to store the photos and metadata.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use AWS Lambda to process the photos. Store the photos in Amazon S3. Retain DynamoDB to store the metadata.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Increase the number of EC2 instances to three. Use Provisioned IOPS SSD (io2) Amazon Elastic Block Store (Amazon EBS) volumes to store the photos and metadata.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 692358,
          "date": "Tue 11 Oct 2022 21:19",
          "username": "\t\t\t\tMXB05\t\t\t",
          "content": "Do not store images in databases ;)... correct answer should be C",
          "upvote_count": "19",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 834981,
          "date": "Fri 10 Mar 2023 13:44",
          "username": "\t\t\t\trdss11\t\t\t",
          "content": "C is the answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 828434,
          "date": "Fri 03 Mar 2023 22:29",
          "username": "\t\t\t\tSdraju\t\t\t",
          "content": "most optimal solution",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 763649,
          "date": "Mon 02 Jan 2023 11:45",
          "username": "\t\t\t\taba2s\t\t\t",
          "content": "Have look in that discution https://www.quora.com/How-can-I-use-DynamoDB-for-storing-metadata-for-Amazon-S3-objects",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 754023,
          "date": "Fri 23 Dec 2022 09:20",
          "username": "\t\t\t\tDavidNamy\t\t\t",
          "content": "Option C involves using AWS Lambda to process the photos and storing the photos in Amazon S3, which can handle a large amount of data and scale to meet the needs of the growing user base. Retaining DynamoDB to store the metadata allows the application to continue to use a fast and highly available database for this purpose.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 754021,
          "date": "Fri 23 Dec 2022 09:18",
          "username": "\t\t\t\tDavidNamy\t\t\t",
          "content": "According to the well-designed framework, option C is the safest and most efficient option.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 753036,
          "date": "Thu 22 Dec 2022 07:48",
          "username": "\t\t\t\ta070112\t\t\t",
          "content": "Static content, C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 752827,
          "date": "Wed 21 Dec 2022 23:44",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "C.  Use AWS Lambda to process the photos. Store the photos in Amazon S3. Retain DynamoDB to store the metadata.<br><br>This solution meets the requirements because it uses AWS Lambda to process the photos, which can automatically scale to meet the needs of the growing user base. The photos can be stored in Amazon S3, which is a highly scalable and durable object storage service. DynamoDB can be retained to store the metadata, which can also scale to meet the needs of the growing user base. This solution allows the application to scale to meet the needs of the growing user base, while also ensuring that the photos and metadata are stored in a scalable and durable manner.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 749177,
          "date": "Sun 18 Dec 2022 21:46",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 737379,
          "date": "Wed 07 Dec 2022 03:51",
          "username": "\t\t\t\tlighrz\t\t\t",
          "content": "photo needs to be on S3",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 727096,
          "date": "Fri 25 Nov 2022 22:32",
          "username": "\t\t\t\trewdboy\t\t\t",
          "content": "C for sure<br>I was originally leaning toward A because it seemed like a simpler setup to keep the images and metadata in the same service, but DynamoDB has a record limit of 64KB, so S3 would be better for image storage and then DynamoDB for metadata",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 723861,
          "date": "Mon 21 Nov 2022 20:43",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 717820,
          "date": "Mon 14 Nov 2022 09:49",
          "username": "\t\t\t\tPamban\t\t\t",
          "content": "photo needs to be on S3",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 716057,
          "date": "Fri 11 Nov 2022 13:54",
          "username": "\t\t\t\tmabotega\t\t\t",
          "content": "photos should be stored on S3",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 704836,
          "date": "Wed 26 Oct 2022 17:48",
          "username": "\t\t\t\tSix_Fingered_Jose\t\t\t",
          "content": "agree with C,<br>Storing image in DB wont be very scalable compared to S3<br>metadata does not take up much space and is more efficiently stored in DB",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 702797,
          "date": "Mon 24 Oct 2022 09:47",
          "username": "\t\t\t\ttubtab\t\t\t",
          "content": "cccccccccc",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        }
      ]
    },
    {
      "question_id": "#115",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A medical records company is hosting an application on Amazon EC2 instances. The application processes customer data files that are stored on Amazon S3. The EC2 instances are hosted in public subnets. The EC2 instances access Amazon S3 over the internet, but they do not require any other network access.<br>A new requirement mandates that the network traffic for file transfers take a private route and not be sent over the internet.<br>Which change to the network architecture should a solutions architect recommend to meet this requirement?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#115",
          "answers": [
            {
              "choice": "<p>A. Create a NAT gateway. Configure the route table for the public subnets to send traffic to Amazon S3 through the NAT gateway.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Configure the security group for the EC2 instances to restrict outbound traffic so that only traffic to the S3 prefix list is permitted.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Move the EC2 instances to private subnets. Create a VPC endpoint for Amazon S3, and link the endpoint to the route table for the private subnets.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Remove the internet gateway from the VPC.  Set up an AWS Direct Connect connection, and route traffic to Amazon S3 over the Direct Connect connection.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 754022,
          "date": "Fri 23 Dec 2022 09:20",
          "username": "\t\t\t\tDavidNamy\t\t\t",
          "content": "According to the well-designed framework, option C is the safest and most efficient option.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 752849,
          "date": "Thu 22 Dec 2022 00:19",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The correct answer is C.  Move the EC2 instances to private subnets. Create a VPC endpoint for Amazon S3, and link the endpoint to the route table for the private subnets.<br><br>To meet the new requirement of transferring files over a private route, the EC2 instances should be moved to private subnets, which do not have direct access to the internet. This ensures that the traffic for file transfers does not go over the internet.<br><br>To enable the EC2 instances to access Amazon S3, a VPC endpoint for Amazon S3 can be created. VPC endpoints allow resources within a VPC to communicate with resources in other services without the traffic being sent over the internet. By linking the VPC endpoint to the route table for the private subnets, the EC2 instances can access Amazon S3 over a private connection within the VPC. <br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A (Create a NAT gateway) would not work, as a NAT gateway is used to allow resources in private subnets to access the internet, while the requirement is to prevent traffic from going over the internet.<br><br>Option B (Configure the security group for the EC2 instances to restrict outbound traffic) would not achieve the goal of routing traffic over a private connection, as the traffic would still be sent over the internet.<br><br>Option D (Remove the internet gateway from the VPC and set up an AWS Direct Connect connection) would not be necessary, as the requirement can be met by simply creating a VPC endpoint for Amazon S3 and routing traffic through it.</li><li>How about the question of moving the instances across subnets. Because according to AWS you can't do it. https://aws.amazon.com/premiumsupport/knowledge-center/move-ec2-instance/#:~:text=It%27s%20not%20possible%20to%20move,%2C%20Availability%20Zone%2C%20or%20VPC. <br>Kindly clarify. Maybe I miss something.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 752851,
          "date": "Thu 22 Dec 2022 00:20",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A (Create a NAT gateway) would not work, as a NAT gateway is used to allow resources in private subnets to access the internet, while the requirement is to prevent traffic from going over the internet.<br><br>Option B (Configure the security group for the EC2 instances to restrict outbound traffic) would not achieve the goal of routing traffic over a private connection, as the traffic would still be sent over the internet.<br><br>Option D (Remove the internet gateway from the VPC and set up an AWS Direct Connect connection) would not be necessary, as the requirement can be met by simply creating a VPC endpoint for Amazon S3 and routing traffic through it.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>How about the question of moving the instances across subnets. Because according to AWS you can't do it. https://aws.amazon.com/premiumsupport/knowledge-center/move-ec2-instance/#:~:text=It%27s%20not%20possible%20to%20move,%2C%20Availability%20Zone%2C%20or%20VPC. <br>Kindly clarify. Maybe I miss something.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 771502,
          "date": "Tue 10 Jan 2023 15:20",
          "username": "\t\t\t\tKayamables\t\t\t",
          "content": "How about the question of moving the instances across subnets. Because according to AWS you can't do it. https://aws.amazon.com/premiumsupport/knowledge-center/move-ec2-instance/#:~:text=It%27s%20not%20possible%20to%20move,%2C%20Availability%20Zone%2C%20or%20VPC. <br>Kindly clarify. Maybe I miss something.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 749180,
          "date": "Sun 18 Dec 2022 21:48",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 728120,
          "date": "Sun 27 Nov 2022 11:05",
          "username": "\t\t\t\tocbn3wby\t\t\t",
          "content": "C is correct. <br>There is no requirement for public access from internet. <br><br>Application must be moved in Private subnet. This is a prerequisite in using VPC endpoints with S3<br>https://aws.amazon.com/blogs/storage/managing-amazon-s3-access-with-vpc-endpoints-and-s3-access-points/",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 723862,
          "date": "Mon 21 Nov 2022 20:45",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 717723,
          "date": "Mon 14 Nov 2022 06:38",
          "username": "\t\t\t\tJtic\t\t\t",
          "content": "Use VPC endpoint",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 717489,
          "date": "Sun 13 Nov 2022 20:54",
          "username": "\t\t\t\tJtic\t\t\t",
          "content": "User VPC endpoint and make the EC2 private<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Use VPC endpoint</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 717490,
          "date": "Sun 13 Nov 2022 20:54",
          "username": "\t\t\t\tJtic\t\t\t",
          "content": "Use VPC endpoint",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 711940,
          "date": "Sat 05 Nov 2022 19:15",
          "username": "\t\t\t\tbackbencher2022\t\t\t",
          "content": "VPC endpoint is the best choice to route S3 traffic without traversing internet. Option A alone can't be used as NAT Gateway requires an Internet gateway for outbound internet traffic. Option B would still require traversing through internet and option D is also not a suitable solution",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        }
      ]
    },
    {
      "question_id": "#116",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company uses a popular content management system (CMS) for its corporate website. However, the required patching and maintenance are burdensome. The company is redesigning its website and wants anew solution. The website will be updated four times a year and does not need to have any dynamic content available. The solution must provide high scalability and enhanced security.<br>Which combination of changes will meet these requirements with the LEAST operational overhead? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: AD</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#116",
          "answers": [
            {
              "choice": "<p>A. Configure Amazon CloudFront in front of the website to use HTTPS functionality.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Deploy an AWS WAF web ACL in front of the website to provide HTTPS functionality.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create and deploy an AWS Lambda function to manage and serve the website content.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create the new website and an Amazon S3 bucket. Deploy the website on the S3 bucket with static website hosting enabled.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. Create the new website. Deploy the website by using an Auto Scaling group of Amazon EC2 instances behind an Application Load Balancer.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 700270,
          "date": "Thu 20 Oct 2022 22:09",
          "username": "\t\t\t\tpalermo777\t\t\t",
          "content": "A -> We can configure CloudFront to require HTTPS from clients (enhanced security) https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html<br>D -> storing static website on S3 provides scalability and less operational overhead, then configuration of Application LB and EC2 instances (hence E is out)<br><br>B is out since AWS WAF Web ACL does not to provide HTTPS functionality, but to protect HTTPS only.",
          "upvote_count": "19",
          "selected_answers": ""
        },
        {
          "id": 842001,
          "date": "Fri 17 Mar 2023 14:21",
          "username": "\t\t\t\tErbug\t\t\t",
          "content": "Since Amazon S3 is unlimited and you pay as you go so it means there will be no limit to scale as long as your data is going to grow, so D is one of the correct answers and another correct answer is A, because of this: https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html<br><br>so my answer is AD. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 819760,
          "date": "Thu 23 Feb 2023 22:11",
          "username": "\t\t\t\tManOnTheMoon\t\t\t",
          "content": "I vote A & C for the reason being least operational overhead.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 806077,
          "date": "Sun 12 Feb 2023 08:43",
          "username": "\t\t\t\tYelizaveta\t\t\t",
          "content": "Here a perfect explanation: <br>https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 788807,
          "date": "Thu 26 Jan 2023 15:32",
          "username": "\t\t\t\tAbdel42\t\t\t",
          "content": "Simple and secure",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 779090,
          "date": "Tue 17 Jan 2023 17:56",
          "username": "\t\t\t\tremand\t\t\t",
          "content": "D.  Create the new website and an Amazon S3 bucket. Deploy the website on the S3 bucket with static website hosting enabled.A.  Configure Amazon CloudFront in front of the website to use HTTPS functionality.<br><br>By deploying the website on an S3 bucket with static website hosting enabled, the company can take advantage of the high scalability and cost-efficiency of S3 while also reducing the operational overhead of managing and patching a CMS.<br>By configuring Amazon CloudFront in front of the website, it will automatically handle the HTTPS functionality, this way the company can have a secure website with very low operational overhead.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 752863,
          "date": "Thu 22 Dec 2022 00:29",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "KEYWORD: LEAST operational overhead<br>D.  Create the new website and an Amazon S3 bucket. Deploy the website on the S3 bucket with static website hosting enabled.<br>C.  Create and deploy an AWS Lambda function to manage and serve the website content.<br><br>Option D (using Amazon S3 with static website hosting) would provide high scalability and enhanced security with minimal operational overhead because it requires little maintenance and can automatically scale to meet increased demand.<br><br>Option C (using an AWS Lambda function) would also provide high scalability and enhanced security with minimal operational overhead. AWS Lambda is a serverless compute service that runs your code in response to events and automatically scales to meet demand. It is easy to set up and requires minimal maintenance.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Why other options are not correct?<br><br>Option A (using Amazon CloudFront) and Option B (using an AWS WAF web ACL) would provide HTTPS functionality but would require additional configuration and maintenance to ensure that they are set up correctly and remain secure.<br><br>Option E (using an Auto Scaling group of Amazon EC2 instances behind an Application Load Balancer) would provide high scalability, but it would require more operational overhead because it involves managing and maintaining EC2 instances.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: CD"
        },
        {
          "id": 752865,
          "date": "Thu 22 Dec 2022 00:31",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Why other options are not correct?<br><br>Option A (using Amazon CloudFront) and Option B (using an AWS WAF web ACL) would provide HTTPS functionality but would require additional configuration and maintenance to ensure that they are set up correctly and remain secure.<br><br>Option E (using an Auto Scaling group of Amazon EC2 instances behind an Application Load Balancer) would provide high scalability, but it would require more operational overhead because it involves managing and maintaining EC2 instances.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 749182,
          "date": "Sun 18 Dec 2022 21:51",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "A and D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 741452,
          "date": "Sun 11 Dec 2022 07:03",
          "username": "\t\t\t\tAlaN652\t\t\t",
          "content": "A: for high availability and security through cloudfront HTTPS<br>D: Scalable storge solution and support of static hosting",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 723863,
          "date": "Mon 21 Nov 2022 20:47",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "A and D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 714448,
          "date": "Wed 09 Nov 2022 11:00",
          "username": "\t\t\t\tPS_R\t\t\t",
          "content": "Cloudfront can do the WAF part so i chose A and D",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 714015,
          "date": "Tue 08 Nov 2022 19:02",
          "username": "\t\t\t\tBevemo\t\t\t",
          "content": "Initially I thought B) WAF for HTTP to HTTPS redirect, but then I found CloudFront can do it so A) adds performance/scale and security. https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 705749,
          "date": "Thu 27 Oct 2022 18:58",
          "username": "\t\t\t\tManoAni\t\t\t",
          "content": "For enhanced security B, and they mentioned patching is burdensome so if its E, then they must patch the EC2 instances. So hosting in S3 is ideal as it is static content.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 704842,
          "date": "Wed 26 Oct 2022 17:54",
          "username": "\t\t\t\tSix_Fingered_Jose\t\t\t",
          "content": "agree with A and D<br><br>static website -> obviously S3, and S3 is super scalable<br>CDN -> CloudFront obviously as well, and with HTTPS security is enhanced.<br><br>B does not make sense because you are not replacing the CDN with anything,<br>E works too but takes too much effort and compared to S3, S3 still wins in term of scalability. plus why use EC2 when you are only hosting static website",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 699671,
          "date": "Thu 20 Oct 2022 10:00",
          "username": "\t\t\t\trob74\t\t\t",
          "content": ". The solution must provide high scalability and enhanced security<br>AWS WAF--> For enhanced security<br>high scalability -->behind an Application Load Balancer.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Please provide informed answers. You are truly correct, but in this case, there is no specific need to host the website/cms on EC2 + ALB.  <br><br>It only requires static website - which can be achieved with scalable S3.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BE"
        },
        {
          "id": 728123,
          "date": "Sun 27 Nov 2022 11:13",
          "username": "\t\t\t\tocbn3wby\t\t\t",
          "content": "Please provide informed answers. You are truly correct, but in this case, there is no specific need to host the website/cms on EC2 + ALB.  <br><br>It only requires static website - which can be achieved with scalable S3.",
          "upvote_count": "4",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#117",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company stores its application logs in an Amazon CloudWatch Logs log group. A new policy requires the company to store all application logs in Amazon OpenSearch Service (Amazon Elasticsearch Service) in near-real time.<br>Which solution will meet this requirement with the LEAST operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#117",
          "answers": [
            {
              "choice": "<p>A. Configure a CloudWatch Logs subscription to stream the logs to Amazon OpenSearch Service (Amazon Elasticsearch Service).<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an AWS Lambda function. Use the log group to invoke the function to write the logs to Amazon OpenSearch Service (Amazon Elasticsearch Service).<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an Amazon Kinesis Data Firehose delivery stream. Configure the log group as the delivery streams sources. Configure Amazon OpenSearch Service (Amazon Elasticsearch Service) as the delivery stream's destination.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Install and configure Amazon Kinesis Agent on each application server to deliver the logs to Amazon Kinesis Data Streams. Configure Kinesis Data Streams to deliver the logs to Amazon OpenSearch Service (Amazon Elasticsearch Service).<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 704843,
          "date": "Wed 26 Oct 2022 17:56",
          "username": "\t\t\t\tSix_Fingered_Jose\t\t\t",
          "content": "answer is A<br>https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_OpenSearch_Stream.html<br><br>> You can configure a CloudWatch Logs log group to stream data it receives to your Amazon OpenSearch Service cluster in NEAR REAL-TIME through a CloudWatch Logs subscription<br><br>least overhead compared to kinesis<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A (Configure a CloudWatch Logs subscription to stream the logs to Amazon OpenSearch Service (Amazon Elasticsearch Service)) is not a suitable option, as a CloudWatch Logs subscription is designed to send log events to a destination such as an Amazon Simple Notification Service (Amazon SNS) topic or an AWS Lambda function. It is not designed to write logs directly to Amazon Elasticsearch Service (Amazon ES).</li><li>that is not true, you can stream logs from CloudWatch Logs directly to OpenSearch</li><li>Zerotn3 is right! There should be a Lambda for writing into ES</li><li>Great link. Convinced me</li></ul>",
          "upvote_count": "39",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 762590,
          "date": "Sat 31 Dec 2022 10:53",
          "username": "\t\t\t\tZerotn3\t\t\t",
          "content": "Option A (Configure a CloudWatch Logs subscription to stream the logs to Amazon OpenSearch Service (Amazon Elasticsearch Service)) is not a suitable option, as a CloudWatch Logs subscription is designed to send log events to a destination such as an Amazon Simple Notification Service (Amazon SNS) topic or an AWS Lambda function. It is not designed to write logs directly to Amazon Elasticsearch Service (Amazon ES).<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>that is not true, you can stream logs from CloudWatch Logs directly to OpenSearch</li></ul>",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 808522,
          "date": "Tue 14 Feb 2023 16:31",
          "username": "\t\t\t\tkucyk\t\t\t",
          "content": "that is not true, you can stream logs from CloudWatch Logs directly to OpenSearch",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 763744,
          "date": "Mon 02 Jan 2023 14:32",
          "username": "\t\t\t\tHayLLlHuK\t\t\t",
          "content": "Zerotn3 is right! There should be a Lambda for writing into ES",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 705585,
          "date": "Thu 27 Oct 2022 15:21",
          "username": "\t\t\t\tUWSFish\t\t\t",
          "content": "Great link. Convinced me",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 752876,
          "date": "Thu 22 Dec 2022 00:47",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The correct answer is C: Create an Amazon Kinesis Data Firehose delivery stream. Configure the log group as the delivery stream source. Configure Amazon OpenSearch Service (Amazon Elasticsearch Service) as the delivery stream's destination.<br><br>This solution uses Amazon Kinesis Data Firehose, which is a fully managed service for streaming data to Amazon OpenSearch Service (Amazon Elasticsearch Service) and other destinations. You can configure the log group as the source of the delivery stream and Amazon OpenSearch Service as the destination. This solution requires minimal operational overhead, as Kinesis Data Firehose automatically scales and handles data delivery, transformation, and indexing.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A: Configure a CloudWatch Logs subscription to stream the logs to Amazon OpenSearch Service (Amazon Elasticsearch Service) would also work, but it may require more operational overhead as you would need to set up and manage the subscription and ensure that the logs are delivered in near-real time.<br><br>Option B: Create an AWS Lambda function. Use the log group to invoke the function to write the logs to Amazon OpenSearch Service (Amazon Elasticsearch Service) would also work, but it may require more operational overhead as you would need to set up and manage the Lambda function and ensure that it scales to handle the incoming logs.<br><br>Option D: Install and configure Amazon Kinesis Agent on each application server to deliver the logs to Amazon Kinesis Data Streams. Configure Kinesis Data Streams to deliver the logs to Amazon OpenSearch Service (Amazon Elasticsearch Service) would also work, but it may require more operational overhead as you would need to install and configure the Kinesis Agent on each application server and set up and manage the Kinesis Data Streams.</li><li>https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_OpenSearch_Stream.html</li></ul>",
          "upvote_count": "7",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 752878,
          "date": "Thu 22 Dec 2022 00:47",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A: Configure a CloudWatch Logs subscription to stream the logs to Amazon OpenSearch Service (Amazon Elasticsearch Service) would also work, but it may require more operational overhead as you would need to set up and manage the subscription and ensure that the logs are delivered in near-real time.<br><br>Option B: Create an AWS Lambda function. Use the log group to invoke the function to write the logs to Amazon OpenSearch Service (Amazon Elasticsearch Service) would also work, but it may require more operational overhead as you would need to set up and manage the Lambda function and ensure that it scales to handle the incoming logs.<br><br>Option D: Install and configure Amazon Kinesis Agent on each application server to deliver the logs to Amazon Kinesis Data Streams. Configure Kinesis Data Streams to deliver the logs to Amazon OpenSearch Service (Amazon Elasticsearch Service) would also work, but it may require more operational overhead as you would need to install and configure the Kinesis Agent on each application server and set up and manage the Kinesis Data Streams.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_OpenSearch_Stream.html</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 791561,
          "date": "Sun 29 Jan 2023 11:52",
          "username": "\t\t\t\tocbn3wby\t\t\t",
          "content": "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_OpenSearch_Stream.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 811989,
          "date": "Fri 17 Feb 2023 15:39",
          "username": "\t\t\t\tAlhaz\t\t\t",
          "content": "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_OpenSearch_Stream.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 796509,
          "date": "Fri 03 Feb 2023 00:06",
          "username": "\t\t\t\timisioluwa\t\t\t",
          "content": "The correct answer remains A.  Kindly check the link for a confirmation. https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_OpenSearch_Stream.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 791562,
          "date": "Sun 29 Jan 2023 11:52",
          "username": "\t\t\t\tocbn3wby\t\t\t",
          "content": "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_OpenSearch_Stream.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 784617,
          "date": "Sun 22 Jan 2023 20:10",
          "username": "\t\t\t\tbullrem\t\t\t",
          "content": "Option C (Create an Amazon Kinesis Data Firehose delivery stream. Configure the log group as the delivery stream's sources. Configure Amazon OpenSearch Service (Amazon Elasticsearch Service) as the delivery stream's destination) would be the best option as it allows to easily and securely stream logs from CloudWatch Logs to Amazon Elasticsearch Service in near-real time with minimal operational overhead. Data Firehose is designed specifically for data stream processing and can automatically handle tasks such as data transformation, data validation, and data loading, simplifying the process of sending logs to Amazon Elasticsearch Service.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 779092,
          "date": "Tue 17 Jan 2023 17:58",
          "username": "\t\t\t\tremand\t\t\t",
          "content": "A.  Configure a CloudWatch Logs subscription to stream the logs to Amazon OpenSearch Service (Amazon Elasticsearch Service).<br><br>This solution meets the requirement of storing all application logs in Amazon OpenSearch Service (Amazon Elasticsearch Service) with the least operational overhead. A CloudWatch Logs subscription allows you to automatically stream logs from CloudWatch Logs to a destination such as Elasticsearch Service, Kinesis Data Streams, or Lambda without the need for additional configurations and management.<br>It eliminates the need for additional infrastructure, Lambda functions and configurations, or separate agents to handle the logs transfer to Elasticsearch Service.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 778635,
          "date": "Tue 17 Jan 2023 07:57",
          "username": "\t\t\t\tChan1509\t\t\t",
          "content": "Answer : A <br>Based on Keywords and Documentation : A is the Answer <br>You can configure a CloudWatch Logs log group to stream data it receives to your Amazon OpenSearch Service cluster in \\\"near real-time through a CloudWatch Logs subscription\\\"<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>But CloudWatch Logs log group does NOT support store(write) performance. It just stream data to Amazon OpenSearch Service.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 802135,
          "date": "Wed 08 Feb 2023 15:48",
          "username": "\t\t\t\tJiyuKim\t\t\t",
          "content": "But CloudWatch Logs log group does NOT support store(write) performance. It just stream data to Amazon OpenSearch Service.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 774909,
          "date": "Fri 13 Jan 2023 23:52",
          "username": "\t\t\t\timisioluwa\t\t\t",
          "content": "The answer is C.  The \\\" in near-real time\\\" makes it more accurate and least operational overhead.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 763733,
          "date": "Mon 02 Jan 2023 14:11",
          "username": "\t\t\t\tgustavtd\t\t\t",
          "content": "No doubt C will work, but seems A is cheaper",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 762591,
          "date": "Sat 31 Dec 2022 10:56",
          "username": "\t\t\t\tZerotn3\t\t\t",
          "content": "Option A (Configure a CloudWatch Logs subscription to stream the logs to Amazon OpenSearch Service (Amazon Elasticsearch Service)) is not a suitable option, as a CloudWatch Logs subscription is designed to send log events to a destination such as an Amazon Simple Notification Service (Amazon SNS) topic or an AWS Lambda function. It is not designed to write logs directly to Amazon Elasticsearch Service (Amazon ES).<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>You're totally right</li></ul>",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 763745,
          "date": "Mon 02 Jan 2023 14:33",
          "username": "\t\t\t\tHayLLlHuK\t\t\t",
          "content": "You're totally right",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 757961,
          "date": "Tue 27 Dec 2022 01:04",
          "username": "\t\t\t\tSoluAWS\t\t\t",
          "content": "LEAST Operational Overhead \\\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_OpenSearch_Stream.html\\\"<br><br>Answer: A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 754150,
          "date": "Fri 23 Dec 2022 12:31",
          "username": "\t\t\t\tduriselvan\t\t\t",
          "content": "Ans c is correctnote :- Kinesis Data Firehose (Near real-time (buffer time min. 60 sec))",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 749186,
          "date": "Sun 18 Dec 2022 21:58",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option A has least amount of changes needed to achieve this. <br>But D is also possible would be better long term solution as it will avoid the duplication of the logs going into Cloudwatch and then moving to opensearch.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 715712,
          "date": "Fri 11 Nov 2022 03:41",
          "username": "\t\t\t\tstudy_aws1\t\t\t",
          "content": "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Subscriptions.html<br><br>You'll need to have destination arn (not mentioned under option A) - either Lambda or Kinesis Firehose.<br><br>The Amazon Resource Name (ARN) of the Kinesis stream, Kinesis Data Firehose stream, or Lambda function you want to use as the destination of the subscription feed.<br><br>Option B) does not mention the Subscription Filter. Looks more towards Option C)",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 707600,
          "date": "Sun 30 Oct 2022 05:39",
          "username": "\t\t\t\tSimonPark\t\t\t",
          "content": "You can configure a CloudWatch Logs log group to stream data it receives to your Amazon OpenSearch Service cluster in near real-time through a CloudWatch Logs subscription.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 704042,
          "date": "Tue 25 Oct 2022 18:59",
          "username": "\t\t\t\tManoAni\t\t\t",
          "content": "They mentioned near real time<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>A is also near real time. plus A is least operational overhead</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 731894,
          "date": "Wed 30 Nov 2022 20:09",
          "username": "\t\t\t\tmj98\t\t\t",
          "content": "A is also near real time. plus A is least operational overhead",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#118",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is building a web-based application running on Amazon EC2 instances in multiple Availability Zones. The web application will provide access to a repository of text documents totaling about 900 TB in size. The company anticipates that the web application will experience periods of high demand. A solutions architect must ensure that the storage component for the text documents can scale to meet the demand of the application at all times. The company is concerned about the overall cost of the solution.<br>Which storage solution meets these requirements MOST cost-effectively?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#118",
          "answers": [
            {
              "choice": "<p>A. Amazon Elastic Block Store (Amazon EBS)<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Amazon Elastic File System (Amazon EFS)<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Amazon OpenSearch Service (Amazon Elasticsearch Service)<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Amazon S3<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 849130,
          "date": "Fri 24 Mar 2023 09:56",
          "username": "\t\t\t\tfrenzoid\t\t\t",
          "content": "I wonder why people choose S3, yet S3 max capacity is 5TB .<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>My bad, the 5TB limit is for individual files. S3 has virtually unlimited storage capacity.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 849133,
          "date": "Fri 24 Mar 2023 09:58",
          "username": "\t\t\t\tfrenzoid\t\t\t",
          "content": "My bad, the 5TB limit is for individual files. S3 has virtually unlimited storage capacity.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 814276,
          "date": "Sun 19 Feb 2023 17:45",
          "username": "\t\t\t\tHelp2023\t\t\t",
          "content": "A.  It is Not a block storage B.  It is Not a file storage C.  Opensearch is useful but can only accommodate up to 600TiB and is mainly for search and anaytics. D.  S3 is more cost effective than all and can handle all objects like Block, File or Text.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 779095,
          "date": "Tue 17 Jan 2023 18:01",
          "username": "\t\t\t\tremand\t\t\t",
          "content": "D.  Amazon S3<br><br>Amazon S3 is an object storage service that can store and retrieve large amounts of data at any time, from anywhere on the web. It is designed for high durability, scalability, and cost-effectiveness, making it a suitable choice for storing a large repository of text documents. With S3, you can store and retrieve any amount of data, at any time, from anywhere on the web, and you can scale your storage up or down as needed, which will help to meet the demand of the web application. Additionally, S3 allows you to choose between different storage classes, such as standard, infrequent access, and archive, which will enable you to optimize costs based on your specific use case.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 769994,
          "date": "Mon 09 Jan 2023 03:38",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "The most cost-effective storage solution for a web application that needs to scale to meet high demand and store a large repository of text documents would be Amazon S3. Amazon S3 is an object storage service that is designed for durability, availability, and scalability. It can store and retrieve any amount of data from anywhere on the internet, making it a suitable choice for storing a large repository of text documents. Additionally, Amazon S3 is designed to be highly scalable and can easily handle periods of high demand without requiring any additional infrastructure or maintenance.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 763734,
          "date": "Mon 02 Jan 2023 14:12",
          "username": "\t\t\t\tgustavtd\t\t\t",
          "content": "Is there anything cheaper than S3?",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 752879,
          "date": "Thu 22 Dec 2022 00:51",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "D.  Amazon S3 is the most cost-effective storage solution that meets the requirements described.<br><br>Amazon S3 is an object storage service that is designed to store and retrieve large amounts of data from anywhere on the web. It is highly scalable, highly available, and cost-effective, making it an ideal choice for storing a large repository of text documents that will experience periods of high demand. S3 is a standalone storage service that can be accessed from anywhere, and it is designed to handle large numbers of objects, making it well-suited for storing the 900 TB repository of text documents described in the scenario. It is also designed to handle high levels of demand, making it suitable for handling periods of high demand.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 749189,
          "date": "Sun 18 Dec 2022 22:03",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 747995,
          "date": "Sat 17 Dec 2022 11:47",
          "username": "\t\t\t\tNikaCZ\t\t\t",
          "content": "Only EFS and S3 meeting the requirements but S3 is better option because it is cheaper.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 723865,
          "date": "Mon 21 Nov 2022 20:52",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "D is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 713889,
          "date": "Tue 08 Nov 2022 15:11",
          "username": "\t\t\t\tPS_R\t\t\t",
          "content": "Only EFS and S3, Since EFS is make it much costly, S3 is the viable option",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 705591,
          "date": "Thu 27 Oct 2022 15:25",
          "username": "\t\t\t\tUWSFish\t\t\t",
          "content": "I originally thought C but the question is specific about wanting the storage to scale not the search capacity.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        }
      ]
    },
    {
      "question_id": "#119",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A global company is using Amazon API Gateway to design REST APIs for its loyalty club users in the us-east-1 Region and the ap-southeast-2 Region. A solutions architect must design a solution to protect these API Gateway managed REST APIs across multiple accounts from SQL injection and cross-site scripting attacks.<br>Which solution will meet these requirements with the LEAST amount of administrative effort?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#119",
          "answers": [
            {
              "choice": "<p>A. Set up AWS WAF in both Regions. Associate Regional web ACLs with an API stage.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Set up AWS Firewall Manager in both Regions. Centrally configure AWS WAF rules.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Set up AWS Shield in bath Regions. Associate Regional web ACLs with an API stage.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Set up AWS Shield in one of the Regions. Associate Regional web ACLs with an API stage.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 713898,
          "date": "Tue 08 Nov 2022 15:28",
          "username": "\t\t\t\tGil80\t\t\t",
          "content": "If you want to use AWS WAF across accounts, accelerate WAF configuration, automate the protection of new resources, use Firewall Manager with AWS WAF",
          "upvote_count": "15",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 713459,
          "date": "Tue 08 Nov 2022 04:29",
          "username": "\t\t\t\tNigma\t\t\t",
          "content": "B<br><br>Using AWS WAF has several benefits. Additional protection against web attacks using criteria that you specify. You can define criteria using characteristics of web requests such as the following:<br>Presence of SQL code that is likely to be malicious (known as SQL injection).<br>Presence of a script that is likely to be malicious (known as cross-site scripting).<br><br>AWS Firewall Manager simplifies your administration and maintenance tasks across multiple accounts and resources for a variety of protections.<br><br>https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Q: Can I create security policies across regions?<br><br>No, AWS Firewall Manager security policies are region specific. Each Firewall Manager policy can only include resources available in that specified AWS Region. You can create a new policy for each region where you operate.<br><br>So you could not centrally (i.e. in one place) configure policies, you would need to do this is each region</li></ul>",
          "upvote_count": "13",
          "selected_answers": ""
        },
        {
          "id": 750983,
          "date": "Tue 20 Dec 2022 15:14",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "Q: Can I create security policies across regions?<br><br>No, AWS Firewall Manager security policies are region specific. Each Firewall Manager policy can only include resources available in that specified AWS Region. You can create a new policy for each region where you operate.<br><br>So you could not centrally (i.e. in one place) configure policies, you would need to do this is each region",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 818630,
          "date": "Thu 23 Feb 2023 02:29",
          "username": "\t\t\t\tbdp123\t\t\t",
          "content": "https://aws.amazon.com/blogs/security/centrally-manage-aws-waf-api-v2-and-aws-managed-rules-at-scale-with-firewall-manager/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 817768,
          "date": "Wed 22 Feb 2023 12:34",
          "username": "\t\t\t\tandyto\t\t\t",
          "content": "B. <br>Set up AWS Firewall Manager<br>https://docs.aws.amazon.com/waf/latest/developerguide/enable-disabled-region.html<br>Create WAF policies separate for each Region:<br>https://docs.aws.amazon.com/waf/latest/developerguide/get-started-fms-create-security-policy.html<br>To protect resources in multiple Regions (other than CloudFront distributions), you must create separate Firewall Manager policies for each Region.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 801854,
          "date": "Wed 08 Feb 2023 10:53",
          "username": "\t\t\t\tJiyuKim\t\t\t",
          "content": "I' ll go with A. <br>B is wrong because<br>To protect resources in multiple Regions (other than CloudFront distributions), you must create separate Firewall Manager policies for each Region.<br><br>https://docs.aws.amazon.com/waf/latest/developerguide/get-started-fms-create-security-policy.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 766991,
          "date": "Thu 05 Jan 2023 20:22",
          "username": "\t\t\t\tMahadeva\t\t\t",
          "content": "Though Option A and B are valid, the question is on Administration efficiency. Since only 2 regions are in consideration, it is much easier to provision WAF than a central Firewall Manager (plus WAF).<br><br>Regarding \\\"to protect API Gateways across multiple accounts\\\". may be it is an extra information. Web ACLs are at regional level, essentially filters out HTTP messages irrespective of the account i.e., it is applicable to all accounts.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>A &amp; B are viable options, however because it is two regions instead of creating WAF twice (one for each region) simply create it all at once in the Central Firewall Manager. Imagine you need to make some changes later and again rather than changing it on each, 1 by 1 simply change it on the Central Firewall Manager once and you can deploy more in the future by just adding regions.</li><li>Option A: WAF</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 814281,
          "date": "Sun 19 Feb 2023 17:54",
          "username": "\t\t\t\tHelp2023\t\t\t",
          "content": "A & B are viable options, however because it is two regions instead of creating WAF twice (one for each region) simply create it all at once in the Central Firewall Manager. Imagine you need to make some changes later and again rather than changing it on each, 1 by 1 simply change it on the Central Firewall Manager once and you can deploy more in the future by just adding regions.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 766993,
          "date": "Thu 05 Jan 2023 20:23",
          "username": "\t\t\t\tMahadeva\t\t\t",
          "content": "Option A: WAF",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 763695,
          "date": "Mon 02 Jan 2023 13:11",
          "username": "\t\t\t\taba2s\t\t\t",
          "content": "Use AWS WAF and set up a managed rule to block request patterns associated with the exploitation of SQL databases, like SQL injection attacks. Associate it with the Application Load Balancer. Integrate AWS WAF with AWS Firewall Manager to reuse the rules across all the AWS accounts.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 762653,
          "date": "Sat 31 Dec 2022 14:04",
          "username": "\t\t\t\tZerotn3\t\t\t",
          "content": "B.  Set up AWS Firewall Manager in both Regions. Centrally configure AWS WAF rules.<br><br>To protect Amazon API Gateway managed REST APIs from SQL injection and cross-site scripting attacks across multiple accounts with the least amount of administrative effort, you can set up AWS Firewall Manager in both Regions and centrally configure AWS WAF rules.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 760149,
          "date": "Wed 28 Dec 2022 18:30",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "Clarified here https://medium.com/@tshemku/aws-waf-vs-firewall-manager-vs-shield-vs-shield-advanced-4c86911e94c6",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 759156,
          "date": "Wed 28 Dec 2022 00:34",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option B, setting up AWS Firewall Manager in both Regions and centrally configuring AWS WAF rules, would require the least amount of administrative effort.<br><br>AWS Firewall Manager is a centralized service that enables you to set security policies across your accounts and applications, including API Gateway-managed REST APIs. By setting up AWS Firewall Manager in both Regions and centrally configuring AWS WAF rules, you can protect your APIs from SQL injection and cross-site scripting attacks with minimal effort, as the rules will be centrally managed and automatically enforced across all of your accounts and applications.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 754046,
          "date": "Fri 23 Dec 2022 09:46",
          "username": "\t\t\t\tDavidNamy\t\t\t",
          "content": "Option B involves setting up AWS Firewall Manager in both regions and centrally configuring AWS WAF rules. This allows you to manage the protection of your APIs across multiple accounts and regions from a central location, reducing the administrative effort required.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 752375,
          "date": "Wed 21 Dec 2022 14:44",
          "username": "\t\t\t\tSilvestr\t\t\t",
          "content": "Correct answer - A<br>WAF - HTTP headers, HTTP body, or URI strings Protects from common attack - SQL<br>injection and Cross-Site Scripting (XSS)",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 751710,
          "date": "Wed 21 Dec 2022 01:33",
          "username": "\t\t\t\tCyoung82\t\t\t",
          "content": "\\\"Least administrative effort\\\" would be answer: B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 749190,
          "date": "Sun 18 Dec 2022 22:07",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option A is right option. <br>Option B does not mention configuring WAFrules it just says Firewall Manager. Firewall Manager is just a management layer that manages all firewall configurations.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>AWS Firewall Manager<br>Centrally configure and manage firewall rules across your accounts <br>Deploy managed rules, such as pre-configured WAF rules on your applications, across accounts.</li><li>https://aws.amazon.com/firewall-manager/</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 750975,
          "date": "Tue 20 Dec 2022 15:10",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "AWS Firewall Manager<br>Centrally configure and manage firewall rules across your accounts <br>Deploy managed rules, such as pre-configured WAF rules on your applications, across accounts.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>https://aws.amazon.com/firewall-manager/</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 750977,
          "date": "Tue 20 Dec 2022 15:10",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "https://aws.amazon.com/firewall-manager/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 744458,
          "date": "Tue 13 Dec 2022 21:56",
          "username": "\t\t\t\tileri_sec\t\t\t",
          "content": "\\\"To protect resources in multiple Regions (other than CloudFront distributions), you must create separate Firewall Manager policies for each Region.\\\"<br><br>https://docs.aws.amazon.com/waf/latest/developerguide/get-started-fms-create-security-policy.html<br><br>I thnk i ll go for A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 738124,
          "date": "Wed 07 Dec 2022 17:18",
          "username": "\t\t\t\tJit\t\t\t",
          "content": "A .<br>WAF for API is a regional. https://docs.aws.amazon.com/waf/latest/developerguide/how-aws-waf-works.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 723866,
          "date": "Mon 21 Nov 2022 20:54",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#120",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has implemented a self-managed DNS solution on three Amazon EC2 instances behind a Network Load Balancer (NLB) in the us-west-2 Region. Most of the company's users are located in the United States and Europe. The company wants to improve the performance and availability of the solution. The company launches and configures three EC2 instances in the eu-west-1 Region and adds the EC2 instances as targets for a new NLB. <br>Which solution can the company use to route traffic to all the EC2 instances?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#120",
          "answers": [
            {
              "choice": "<p>A. Create an Amazon Route 53 geolocation routing policy to route requests to one of the two NLBs. Create an Amazon CloudFront distribution. Use the Route 53 record as the distribution's origin.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a standard accelerator in AWS Global Accelerator. Create endpoint groups in us-west-2 and eu-west-1. Add the two NLBs as endpoints for the endpoint groups.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Attach Elastic IP addresses to the six EC2 instances. Create an Amazon Route 53 geolocation routing policy to route requests to one of the six EC2 instances. Create an Amazon CloudFront distribution. Use the Route 53 record as the distribution's origin.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Replace the two NLBs with two Application Load Balancers (ALBs). Create an Amazon Route 53 latency routing policy to route requests to one of the two ALBs. Create an Amazon CloudFront distribution. Use the Route 53 record as the distribution's origin.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 698257,
          "date": "Tue 18 Oct 2022 15:05",
          "username": "\t\t\t\tLeGloupier\t\t\t",
          "content": "for me it is B",
          "upvote_count": "8",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 707556,
          "date": "Sun 30 Oct 2022 01:56",
          "username": "\t\t\t\tdokaedu\t\t\t",
          "content": "B is the correct one for seld manage DNS<br>If need to use Route53, ALB(layar 7 ) needs to be used as end points for 2 reginal x 3 EC2s, if it the case answer would be the option 4",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 848633,
          "date": "Thu 23 Mar 2023 21:14",
          "username": "\t\t\t\tBang3R\t\t\t",
          "content": "Both A and B will do the job... B provides access to the AWS backbone and therefore better performance",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 847120,
          "date": "Wed 22 Mar 2023 14:48",
          "username": "\t\t\t\tMssP\t\t\t",
          "content": "\\\"self-managed DNS solution\\\". You cannot make anything in Route53 if you dont use :-)Answer is B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 835508,
          "date": "Fri 10 Mar 2023 23:50",
          "username": "\t\t\t\tfkie4\t\t\t",
          "content": "I vote B.  \\\"A\\\" doesn't sound right. When NLB is used, it means it is redicting TCP/IP packets. CloudFont is used for Http request, not for TCP/IP",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 802309,
          "date": "Wed 08 Feb 2023 18:30",
          "username": "\t\t\t\tOuk\t\t\t",
          "content": "Not only this question, but in many replies for C03 questions seem intentionally wrong<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>What do you mean ? we will fail ?</li><li>Can you explain what you say?</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 823523,
          "date": "Mon 27 Feb 2023 11:24",
          "username": "\t\t\t\tahalamri\t\t\t",
          "content": "What do you mean ? we will fail ?",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 822015,
          "date": "Sun 26 Feb 2023 03:37",
          "username": "\t\t\t\tJa13\t\t\t",
          "content": "Can you explain what you say?",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 800619,
          "date": "Tue 07 Feb 2023 07:40",
          "username": "\t\t\t\tProfXsamson\t\t\t",
          "content": "With a standard accelerator, Global Accelerator directs traffic over the AWS global network to endpoints in the nearest Region to the client.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>For standard accelerators, Global Accelerator uses the AWS global network to route traffic to the optimal regional endpoint based on health, client location, and policies that you configure, which increases the availability of your applications. Endpoints for standard accelerators can be Network Load Balancers, Application Load Balancers, Amazon EC2 instances, or Elastic IP addresses that are located in one AWS Region or multiple Regions.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 800622,
          "date": "Tue 07 Feb 2023 07:42",
          "username": "\t\t\t\tProfXsamson\t\t\t",
          "content": "For standard accelerators, Global Accelerator uses the AWS global network to route traffic to the optimal regional endpoint based on health, client location, and policies that you configure, which increases the availability of your applications. Endpoints for standard accelerators can be Network Load Balancers, Application Load Balancers, Amazon EC2 instances, or Elastic IP addresses that are located in one AWS Region or multiple Regions.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 792299,
          "date": "Mon 30 Jan 2023 03:10",
          "username": "\t\t\t\tgogod2\t\t\t",
          "content": "Had a little chat with ChatGTP.<br>(in this case) B is not the best option because it is meant for optimizing performance for users globally by directing traffic to the AWS Region that provides the lowest latency. However, in this case the company wants to improve performance and availability for its users located in the US and Europe, so using a geolocation routing policy in Amazon Route 53 would be more suitable.<br><br>If the question involved users globally, then option B would likely be the best solution. The standard accelerator in AWS Global Accelerator is specifically designed for optimizing performance for users globally by directing traffic to the AWS Region that provides the lowest latency. This would help improve the performance and availability of the company's self-managed DNS solution for users worldwide.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I did same and getting both A &amp; B when regenerated the response :)</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 798292,
          "date": "Sat 04 Feb 2023 20:24",
          "username": "\t\t\t\tRocky2023\t\t\t",
          "content": "I did same and getting both A & B when regenerated the response :)",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 779104,
          "date": "Tue 17 Jan 2023 18:08",
          "username": "\t\t\t\tremand\t\t\t",
          "content": "B.  Create a standard accelerator in AWS Global Accelerator. Create endpoint groups in us-west-2 and eu-west-1. Add the two NLBs as endpoints for the endpoint groups.<br><br>AWS Global Accelerator is a service that improves the availability and performance of internet applications by routing traffic to the optimal AWS region for a given user. The company can create a standard accelerator and create endpoint groups in us-west-2 and eu-west-1. Then add the two NLBs as endpoints for the endpoint groups. This will allow the company to route traffic to all the EC2 instances based on the optimal region for the user.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 774380,
          "date": "Fri 13 Jan 2023 12:28",
          "username": "\t\t\t\tVickysss\t\t\t",
          "content": "https://docs.aws.amazon.com/global-accelerator/latest/dg/what-is-global-accelerator.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 766965,
          "date": "Thu 05 Jan 2023 20:04",
          "username": "\t\t\t\tMahadeva\t\t\t",
          "content": "Though Option A and B are valid, the question is on Administration efficiency. Since only 2 regions are in consideration, it is much easier to provision WAF than a central Firewall Manager (plus WAF). <br><br>Regarding \\\"to protect API Gateways across multiple accounts\\\". may be it is an extra information. Web ACLs are at regional level, essentially filters out HTTP messages irrespective of the account i.e., it is applicable to all accounts.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 766148,
          "date": "Thu 05 Jan 2023 00:38",
          "username": "\t\t\t\tdan80\t\t\t",
          "content": "https://aws.amazon.com/global-accelerator/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 762860,
          "date": "Sat 31 Dec 2022 19:53",
          "username": "\t\t\t\tMindvision\t\t\t",
          "content": "B is correct answer. <br>Use case - Use traffic dials to route traffic to the nearest Region or achieve fast failover across Regions in the case to the users in there appropriate regions. https://aws.amazon.com/global-accelerator/<br><br>A - incorrect as DNS is self managed just in the us not eu",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 762658,
          "date": "Sat 31 Dec 2022 14:16",
          "username": "\t\t\t\tZerotn3\t\t\t",
          "content": "B solution is not correct because it does not fully address the requirements of the question.<br><br>AWS Global Accelerator is a service that routes traffic over the Amazon global network to the optimal AWS Region for the user, based on network performance. It does not allow routing based on the geographic location of the user.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>But the question is for \\\"improving performance and availability.\\\" Network performance is offered by Global Accelerator. Why should a European user be stuck on Euro region if the US-West offers better network performance? Geolocation binds a user to Location. <br><br>In addition to Performance, Global Accelerator offers Failover to other region (satisfies another part of the question -- solution to make use of all EC2 instances across regions).</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 767000,
          "date": "Thu 05 Jan 2023 20:32",
          "username": "\t\t\t\tMahadeva\t\t\t",
          "content": "But the question is for \\\"improving performance and availability.\\\" Network performance is offered by Global Accelerator. Why should a European user be stuck on Euro region if the US-West offers better network performance? Geolocation binds a user to Location. <br><br>In addition to Performance, Global Accelerator offers Failover to other region (satisfies another part of the question -- solution to make use of all EC2 instances across regions).",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 759162,
          "date": "Wed 28 Dec 2022 00:42",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The correct solution is Option A.  Create an Amazon Route 53 geolocation routing policy to route requests to one of the two NLBs. Create an Amazon CloudFront distribution. Use the Route 53 record as the distribution's origin.<br><br>To improve the performance and availability of the self-managed DNS solution, the company can use Amazon Route 53 geolocation routing to route traffic to the NLBs that are closest to the users. Geolocation routing allows the company to route traffic to a specific resource based on the geographic location of the user making the request. By using geolocation routing, the company can ensure that users are directed to the NLBs that are closest to them, improving the performance of the DNS solution.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>To implement this solution, the company can create a geolocation routing policy in Amazon Route 53 and specify the two NLBs as the target resources. The company can then create an Amazon CloudFront distribution and use the Route 53 record as the origin for the distribution. This will allow the company to distribute traffic to the NLBs through the CloudFront distribution, improving the performance and availability of the DNS solution.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 759163,
          "date": "Wed 28 Dec 2022 00:43",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "To implement this solution, the company can create a geolocation routing policy in Amazon Route 53 and specify the two NLBs as the target resources. The company can then create an Amazon CloudFront distribution and use the Route 53 record as the origin for the distribution. This will allow the company to distribute traffic to the NLBs through the CloudFront distribution, improving the performance and availability of the DNS solution.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 754161,
          "date": "Fri 23 Dec 2022 12:43",
          "username": "\t\t\t\tduriselvan\t\t\t",
          "content": "company wants to improve the performance<br><br>AWS Global Accelerator<br>Improve application availability, performance, and security using the AWS global network<br><br>https://aws.amazon.com/global-accelerator/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 754160,
          "date": "Fri 23 Dec 2022 12:42",
          "username": "\t\t\t\tduriselvan\t\t\t",
          "content": "B is correct ans",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#121",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is running an online transaction processing (OLTP) workload on AWS. This workload uses an unencrypted Amazon RDS DB instance in a Multi-AZ deployment. Daily database snapshots are taken from this instance.<br>What should a solutions architect do to ensure the database and snapshots are always encrypted moving forward?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#121",
          "answers": [
            {
              "choice": "<p>A. Encrypt a copy of the latest DB snapshot. Replace existing DB instance by restoring the encrypted snapshot.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a new encrypted Amazon Elastic Block Store (Amazon EBS) volume and copy the snapshots to it. Enable encryption on the DB instance.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Copy the snapshots and enable encryption using AWS Key Management Service (AWS KMS) Restore encrypted snapshot to an existing DB instance.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Copy the snapshots to an Amazon S3 bucket that is encrypted using server-side encryption with AWS Key Management Service (AWS KMS) managed keys (SSE-KMS).<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 699279,
          "date": "Wed 19 Oct 2022 20:53",
          "username": "\t\t\t\t123jhl0\t\t\t",
          "content": "\\\"You can enable encryption for an Amazon RDS DB instance when you create it, but not after it's created. However, you can add encryption to an unencrypted DB instance by creating a snapshot of your DB instance, and then creating an encrypted copy of that snapshot. You can then restore a DB instance from the encrypted snapshot to get an encrypted copy of your original DB instance.\\\"<br>https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/encrypt-an-existing-amazon-rds-for-postgresql-db-instance.html",
          "upvote_count": "30",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 849757,
          "date": "Sat 25 Mar 2023 03:28",
          "username": "\t\t\t\tAbhineet9148232\t\t\t",
          "content": "Encryption is enabled during the Copy process itself.<br>https://repost.aws/knowledge-center/encrypt-rds-snapshots",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 848641,
          "date": "Thu 23 Mar 2023 21:31",
          "username": "\t\t\t\tBang3R\t\t\t",
          "content": "C is the more complete answer as you need KMS to encrypt the snapshot copy prior to restoring it to the Database instance.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 834577,
          "date": "Fri 10 Mar 2023 03:56",
          "username": "\t\t\t\tTungPham\t\t\t",
          "content": "A not resolve data create in future. <br>You can enable encryption for an Amazon RDS DB instance when you create it, but not after it's created.<br>C will make this, see image below <br>Architecture<br>Source architecture<br><br>Unencrypted RDS DB instance<br><br>Target architecture <br><br>Encrypted RDS DB instance<br><br>The destination RDS DB instance is created by restoring the DB snapshot copy of the source RDS DB instance.<br><br>An AWS KMS key is used for encryption while restoring the snapshot.<br><br>An AWS DMS replication task is used to migrate the data.<br>https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/encrypt-an-existing-amazon-rds-for-postgresql-db-instance.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A seems correct.<br>With option (A)we already have DB snapshots. Just encrypt the latest available copy of snapshot, why to copy the snapshot once again (as told in option C).</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 847496,
          "date": "Wed 22 Mar 2023 21:30",
          "username": "\t\t\t\tjaswantn\t\t\t",
          "content": "Option A seems correct.<br>With option (A)we already have DB snapshots. Just encrypt the latest available copy of snapshot, why to copy the snapshot once again (as told in option C).",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 808069,
          "date": "Tue 14 Feb 2023 05:23",
          "username": "\t\t\t\tjkmaws\t\t\t",
          "content": "A<br>You can enable encryption for an Amazon RDS DB instance when you create it, but not after it's created. However, you can add encryption to an unencrypted DB instance by creating a snapshot of your DB instance, and then creating an encrypted copy of that snapshot. You can then restore a DB instance from the encrypted snapshot to get an encrypted copy of your original DB instance. If your project allows for downtime (at least for write transactions) during this activity, this is all you need to do. When the new, encrypted copy of the DB instance becomes available, you can point your applications to the new database.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 798910,
          "date": "Sun 05 Feb 2023 15:55",
          "username": "\t\t\t\tCaoMengde09\t\t\t",
          "content": "It's A for the following reasons : <br>--> To restore an Encrypted DB Instance from an encrypted snapshot we'll need to replace the old one - as we cannot enable encryption on an existing DB Instance<br>--> We have both Snap/Db Instance encrypted moving forward since all the daily Backups on an already encrypted DB Instance would be encrypted",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 788822,
          "date": "Thu 26 Jan 2023 15:52",
          "username": "\t\t\t\tsassy2023\t\t\t",
          "content": "C is right<br>You can enable encryption for an Amazon RDS DB instance when you create it, but not after it's created. However, you can add encryption to an unencrypted DB instance by creating a snapshot of your DB instance, and then creating an encrypted copy of that snapshot. You can then restore a DB instance from the encrypted snapshot to get an encrypted copy of your original DB instance.<br><br>Tools used to enable encryption:<br><br>AWS KMS key for encryption  When you create an encrypted DB instance, you can choose a customer managed key or the AWS managed key for Amazon RDS to encrypt your DB instance. If you don't specify the key identifier for a customer managed key, Amazon RDS uses the AWS managed key for your new DB instance. Amazon RDS creates an AWS managed key for Amazon RDS for your AWS account.<br><br>https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/encrypt-an-existing-amazon-rds-for-postgresql-db-instance.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 784440,
          "date": "Sun 22 Jan 2023 16:31",
          "username": "\t\t\t\tbullrem\t\t\t",
          "content": "The correct answer is C,<br><br>Copy the snapshots and enable encryption using AWS Key Management Service (AWS KMS)<br>Restore encrypted snapshot to an existing DB instance.<br>This is the correct approach as it allows you to encrypt the existing snapshots and the existing DB instance using AWS KMS. This way, you can ensure that all data stored in the DB instance and the snapshots are encrypted at rest, providing an additional layer of security.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 779152,
          "date": "Tue 17 Jan 2023 18:40",
          "username": "\t\t\t\tremand\t\t\t",
          "content": "D.  Copy the snapshots to an Amazon S3 bucket that is encrypted using server-side encryption with AWS Key Management Service (AWS KMS) managed keys (SSE-KMS).<br><br>This option ensures that the database snapshots are encrypted at rest by copying them to an S3 bucket that is encrypted using SSE-KMS. This option also provides the flexibility to restore the snapshots to a new RDS DB instance in the future, which will also be encrypted by default.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 775492,
          "date": "Sat 14 Jan 2023 15:18",
          "username": "\t\t\t\tgoodmail\t\t\t",
          "content": "If C means doing encryption while making snapshot, then it is incorrect. It is not able to make an encrypted snapshot from unencrypted RDS. But it will be correct if it means enabling KMS function when restoring DB instance. Bad in wordings.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 774907,
          "date": "Fri 13 Jan 2023 23:50",
          "username": "\t\t\t\timisioluwa\t\t\t",
          "content": "The correct answer is A.  Check this link \\\" https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/encrypt-an-existing-amazon-rds-for-postgresql-db-instance.html \\\"<br>\\\" However, you can add encryption to an unencrypted DB instance by creating a snapshot of your DB instance, and then creating an encrypted copy of that snapshot. You can then restore a DB instance from the encrypted snapshot to get an encrypted copy of your original DB instance\\\".",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 770319,
          "date": "Mon 09 Jan 2023 12:55",
          "username": "\t\t\t\tlfrad\t\t\t",
          "content": "I feel this is a bit tricky in the way the question is asked, but C implies that you are encrypting the snapshot. You are not. It is the DB that receives a KMS key upon restoring, but the snapshot is still unencrypted.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Also C does not make mention of replacing the base DB, which means you would need to copy the snapshot every time a new one is created to encrypt it, and the base DB would remain unencrypted. The solution in A takes the root of the problem by replacing the unencrypted RDS DB with a new encrypted one, thus making every snapshot created in the future automatically encrypted.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 770334,
          "date": "Mon 09 Jan 2023 13:06",
          "username": "\t\t\t\tlfrad\t\t\t",
          "content": "Also C does not make mention of replacing the base DB, which means you would need to copy the snapshot every time a new one is created to encrypt it, and the base DB would remain unencrypted. The solution in A takes the root of the problem by replacing the unencrypted RDS DB with a new encrypted one, thus making every snapshot created in the future automatically encrypted.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 759169,
          "date": "Wed 28 Dec 2022 00:50",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The correct answer is Option C.  To ensure that the database and snapshots are always encrypted moving forward, the solutions architect should copy the snapshots and enable encryption using AWS Key Management Service (AWS KMS). Then, the encrypted snapshot can be restored to the existing DB instance.<br><br>Option A involves creating an encrypted copy of the latest DB snapshot and replacing the existing DB instance by restoring the encrypted snapshot. This option would result in the database being encrypted, but it would not ensure that future snapshots are encrypted.<br><br>Option B involves creating a new encrypted Amazon Elastic Block Store (Amazon EBS) volume and copying the snapshots to it. While this option would encrypt the snapshots, it would not encrypt the existing DB instance.<br><br>Option D involves copying the snapshots to an Amazon S3 bucket that is encrypted using server-side encryption with AWS KMS-managed keys (SSE-KMS). While this option would encrypt the snapshots, it would not ensure that the existing DB instance is encrypted.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 754644,
          "date": "Sat 24 Dec 2022 02:42",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "C is better answer than A as snapshot has to be encrypted using KMS keys.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 749128,
          "date": "Sun 18 Dec 2022 20:47",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "A is the right answer<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>C Is more Accurate answer as snapshot has to be encrypted using KMS keys.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 754640,
          "date": "Sat 24 Dec 2022 02:30",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "C Is more Accurate answer as snapshot has to be encrypted using KMS keys.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 728822,
          "date": "Mon 28 Nov 2022 07:07",
          "username": "\t\t\t\tocbn3wby\t\t\t",
          "content": "You cannot restore to existing DB (hence answer C is wrong). You create new DB for which you choose new unique Identifier.<br><br>https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/encrypt-an-existing-amazon-rds-for-postgresql-db-instance.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 724311,
          "date": "Tue 22 Nov 2022 13:23",
          "username": "\t\t\t\tmricee9\t\t\t",
          "content": "Cant be C - you cant restore it to an existing DB instance",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        }
      ]
    },
    {
      "question_id": "#122",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company wants to build a scalable key management infrastructure to support developers who need to encrypt data in their applications.<br>What should a solutions architect do to reduce the operational burden?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#122",
          "answers": [
            {
              "choice": "<p>A. Use multi-factor authentication (MFA) to protect the encryption keys.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use AWS Key Management Service (AWS KMS) to protect the encryption keys.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use AWS Certificate Manager (ACM) to create, store, and assign the encryption keys.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use an IAM policy to limit the scope of users who have access permissions to protect the encryption keys.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 699283,
          "date": "Wed 19 Oct 2022 21:01",
          "username": "\t\t\t\t123jhl0\t\t\t",
          "content": "If you are a developer who needs to digitally sign or verify data using asymmetric keys, you should use the service to create and manage the private keys you'll need. If you're looking for a scalable key management infrastructure to support your developers and their growing number of applications, you should use it to reduce your licensing costs and operational burden...<br>https://aws.amazon.com/kms/faqs/#:~:text=If%20you%20are%20a%20developer%20who%20needs%20to%20digitally,a%20broad%20set%20of%20industry%20and%20regional%20compliance%20regimes.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Most documented answers. Thank you, 123jhl0.</li></ul>",
          "upvote_count": "13",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 728825,
          "date": "Mon 28 Nov 2022 07:09",
          "username": "\t\t\t\tocbn3wby\t\t\t",
          "content": "Most documented answers. Thank you, 123jhl0.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 759170,
          "date": "Wed 28 Dec 2022 00:52",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The correct answer is Option B.  To reduce the operational burden, the solutions architect should use AWS Key Management Service (AWS KMS) to protect the encryption keys.<br><br>AWS KMS is a fully managed service that makes it easy to create and manage encryption keys. It allows developers to easily encrypt and decrypt data in their applications, and it automatically handles the underlying key management tasks, such as key generation, key rotation, and key deletion. This can help to reduce the operational burden associated with key management.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 749132,
          "date": "Sun 18 Dec 2022 20:51",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 723941,
          "date": "Mon 21 Nov 2022 22:24",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 723940,
          "date": "Mon 21 Nov 2022 22:21",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 717481,
          "date": "Sun 13 Nov 2022 20:40",
          "username": "\t\t\t\tJtic\t\t\t",
          "content": "If you are responsible for securing your data across AWS services, you should use it to centrally manage the encryption keys that control access to your data. If you are a developer who needs to encrypt data in your applications, you should use the AWS Encryption SDK with AWS KMS to easily generate, use and protect symmetric encryption keys in your code.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        }
      ]
    },
    {
      "question_id": "#123",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has a dynamic web application hosted on two Amazon EC2 instances. The company has its own SSL certificate, which is on each instance to perform SSL termination.<br>There has been an increase in traffic recently, and the operations team determined that SSL encryption and decryption is causing the compute capacity of the web servers to reach their maximum limit.<br>What should a solutions architect do to increase the application's performance?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#123",
          "answers": [
            {
              "choice": "<p>A. Create a new SSL certificate using AWS Certificate Manager (ACM). Install the ACM certificate on each instance.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an Amazon S3 bucket Migrate the SSL certificate to the S3 bucket. Configure the EC2 instances to reference the bucket for SSL termination.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create another EC2 instance as a proxy server. Migrate the SSL certificate to the new instance and configure it to direct connections to the existing EC2 instances.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Import the SSL certificate into AWS Certificate Manager (ACM). Create an Application Load Balancer with an HTTPS listener that uses the SSL certificate from ACM.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 699292,
          "date": "Wed 19 Oct 2022 21:09",
          "username": "\t\t\t\t123jhl0\t\t\t",
          "content": "This issue is solved by SSL offloading, i.e. by moving the SSL termination task to the ALB. <br>https://aws.amazon.com/blogs/aws/elastic-load-balancer-support-for-ssl-termination/",
          "upvote_count": "11",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 802012,
          "date": "Wed 08 Feb 2023 13:28",
          "username": "\t\t\t\tdejung\t\t\t",
          "content": "Why is A wrong?",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 779135,
          "date": "Tue 17 Jan 2023 18:28",
          "username": "\t\t\t\tremand\t\t\t",
          "content": "SSL termination is the process of ending an SSL/TLS connection. This is typically done by a device, such as a load balancer or a reverse proxy, that is positioned in front of one or more web servers. The device decrypts incoming SSL/TLS traffic and then forwards the unencrypted request to the web server. This allows the web server to process the request without the overhead of decrypting and encrypting the traffic. The device then re-encrypts the response from the web server and sends it back to the client. This allows the device to offload the SSL/TLS processing from the web servers and also allows for features such as SSL offloading, SSL bridging, and SSL acceleration.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 759173,
          "date": "Wed 28 Dec 2022 00:54",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The correct answer is D.  To increase the application's performance, the solutions architect should import the SSL certificate into AWS Certificate Manager (ACM) and create an Application Load Balancer with an HTTPS listener that uses the SSL certificate from ACM.<br><br>An Application Load Balancer (ALB) can offload the SSL termination process from the EC2 instances, which can help to increase the compute capacity available for the web application. By creating an ALB with an HTTPS listener and using the SSL certificate from ACM, the ALB can handle the SSL termination process, leaving the EC2 instances free to focus on running the web application.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 749133,
          "date": "Sun 18 Dec 2022 20:53",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option D to offload the SSL encryption workload",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 738320,
          "date": "Wed 07 Dec 2022 21:23",
          "username": "\t\t\t\tAamee\t\t\t",
          "content": "Due to this statement particularly: \\\"The company has its own SSL certificate\\\" as it's not created from AWS ACM itself.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 723942,
          "date": "Mon 21 Nov 2022 22:25",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "D is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 704852,
          "date": "Wed 26 Oct 2022 18:12",
          "username": "\t\t\t\tSix_Fingered_Jose\t\t\t",
          "content": "agree with D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        }
      ]
    },
    {
      "question_id": "#124",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has a highly dynamic batch processing job that uses many Amazon EC2 instances to complete it. The job is stateless in nature, can be started and stopped at any given time with no negative impact, and typically takes upwards of 60 minutes total to complete. The company has asked a solutions architect to design a scalable and cost-effective solution that meets the requirements of the job.<br>What should the solutions architect recommend?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#124",
          "answers": [
            {
              "choice": "<p>A. Implement EC2 Spot Instances.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Purchase EC2 Reserved Instances.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Implement EC2 On-Demand Instances.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Implement the processing on AWS Lambda.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 728811,
          "date": "Mon 28 Nov 2022 06:47",
          "username": "\t\t\t\tKapello10\t\t\t",
          "content": "Cant be implemented on Lambda because the timeout for Lambda is 15mins and the Job takes 60minutes to complete<br><br>Answer >> A",
          "upvote_count": "9",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 699912,
          "date": "Thu 20 Oct 2022 14:37",
          "username": "\t\t\t\tEvangelia\t\t\t",
          "content": "spot instances",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 759175,
          "date": "Wed 28 Dec 2022 00:54",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The correct answer is Option A.  To design a scalable and cost-effective solution for the batch processing job, the solutions architect should recommend implementing EC2 Spot Instances.<br><br>EC2 Spot Instances allow users to bid on spare Amazon EC2 computing capacity and can be a cost-effective solution for stateless, interruptible workloads that can be started and stopped at any time. Since the batch processing job is stateless, can be started and stopped at any time, and typically takes upwards of 60 minutes to complete, EC2 Spot Instances would be a good fit for this workload.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 755445,
          "date": "Sun 25 Dec 2022 07:40",
          "username": "\t\t\t\tk1kavi1\t\t\t",
          "content": "Spot Instances should be good enough and cost effective because the job can be started and stopped at any given time with no negative impact.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 749135,
          "date": "Sun 18 Dec 2022 20:55",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 723943,
          "date": "Mon 21 Nov 2022 22:26",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 707611,
          "date": "Sun 30 Oct 2022 06:01",
          "username": "\t\t\t\tSimonPark\t\t\t",
          "content": "A is the answer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        }
      ]
    },
    {
      "question_id": "#125",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company runs its two-tier ecommerce website on AWS. The web tier consists of a load balancer that sends traffic to Amazon EC2 instances. The database tier uses an Amazon RDS DB instance. The EC2 instances and the RDS DB instance should not be exposed to the public internet. The EC2 instances require internet access to complete payment processing of orders through a third-party web service. The application must be highly available.<br>Which combination of configuration options will meet these requirements? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: AD</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#125",
          "answers": [
            {
              "choice": "<p>A. Use an Auto Scaling group to launch the EC2 instances in private subnets. Deploy an RDS Multi-AZ DB instance in private subnets.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Configure a VPC with two private subnets and two NAT gateways across two Availability Zones. Deploy an Application Load Balancer in the private subnets.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use an Auto Scaling group to launch the EC2 instances in public subnets across two Availability Zones. Deploy an RDS Multi-AZ DB instance in private subnets.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure a VPC with one public subnet, one private subnet, and two NAT gateways across two Availability Zones. Deploy an Application Load Balancer in the public subnet.D.  Configure a VPC with two public subnets, two private subnets, and two NAT gateways across two Availability Zones. Deploy an Application Load Balancer in the public subnets.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 716691,
          "date": "Sat 12 Nov 2022 14:00",
          "username": "\t\t\t\tmabotega\t\t\t",
          "content": "Answer A for: The EC2 instances and the RDS DB instance should not be exposed to the public internet. Answer D for: The EC2 instances require internet access to complete payment processing of orders through a third-party web service. Answer A for: The application must be highly available.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>We will require 2 private subnets, D does mention 1 subnet</li></ul>",
          "upvote_count": "13",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 721948,
          "date": "Sat 19 Nov 2022 11:55",
          "username": "\t\t\t\tAbhiJo\t\t\t",
          "content": "We will require 2 private subnets, D does mention 1 subnet",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 844542,
          "date": "Mon 20 Mar 2023 05:48",
          "username": "\t\t\t\tSuketuKohli\t\t\t",
          "content": "https://docs.aws.amazon.com/prescriptive-guidance/latest/load-balancer-stickiness/subnets-routing.html ALB should be in Public Subnet",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 829427,
          "date": "Sat 04 Mar 2023 22:34",
          "username": "\t\t\t\tSdraju\t\t\t",
          "content": "A&D<br>ALb associated with public subnets and the route table configured for local traffic flow. <br>NAT gateways allow for internet connectivity for EC2 instances",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 825389,
          "date": "Wed 01 Mar 2023 00:36",
          "username": "\t\t\t\tKZM\t\t\t",
          "content": "No public subnet is needed I think.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>How to implement NAT GW if you don't have public subnet?</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 836826,
          "date": "Sun 12 Mar 2023 10:03",
          "username": "\t\t\t\tVeseljkoD\t\t\t",
          "content": "How to implement NAT GW if you don't have public subnet?",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 815917,
          "date": "Mon 20 Feb 2023 22:46",
          "username": "\t\t\t\tSmartDude\t\t\t",
          "content": "A&D(First D) as EC2 is in AutoScaling group.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 815505,
          "date": "Mon 20 Feb 2023 17:11",
          "username": "\t\t\t\tAlhaz\t\t\t",
          "content": "A NAT gateway is a Network Address Translation (NAT) service. You can use a NAT gateway so that instances in a private subnet can connect to services outside your VPC but external services cannot initiate a connection with those instances. <br>We dont need to use any public subnet hence D and E is out<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>if you don't use public subnet,where will you place ur LB and NAT gateway</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 833127,
          "date": "Wed 08 Mar 2023 16:44",
          "username": "\t\t\t\tNdekeh1\t\t\t",
          "content": "if you don't use public subnet,where will you place ur LB and NAT gateway",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 795819,
          "date": "Thu 02 Feb 2023 07:25",
          "username": "\t\t\t\twchoi189\t\t\t",
          "content": "If the ec2 instances should not be exposed to the internet how can they be able to connect to the internet to process the payments? I don't think the question makes much sense to me. I think the question intended to say that the RDS should not be exposed to the internet.If so, CE would be correct. Otherwise, AE. <br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Exposing to internet means, a connection originated from internet can target your EC2 instance.<br>While have internet access to payment gateways, you can use NAT gateway and only traffic from internet will be allowed for which a session was originated from your EC2 instance. Hope it helps.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 823089,
          "date": "Mon 27 Feb 2023 01:59",
          "username": "\t\t\t\tMYN\t\t\t",
          "content": "Exposing to internet means, a connection originated from internet can target your EC2 instance.<br>While have internet access to payment gateways, you can use NAT gateway and only traffic from internet will be allowed for which a session was originated from your EC2 instance. Hope it helps.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 779147,
          "date": "Tue 17 Jan 2023 18:38",
          "username": "\t\t\t\tremand\t\t\t",
          "content": "A.  Use an Auto Scaling group to launch the EC2 instances in private subnets. Deploy an RDS Multi-AZ DB instance in private subnets.B.  Configure a VPC with two private subnets and two NAT gateways across two Availability Zones. Deploy an Application Load Balancer in the private subnets.<br><br>Option A meets the requirement of keeping the EC2 instances and the RDS DB instance private by launching them in private subnets. Option B meets the requirement of providing internet access to the EC2 instances for payment processing by configuring NAT gateways in the VPC, and also meets the requirement of high availability by deploying the Application Load Balancer across multiple availability zones.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 777646,
          "date": "Mon 16 Jan 2023 13:45",
          "username": "\t\t\t\tEllo2023\t\t\t",
          "content": "A and E. ",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 775945,
          "date": "Sat 14 Jan 2023 22:26",
          "username": "\t\t\t\tmarcioicebr\t\t\t",
          "content": "A and B.  Privates subnets<br><br>https://docs.aws.amazon.com/pt_br/vpc/latest/userguide/vpc-nat-gateway.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 764486,
          "date": "Tue 03 Jan 2023 12:02",
          "username": "\t\t\t\taba2s\t\t\t",
          "content": "D here is the last D.  There is mistake on the letter with two D. <br>The following link can help getting an idea.<br>https://medium.com/awesome-cloud/aws-vpc-difference-between-internet-gateway-and-nat-gateway-c9177e710af6",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 763916,
          "date": "Mon 02 Jan 2023 19:44",
          "username": "\t\t\t\tHayLLlHuK\t\t\t",
          "content": "A and E!<br>Application has to be highly available while the instance and database should not be exposed to the public internet, but the instances still requires access to the internet. NAT gateway has to be deployed in public subnets in this case while instances and database remain in private subnets in the VPC, therefore answer is (A) and (E).<br>https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html<br><br>If the instances did not require access to the internet, then the answer could have been<br>(B) to use a private NAT gateway and keep it in the private subnets to communicate only to the VPCs.<br><br>https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario2.html",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 763285,
          "date": "Sun 01 Jan 2023 18:43",
          "username": "\t\t\t\tMindvision\t\t\t",
          "content": "AE = correct answer <br>ES2 Instances, RDS DB instances must not be exposed to the internet. So it's to be deployed in private subnet. Public subnet is needed so the resources in the private subnet can access the internet by using NAT gateway. <br>Should be highly available - Auto Scaling group and RDS Multi-AZ DB instance across Availability Zones<br>Reason for 2 public and 2 private subnets and 2 NAT gateways is that the subnets don't span across availability zones.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 762686,
          "date": "Sat 31 Dec 2022 16:00",
          "username": "\t\t\t\tZerotn3\t\t\t",
          "content": "The EC2 instances and the RDS DB instance should not be exposed to the public internet, so they should be placed in private subnets.<br>To ensure high availability, the EC2 instances should be launched in an Auto Scaling group, and the RDS DB instance should be deployed as a Multi-AZ (multi-availability zone) instance.<br>To allow the EC2 instances to access the internet for payment processing, the VPC should have NAT gateways in multiple availability zones.<br>The Application Load Balancer should be deployed in the private subnets to ensure that it is not exposed to the public internet.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>NAT gateway should be in public subnet, so B is incorrect. The answers of this question are very unclear by not clearly mentioning how NAT GW is configured.</li><li>What is the reason to not expose the ALB to public? I have seen architectures, where ALBs or NLs are part of public subnets, :)</li><li>Option C is incorrect because it places the EC2 instances in public subnets, which exposes them to the public internet.<br>Option D is incorrect because it has only one NAT gateway, which does not meet the requirement for high availability.<br>Option E is incorrect because it has both public and private subnets, but the EC2 instances and the RDS DB instance should be placed in private subnets to prevent them from being exposed to the public internet.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 775518,
          "date": "Sat 14 Jan 2023 15:35",
          "username": "\t\t\t\tgoodmail\t\t\t",
          "content": "NAT gateway should be in public subnet, so B is incorrect. The answers of this question are very unclear by not clearly mentioning how NAT GW is configured.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 767792,
          "date": "Fri 06 Jan 2023 15:28",
          "username": "\t\t\t\tMahadeva\t\t\t",
          "content": "What is the reason to not expose the ALB to public? I have seen architectures, where ALBs or NLs are part of public subnets, :)",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 762687,
          "date": "Sat 31 Dec 2022 16:00",
          "username": "\t\t\t\tZerotn3\t\t\t",
          "content": "Option C is incorrect because it places the EC2 instances in public subnets, which exposes them to the public internet.<br>Option D is incorrect because it has only one NAT gateway, which does not meet the requirement for high availability.<br>Option E is incorrect because it has both public and private subnets, but the EC2 instances and the RDS DB instance should be placed in private subnets to prevent them from being exposed to the public internet.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 761717,
          "date": "Fri 30 Dec 2022 07:19",
          "username": "\t\t\t\tjayshinde\t\t\t",
          "content": "E option is not available here",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 759395,
          "date": "Wed 28 Dec 2022 06:39",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The correct answers are Option A and Option B.  To meet the requirements of the eCommerce website, the solutions architect should use an Auto Scaling group to launch the EC2 instances in private subnets and deploy an RDS Multi-AZ DB instance in private subnets. Additionally, the VPC should be configured with two private subnets and two NAT gateways across two Availability Zones, and an Application Load Balancer should be deployed in the private subnets.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>A NAT Gateway must be in a public subnet because only devices on public subnets can actually use a public IP address. https://serverfault.com/questions/854475/aws-nat-gateway-in-public-subnet-why</li><li>Options A &amp; B meet the requirements because it ensures that the EC2 instances and the RDS DB instance are not exposed to the public internet and are highly available.<br><br>The Auto Scaling group in Option A provides scalability, and the use of private subnets and a Multi-AZ RDS DB instance ensures high availability. The use of two NAT gateways in Option B across two Availability Zones provides high availability, and the Application Load Balancer in the private subnets ensures that traffic to the web tier is not exposed to the public internet.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 766199,
          "date": "Thu 05 Jan 2023 03:11",
          "username": "\t\t\t\tkmliuy73\t\t\t",
          "content": "A NAT Gateway must be in a public subnet because only devices on public subnets can actually use a public IP address. https://serverfault.com/questions/854475/aws-nat-gateway-in-public-subnet-why",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 759403,
          "date": "Wed 28 Dec 2022 06:44",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Options A & B meet the requirements because it ensures that the EC2 instances and the RDS DB instance are not exposed to the public internet and are highly available.<br><br>The Auto Scaling group in Option A provides scalability, and the use of private subnets and a Multi-AZ RDS DB instance ensures high availability. The use of two NAT gateways in Option B across two Availability Zones provides high availability, and the Application Load Balancer in the private subnets ensures that traffic to the web tier is not exposed to the public internet.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 758757,
          "date": "Tue 27 Dec 2022 17:03",
          "username": "\t\t\t\tChirantan\t\t\t",
          "content": "Answer should be A and E",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#126",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A solutions architect needs to implement a solution to reduce a company's storage costs. All the company's data is in the Amazon S3 Standard storage class. The company must keep all data for at least 25 years. Data from the most recent 2 years must be highly available and immediately retrievable.<br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#126",
          "answers": [
            {
              "choice": "<p>A. Set up an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive immediately.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Set up an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive after 2 years.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use S3 Intelligent-Tiering. Activate the archiving option to ensure that data is archived in S3 Glacier Deep Archive.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Set up an S3 Lifecycle policy to transition objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) immediately and to S3 Glacier Deep Archive after 2 years.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 720882,
          "date": "Thu 17 Nov 2022 22:31",
          "username": "\t\t\t\tTelaO\t\t\t",
          "content": "B is the only right answer. C does not indicate archiving after 2 years.If it did specify 2 years, then C would also be an option.",
          "upvote_count": "5",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 798940,
          "date": "Sun 05 Feb 2023 16:34",
          "username": "\t\t\t\tCaoMengde09\t\t\t",
          "content": "It's pretty straight forward.<br><br>S3 Standard answers for High Availaibility/Immediate retrieval for 2 years. S3 Intelligent Tiering would just incur additional cost of analysis while the company insures that it requires immediate retrieval in any moment and without risk to Availability. So a capital B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 792037,
          "date": "Sun 29 Jan 2023 21:25",
          "username": "\t\t\t\tG3\t\t\t",
          "content": "C appears to be appropriate - good case for intelligent tiering<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Intelligent tiering appears to be best suited for unknown usage pattern.. but with a known usage pattern Life cycle policy may be optimal.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 829432,
          "date": "Sat 04 Mar 2023 22:39",
          "username": "\t\t\t\tSdraju\t\t\t",
          "content": "Intelligent tiering appears to be best suited for unknown usage pattern.. but with a known usage pattern Life cycle policy may be optimal.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 773787,
          "date": "Thu 12 Jan 2023 19:37",
          "username": "\t\t\t\tDaveNL\t\t\t",
          "content": "C.  Use S3 Intelligent-Tiering. Activate the archiving option to ensure that data is archived in S3 Glacier Deep Archive.<br><br>S3 Intelligent Tiering supports changing the default archival time to 730 days (2 years) from the default 90 or 180 days.Other levels of tiering are instant access tiers.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 762691,
          "date": "Sat 31 Dec 2022 16:17",
          "username": "\t\t\t\tZerotn3\t\t\t",
          "content": "Option D is the correct solution for this scenario.<br><br>S3 Lifecycle policies allow you to automatically transition objects to different storage classes based on the age of the object or other specific criteria. In this case, the company needs to keep all data for at least 25 years, and the data from the most recent 2 years must be highly available and immediately retrievable.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>If the option for D was Infrequent Access it would be good, but here it is One Zone-IA which is not highly available. Then it must be B</li><li>Option A is not a good solution because it would transition all objects to S3 Glacier Deep Archive immediately, making the data from the most recent 2 years not immediately retrievable. Option B is not a good solution because it would not make the data from the most recent 2 years immediately retrievable.<br><br>Option C is not a good solution because S3 Intelligent-Tiering is designed to automatically move objects between two storage classes (Standard and Infrequent Access) based on object access patterns. It does not provide a way to transition objects to S3 Glacier Deep Archive, which is required for long-term storage.<br><br>Option D is the correct solution because it would transition objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) immediately, making the data from the most recent 2 years immediately retrievable. After 2 years, the objects would be transitioned to S3 Glacier Deep Archive for long-term storage. This solution meets the requirements of the company to keep all data for at least 25 years and make the data from the most recent 2 years immediately retrievable.</li><li>B is immediately retrievable, has high availability and using the lifecycle you can transition to deep archive after the 2 years time period.</li><li>S3 One Zone-IA is not highly available compared with S3 standard<br>https://aws.amazon.com/about-aws/whats-new/2018/04/announcing-s3-one-zone-infrequent-access-a-new-amazon-s3-storage-class/?nc1=h_ls</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 770357,
          "date": "Mon 09 Jan 2023 13:32",
          "username": "\t\t\t\tlfrad\t\t\t",
          "content": "If the option for D was Infrequent Access it would be good, but here it is One Zone-IA which is not highly available. Then it must be B",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 762692,
          "date": "Sat 31 Dec 2022 16:18",
          "username": "\t\t\t\tZerotn3\t\t\t",
          "content": "Option A is not a good solution because it would transition all objects to S3 Glacier Deep Archive immediately, making the data from the most recent 2 years not immediately retrievable. Option B is not a good solution because it would not make the data from the most recent 2 years immediately retrievable.<br><br>Option C is not a good solution because S3 Intelligent-Tiering is designed to automatically move objects between two storage classes (Standard and Infrequent Access) based on object access patterns. It does not provide a way to transition objects to S3 Glacier Deep Archive, which is required for long-term storage.<br><br>Option D is the correct solution because it would transition objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) immediately, making the data from the most recent 2 years immediately retrievable. After 2 years, the objects would be transitioned to S3 Glacier Deep Archive for long-term storage. This solution meets the requirements of the company to keep all data for at least 25 years and make the data from the most recent 2 years immediately retrievable.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>B is immediately retrievable, has high availability and using the lifecycle you can transition to deep archive after the 2 years time period.</li><li>S3 One Zone-IA is not highly available compared with S3 standard<br>https://aws.amazon.com/about-aws/whats-new/2018/04/announcing-s3-one-zone-infrequent-access-a-new-amazon-s3-storage-class/?nc1=h_ls</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 777652,
          "date": "Mon 16 Jan 2023 13:49",
          "username": "\t\t\t\tEllo2023\t\t\t",
          "content": "B is immediately retrievable, has high availability and using the lifecycle you can transition to deep archive after the 2 years time period.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 776509,
          "date": "Sun 15 Jan 2023 13:09",
          "username": "\t\t\t\thahahumble\t\t\t",
          "content": "S3 One Zone-IA is not highly available compared with S3 standard<br>https://aws.amazon.com/about-aws/whats-new/2018/04/announcing-s3-one-zone-infrequent-access-a-new-amazon-s3-storage-class/?nc1=h_ls",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 755447,
          "date": "Sun 25 Dec 2022 07:45",
          "username": "\t\t\t\tk1kavi1\t\t\t",
          "content": "B looks correct",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 749142,
          "date": "Sun 18 Dec 2022 21:09",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 740730,
          "date": "Sat 10 Dec 2022 07:40",
          "username": "\t\t\t\tlapaki\t\t\t",
          "content": "B.  Most correct",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 729126,
          "date": "Mon 28 Nov 2022 14:16",
          "username": "\t\t\t\tCizzla7049\t\t\t",
          "content": "https://aws.amazon.com/blogs/aws/s3-intelligent-tiering-adds-archive-access-tiers/<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>From your link \\\"We added S3 Intelligent-Tiering to Amazon Amazon S3 to solve the problem of using the right storage class and optimizing costs when access patterns are irregular.\\\". But access patterns are not irregular, they are clearly stated on the question, so this is not required.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 752252,
          "date": "Wed 21 Dec 2022 13:03",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "From your link \\\"We added S3 Intelligent-Tiering to Amazon Amazon S3 to solve the problem of using the right storage class and optimizing costs when access patterns are irregular.\\\". But access patterns are not irregular, they are clearly stated on the question, so this is not required.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 723948,
          "date": "Mon 21 Nov 2022 22:31",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 720345,
          "date": "Thu 17 Nov 2022 10:00",
          "username": "\t\t\t\trjam\t\t\t",
          "content": "Why Not C? Because Intelligent Tierthe objects are automatically moved to different tiers.<br>The question says \\\"the data from most recent 2 yrs should be highly available and immediately retrievable\\\", which means in intelligent tier , if you activate archiving option(as Option C specifies) , the objects will be moved to Archive tiers(instant to access to deep archive access tiers) in 90 to 730 days. Remember these archive tiers performance will be similar to S3 glacier flexible and s3 deep archive which means files cannot be retrieved immediately within 2 yrs .<br><br>We have a hard requirement in question which says it should be retreivable immediately for the 2 yrs.which cannot be acheived in Intelligent tier.So B is the correct option imho.<br><br>Because of the above reason Its possible only in S3 standard and then configure lifecycle configuration to move to S3 Glacier Deep Archive after 2 yrs.",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 717478,
          "date": "Sun 13 Nov 2022 20:32",
          "username": "\t\t\t\tJtic\t\t\t",
          "content": "C - S3 Intelligent-Tiering<br>Customers saving on storage with S3 Intelligent-Tiering<br><br>S3 Intelligent-Tiering automatically stores objects in three access tiers: one tier optimized for frequent access, a lower-cost tier optimized for infrequent access, and a very-low-cost tier optimized for rarely accessed data. For a small monthly object monitoring and automation charge, S3 Intelligent-Tiering moves objects that have not been accessed for 30 consecutive days to the Infrequent Access tier for savings of 40%; and after 90 days of no access, they're<br><br>There are no retrieval charges in S3 Intelligent-Tiering. S3 Intelligent-Tiering has no minimum eligible object size, but objects smaller than 128 KB are not eligible for auto tiering. These smaller objects may be stored, but they'll always be charged at the Frequent Access tier rates and don't incur the monitoring and automation charge<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>\\\"moves objects that have not been accessed for 30 consecutive days to the Infrequent Access tier...\\\" This is not required, they should remain where they are for 2 years.</li><li>Once you have activated one or both of the archive access tiers, S3 Intelligent-Tiering will automatically move objects that haven't been accessed for 90 days to the Archive Access tier, ...Objects in the archive access tiers are retrieved in 3-5 hours!<br>Yet the requirements are \\\"Data from the most recent 2 years must be highly available and immediately retrievable\\\". Not C!</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 752253,
          "date": "Wed 21 Dec 2022 13:04",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "\\\"moves objects that have not been accessed for 30 consecutive days to the Infrequent Access tier...\\\" This is not required, they should remain where they are for 2 years.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Once you have activated one or both of the archive access tiers, S3 Intelligent-Tiering will automatically move objects that haven't been accessed for 90 days to the Archive Access tier, ...Objects in the archive access tiers are retrieved in 3-5 hours!<br>Yet the requirements are \\\"Data from the most recent 2 years must be highly available and immediately retrievable\\\". Not C!</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 752257,
          "date": "Wed 21 Dec 2022 13:07",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "Once you have activated one or both of the archive access tiers, S3 Intelligent-Tiering will automatically move objects that haven't been accessed for 90 days to the Archive Access tier, ...Objects in the archive access tiers are retrieved in 3-5 hours!<br>Yet the requirements are \\\"Data from the most recent 2 years must be highly available and immediately retrievable\\\". Not C!",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 717346,
          "date": "Sun 13 Nov 2022 15:11",
          "username": "\t\t\t\tDeplake\t\t\t",
          "content": "Option C doesn't look correct for me because it is not clear when it will be moved to the Deep Archive. It could be earlier then 2 years, which is not correct<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>https://docs.aws.amazon.com/AmazonS3/latest/userguide/intelligent-tiering-overview.html#:~:text=S3%20Intelligent%2DTiering%20provides%20you,minimum%20of%2090%20consecutive%20days. Option B /S3 Glacier Deep Archive seems correct to reduce a company's storage costs.</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 718998,
          "date": "Tue 15 Nov 2022 18:57",
          "username": "\t\t\t\tWilson_S\t\t\t",
          "content": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/intelligent-tiering-overview.html#:~:text=S3%20Intelligent%2DTiering%20provides%20you,minimum%20of%2090%20consecutive%20days. Option B /S3 Glacier Deep Archive seems correct to reduce a company's storage costs.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 716865,
          "date": "Sat 12 Nov 2022 19:43",
          "username": "\t\t\t\tMyNameIsJulien\t\t\t",
          "content": "The answer C seems correct",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 713907,
          "date": "Tue 08 Nov 2022 15:51",
          "username": "\t\t\t\tArielSchivo\t\t\t",
          "content": "Glacier Deep Archive restores objects within 12 hours, so option A is out.<br>Option B could work but you will be paying S3 Standard for 2 years.<br>I would go with Option C then.<br>Option D is out since S3 One Zone IA is not highly available.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 708730,
          "date": "Mon 31 Oct 2022 22:27",
          "username": "\t\t\t\trjam\t\t\t",
          "content": "Option D as one-zone IA is cheaper than standard s3 . they never mentioned about multi zone. so we will go for one zone IA.  The question mainly talks about reducing storage costs<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Data from the most recent 2 years must be highly available and immediately retrievable.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 709990,
          "date": "Wed 02 Nov 2022 19:31",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "Data from the most recent 2 years must be highly available and immediately retrievable.",
          "upvote_count": "5",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#127",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A media company is evaluating the possibility of moving its systems to the AWS Cloud. The company needs at least 10 TB of storage with the maximum possible I/O performance for video processing, 300 TB of very durable storage for storing media content, and 900 TB of storage to meet requirements for archival media that is not in use anymore.<br>Which set of services should a solutions architect recommend to meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#127",
          "answers": [
            {
              "choice": "<p>A. Amazon EBS for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Amazon EBS for maximum performance, Amazon EFS for durable data storage, and Amazon S3 Glacier for archival storage<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Amazon EC2 instance store for maximum performance, Amazon EFS for durable data storage, and Amazon S3 for archival storage<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Amazon EC2 instance store for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 699085,
          "date": "Wed 19 Oct 2022 16:01",
          "username": "\t\t\t\tSauran\t\t\t",
          "content": "Max instance store possible at this time is 30TB for NVMe which has the higher I/O compared to EBS.<br><br>is4gen.8xlarge 4 x 7,500 GB (30 TB) NVMe SSD<br><br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html#instance-store-volumes<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>instance store volume for the root volume, the size of this volume varies by AMI, but the maximum size is 10 GB</li><li>This link shows a max capacity of 30TB, so what is the problem? https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html#instance-store-volumes</li><li>Only the following instance types support an instance store volume as the root device: C3, D2, G2, I2, M3, and R3, and we're using an I3, so an instance store volume is irrelevant.</li></ul>",
          "upvote_count": "18",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 749826,
          "date": "Mon 19 Dec 2022 13:52",
          "username": "\t\t\t\tishitamodi4\t\t\t",
          "content": "instance store volume for the root volume, the size of this volume varies by AMI, but the maximum size is 10 GB<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>This link shows a max capacity of 30TB, so what is the problem? https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html#instance-store-volumes</li><li>Only the following instance types support an instance store volume as the root device: C3, D2, G2, I2, M3, and R3, and we're using an I3, so an instance store volume is irrelevant.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 752262,
          "date": "Wed 21 Dec 2022 13:14",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "This link shows a max capacity of 30TB, so what is the problem? https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html#instance-store-volumes<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Only the following instance types support an instance store volume as the root device: C3, D2, G2, I2, M3, and R3, and we're using an I3, so an instance store volume is irrelevant.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 752265,
          "date": "Wed 21 Dec 2022 13:18",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "Only the following instance types support an instance store volume as the root device: C3, D2, G2, I2, M3, and R3, and we're using an I3, so an instance store volume is irrelevant.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 704858,
          "date": "Wed 26 Oct 2022 18:21",
          "username": "\t\t\t\tSix_Fingered_Jose\t\t\t",
          "content": "agree with D, since it is only used for video processing instance store should be the fastest here (being ephemeral shouldnt be an issue because they are moving the data to S3 after processing)",
          "upvote_count": "6",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 845951,
          "date": "Tue 21 Mar 2023 13:48",
          "username": "\t\t\t\tErbug\t\t\t",
          "content": "When you want to compare S3 storage and EBS as durable storage types according to the maximum IOPS, you will see that s3 is better than EBS based on storage-optimized values.<br>Exp: Whereas EBS has 40000 max IOPS for storage-optimized value, EC2 provides you a better option with a max of 2146664 random read and 1073336 write.<br>To get further information, you can visit the below links:<br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/compute-optimized-instances.html#compute-ssd-perf<br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-optimized.html<br><br>So my answer is D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 829447,
          "date": "Sat 04 Mar 2023 22:57",
          "username": "\t\t\t\tSdraju\t\t\t",
          "content": "Instance store for max I/O, S3 for durable storage and Glacier for archival",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 819526,
          "date": "Thu 23 Feb 2023 19:09",
          "username": "\t\t\t\tanthony2021\t\t\t",
          "content": "The issue with using an instance store that size seems to be you have to have a specific ami, but paying for an 8xlarge for those extra IO will normally not be a good solution, the question is open as to compute requirments and cost isn't mentioned",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 818644,
          "date": "Thu 23 Feb 2023 02:46",
          "username": "\t\t\t\tbdp123\t\t\t",
          "content": "for valuable, long-term data. Instead, use more durable data storage, such as Amazon S3, Amazon EBS, or Amazon EFS.<br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 815932,
          "date": "Mon 20 Feb 2023 23:06",
          "username": "\t\t\t\tSmartDude\t\t\t",
          "content": "---Chat GTP-----<br>There are several Amazon EC2 instance types that support 30 TB of instance store volume storage. The specific instance types available may vary depending on the AWS region. Here are a few examples of EC2 instance types that support 30 TB of instance store:<br><br>i3en.24xlarge: This instance type is part of the I3en family of instances and provides 24 vCPUs, 96 GiB of memory, and 30.5 TB of NVMe SSD instance store. It is optimized for high-performance workloads and applications that require large amounts of storage, such as data warehousing, Hadoop, and NoSQL databases.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 791159,
          "date": "Sat 28 Jan 2023 23:29",
          "username": "\t\t\t\tVicky_2023\t\t\t",
          "content": "A & D looks most close. But in question it never gives a clue for temporary storage asAWS EC2 instance store is \\\" An instance store provides temporary block-level storage for your instance\\\" Hence I will choose A as per my understanding. Pls correct if I am wrong.<br>Ref#https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 786073,
          "date": "Tue 24 Jan 2023 03:29",
          "username": "\t\t\t\tLuckyAro\t\t\t",
          "content": "EBS is more durable than Instance store, I don't think anyone would risk that much data on a non-durable storage system.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 778345,
          "date": "Mon 16 Jan 2023 23:24",
          "username": "\t\t\t\tmackeda\t\t\t",
          "content": "A, Amazon EBS for high I/O compute performance",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 777658,
          "date": "Mon 16 Jan 2023 13:57",
          "username": "\t\t\t\tEllo2023\t\t\t",
          "content": "A.  It says \\\"The company needs at least 10 TB of STORAGE with the MAXIMUM possible I/O performance for video processing\\\" for high performance it is instance store but the risk is that instance storage is ephemeral, if anything happens than than 10TB of storage is lost. There is no High Availability. Where as EBS has HA and use IO2 to maximise performance. <br>Correct me if i am wrong.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 763094,
          "date": "Sun 01 Jan 2023 08:34",
          "username": "\t\t\t\tZerotn3\t\t\t",
          "content": "Amazon Elastic Block Store (EBS) is a service that provides raw block-level storage for Amazon Elastic Compute Cloud (EC2) instances. It is designed to provide high performance for workloads that require the lowest possible latency, such as video processing.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Amazon Elastic Compute Cloud (EC2) instance store is a temporary storage option that is located on the same physical hardware as the EC2 instance. It is designed to provide high performance for workloads that require the lowest possible latency, such as video processing. However, instance store data is not persisted when the EC2 instance is stopped or terminated, so it is not a good fit for storing data that needs to be persisted long-term.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 763095,
          "date": "Sun 01 Jan 2023 08:34",
          "username": "\t\t\t\tZerotn3\t\t\t",
          "content": "Amazon Elastic Compute Cloud (EC2) instance store is a temporary storage option that is located on the same physical hardware as the EC2 instance. It is designed to provide high performance for workloads that require the lowest possible latency, such as video processing. However, instance store data is not persisted when the EC2 instance is stopped or terminated, so it is not a good fit for storing data that needs to be persisted long-term.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 761128,
          "date": "Thu 29 Dec 2022 15:37",
          "username": "\t\t\t\tmp165\t\t\t",
          "content": "I was going A. ...but after reading this. EC2 has newer feature to support video<br><br>https://aws.amazon.com/premiumsupport/knowledge-center/instance-store-vs-ebs/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 759427,
          "date": "Wed 28 Dec 2022 07:10",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The correct answer is D.  To meet the requirements, the solutions architect should recommend using Amazon EC2 instance store for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage.<br><br>Amazon EC2 is a good fit for the requirement of 10 TB of storage with the maximum possible I/O performance for video processing.<br><br>Amazon S3 is a good fit for the requirement of 300 TB of very durable storage for storing media content.<br><br>Amazon S3 Glacier is a good fit for the requirement of 900 TB of storage to meet the requirements for archival media that is not in use anymore.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 757690,
          "date": "Mon 26 Dec 2022 18:28",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "Max Instance Store is 30 TB ,so our requirment is getting fulfilled here.Instance store will give high iops,COMPARE to EBS.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 757100,
          "date": "Mon 26 Dec 2022 07:02",
          "username": "\t\t\t\tmuhtoy\t\t\t",
          "content": "A solutions architect should recommend Amazon EBS for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage.<br><br>Amazon EBS is a block storage service that provides high I/O performance for applications such as video processing. It is suitable for the company's requirement of at least 10 TB of storage with the maximum possible I/O performance.<br><br>Amazon S3 is a durable object storage service that can store unlimited amounts of data with 99.999999999% durability. It is suitable for the company's requirement of 300 TB of very durable storage for storing media content.<br><br>Amazon S3 Glacier is a secure, durable, and extremely low-cost Amazon S3 storage class for data archiving and long-term backup. It is suitable for the company's requirement of 900 TB of storage for archival media that is not in use anymore.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 749148,
          "date": "Sun 18 Dec 2022 21:17",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option D - You will have to select right instance type that can support 10TB of instance store.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        }
      ]
    },
    {
      "question_id": "#128",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company wants to run applications in containers in the AWS Cloud. These applications are stateless and can tolerate disruptions within the underlying infrastructure. The company needs a solution that minimizes cost and operational overhead.<br>What should a solutions architect do to meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#128",
          "answers": [
            {
              "choice": "<p>A. Use Spot Instances in an Amazon EC2 Auto Scaling group to run the application containers.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use Spot Instances in an Amazon Elastic Kubernetes Service (Amazon EKS) managed node group.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use On-Demand Instances in an Amazon EC2 Auto Scaling group to run the application containers.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use On-Demand Instances in an Amazon Elastic Kubernetes Service (Amazon EKS) managed node group.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 752285,
          "date": "Wed 21 Dec 2022 13:36",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "Running your Kubernetes and containerized workloads on Amazon EC2 Spot Instances is a great way to save costs. ... AWS makes it easy to run Kubernetes with Amazon Elastic Kubernetes Service (EKS) a managed Kubernetes service to run production-grade workloads on AWS. To cost optimize these workloads, run them on Spot Instances. https://aws.amazon.com/blogs/compute/cost-optimization-and-resilience-eks-with-spot-instances/",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 837659,
          "date": "Mon 13 Mar 2023 06:35",
          "username": "\t\t\t\tbgsanata\t\t\t",
          "content": "The answer should be D.  Spot instance is not good option at all. The question say \\\"...can tolerate disruptions\\\" this doesn't mean it can run at random time intervals.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 831915,
          "date": "Tue 07 Mar 2023 14:04",
          "username": "\t\t\t\tGalileoEC2\t\t\t",
          "content": "Answer is A:<br>Amazon ECS: ECS itself is free, you pay only for Amazon EC2 resources you use.<br>Amazon EKS: The EKS management layer incurs an additional cost of $144 per month per cluster.<br>Advantages of Amazon ECS include: Spot instances: Because containers are immutable, you can run many workloads using Amazon EC2 Spot Instances (which can be shut down with no advance notice) and save 90% on on-demand instance costs.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 829450,
          "date": "Sat 04 Mar 2023 23:02",
          "username": "\t\t\t\tSdraju\t\t\t",
          "content": "Spot instances for cost optimisation and Kubernetes for container management",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 763101,
          "date": "Sun 01 Jan 2023 08:58",
          "username": "\t\t\t\tZerotn3\t\t\t",
          "content": "A and B are working. but requirements have \\\"operational overhead\\\". EKS would allow the company to use Amazon EKS to manage the containerized applications.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 759428,
          "date": "Wed 28 Dec 2022 07:11",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The correct answer is B.  To minimize cost and operational overhead, the solutions architect should use Spot Instances in an Amazon Elastic Kubernetes Service (Amazon EKS) managed node group to run the application containers.<br><br>Amazon EKS is a fully managed service that makes it easy to run Kubernetes on AWS. By using a managed node group, the company can take advantage of the operational benefits of Amazon EKS while minimizing the operational overhead of managing the Kubernetes infrastructure. Spot Instances provide a cost-effective way to run stateless, fault-tolerant applications in containers, making them a good fit for the company's requirements.",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 749150,
          "date": "Sun 18 Dec 2022 21:19",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 743143,
          "date": "Mon 12 Dec 2022 19:05",
          "username": "\t\t\t\tQjb8m9h\t\t\t",
          "content": "B.  Use Spot Instances - Supports Disruption ( stop and start at anytime)<br>Elastic Kubernetes Service (Amazon EKS) managed node group - Supports containerized application.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 737358,
          "date": "Wed 07 Dec 2022 02:52",
          "username": "\t\t\t\tjiemin\t\t\t",
          "content": "why not A, EC2 can run container with lower cost than EKS...<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>There are no additional costs to use Amazon EKS managed node groups, you only pay for the AWS resources you provision, so I disagree</li></ul>",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 752281,
          "date": "Wed 21 Dec 2022 13:33",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "There are no additional costs to use Amazon EKS managed node groups, you only pay for the AWS resources you provision, so I disagree",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 723950,
          "date": "Mon 21 Nov 2022 22:34",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 720145,
          "date": "Thu 17 Nov 2022 03:31",
          "username": "\t\t\t\tstudy_aws1\t\t\t",
          "content": "This should explain<br><br>https://docs.aws.amazon.com/eks/latest/userguide/managed-node-groups.html",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 716692,
          "date": "Sat 12 Nov 2022 14:02",
          "username": "\t\t\t\tmabotega\t\t\t",
          "content": "Answer B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 704859,
          "date": "Wed 26 Oct 2022 18:23",
          "username": "\t\t\t\tSix_Fingered_Jose\t\t\t",
          "content": "agreed with B cause container",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 704448,
          "date": "Wed 26 Oct 2022 08:43",
          "username": "\t\t\t\ttubtab\t\t\t",
          "content": "bbbbbb",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 694433,
          "date": "Fri 14 Oct 2022 03:30",
          "username": "\t\t\t\tLilibell\t\t\t",
          "content": "The answer is B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 694034,
          "date": "Thu 13 Oct 2022 17:24",
          "username": "\t\t\t\tbrushek\t\t\t",
          "content": "it should be B:<br><br>https://aws.amazon.com/about-aws/whats-new/2020/12/amazon-eks-support-ec2-spot-instances-managed-node-groups/",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: B"
        }
      ]
    },
    {
      "question_id": "#129",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is running a multi-tier web application on premises. The web application is containerized and runs on a number of Linux hosts connected to a PostgreSQL database that contains user records. The operational overhead of maintaining the infrastructure and capacity planning is limiting the company's growth. A solutions architect must improve the application's infrastructure.<br>Which combination of actions should the solutions architect take to accomplish this? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: AE</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#129",
          "answers": [
            {
              "choice": "<p>A. Migrate the PostgreSQL database to Amazon Aurora.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Migrate the web application to be hosted on Amazon EC2 instances.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Set up an Amazon CloudFront distribution for the web application content.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Set up Amazon ElastiCache between the web application and the PostgreSQL database.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Migrate the web application to be hosted on AWS Fargate with Amazon Elastic Container Service (Amazon ECS).<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 713913,
          "date": "Tue 08 Nov 2022 15:58",
          "username": "\t\t\t\tArielSchivo\t\t\t",
          "content": "I would say A and E since Aurora and Fargate are serverless (less operational overhead).",
          "upvote_count": "6",
          "selected_answers": "Selected Answer: AE"
        },
        {
          "id": 837660,
          "date": "Mon 13 Mar 2023 06:37",
          "username": "\t\t\t\tbgsanata\t\t\t",
          "content": "A and E",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AE"
        },
        {
          "id": 783907,
          "date": "Sun 22 Jan 2023 04:18",
          "username": "\t\t\t\trapatajones\t\t\t",
          "content": "a e..............",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AE"
        },
        {
          "id": 775548,
          "date": "Sat 14 Jan 2023 15:58",
          "username": "\t\t\t\tgoodmail\t\t\t",
          "content": "One should that Aurora is not serverless. Aurora serverless and Aurora are 2 Amazon services. I prefer C, however the question does not mention any frontend requirements.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 764641,
          "date": "Tue 03 Jan 2023 13:52",
          "username": "\t\t\t\taba2s\t\t\t",
          "content": "Yes, go for A and E since thes two ressources are serverless.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: AE"
        },
        {
          "id": 759430,
          "date": "Wed 28 Dec 2022 07:12",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The correct answers are A and E.  To improve the application's infrastructure, the solutions architect should migrate the PostgreSQL database to Amazon Aurora and migrate the web application to be hosted on AWS Fargate with Amazon Elastic Container Service (Amazon ECS).<br><br>Amazon Aurora is a fully managed, scalable, and highly available relational database service that is compatible with PostgreSQL. Migrating the database to Amazon Aurora would reduce the operational overhead of maintaining the database infrastructure and allow the company to focus on building and scaling the application.<br><br>AWS Fargate is a fully managed container orchestration service that enables users to run containers without the need to manage the underlying EC2 instances. By using AWS Fargate with Amazon Elastic Container Service (Amazon ECS), the solutions architect can improve the scalability and efficiency of the web application and reduce the operational overhead of maintaining the underlying infrastructure.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AE"
        },
        {
          "id": 757702,
          "date": "Mon 26 Dec 2022 18:38",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "A and E are obvious choices.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 749154,
          "date": "Sun 18 Dec 2022 21:22",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option A and E",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AE"
        },
        {
          "id": 746256,
          "date": "Thu 15 Dec 2022 16:36",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "A and E",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AE"
        },
        {
          "id": 744560,
          "date": "Wed 14 Dec 2022 00:51",
          "username": "\t\t\t\t333666999\t\t\t",
          "content": "C not A.  and E",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: CE"
        },
        {
          "id": 723953,
          "date": "Mon 21 Nov 2022 22:35",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "A and E",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 713561,
          "date": "Tue 08 Nov 2022 08:29",
          "username": "\t\t\t\tNigma\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/46457-exam-aws-certified-solutions-architect-associate-saa-c02/<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>A and E<br><br>Aurora and serverless</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 713563,
          "date": "Tue 08 Nov 2022 08:30",
          "username": "\t\t\t\tNigma\t\t\t",
          "content": "A and E<br><br>Aurora and serverless",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 707622,
          "date": "Sun 30 Oct 2022 06:28",
          "username": "\t\t\t\tSimonPark\t\t\t",
          "content": "B(X) E(O) not sure about A,C,D but A looks making sense",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AE"
        }
      ]
    },
    {
      "question_id": "#130",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>An application runs on Amazon EC2 instances across multiple Availability Zonas. The instances run in an Amazon EC2 Auto Scaling group behind an Application Load Balancer. The application performs best when the CPU utilization of the EC2 instances is at or near 40%.<br>What should a solutions architect do to maintain the desired performance across all instances in the group?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#130",
          "answers": [
            {
              "choice": "<p>A. Use a simple scaling policy to dynamically scale the Auto Scaling group.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use a target tracking policy to dynamically scale the Auto Scaling group.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use an AWS Lambda function ta update the desired Auto Scaling group capacity.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use scheduled scaling actions to scale up and scale down the Auto Scaling group.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 764697,
          "date": "Tue 03 Jan 2023 14:43",
          "username": "\t\t\t\taba2s\t\t\t",
          "content": "B seem to the correct response.<br><br>With a target tracking scaling policy, you can increase or decrease the current capacity of the group based on a target value for a specific metric. This policy will help resolve the over-provisioning of your resources. The scaling policy adds or removes capacity as required to keep the metric at, or close to, the specified target value. In addition to keeping the metric close to the target value, a target tracking scaling policy also adjusts to changes in the metric due to a changing load pattern.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 759433,
          "date": "Wed 28 Dec 2022 07:16",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The correct answer is B.  To maintain the desired performance across all instances in the Amazon EC2 Auto Scaling group, the solutions architect should use a target tracking policy to dynamically scale the Auto Scaling group.<br><br>A target tracking policy allows the Auto Scaling group to automatically adjust the number of EC2 instances in the group based on a target value for a metric. In this case, the target value for the CPU utilization metric could be set to 40% to maintain the desired performance of the application. The Auto Scaling group would then automatically scale the number of instances up or down as needed to maintain the target value for the metric.<br><br>https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-simple-step.html",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 754817,
          "date": "Sat 24 Dec 2022 11:52",
          "username": "\t\t\t\torionizzie\t\t\t",
          "content": "target tracking - CPU at 40%",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 748618,
          "date": "Sun 18 Dec 2022 06:17",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 723954,
          "date": "Mon 21 Nov 2022 22:36",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 713914,
          "date": "Tue 08 Nov 2022 16:00",
          "username": "\t\t\t\tArielSchivo\t\t\t",
          "content": "Option B.  Target tracking policy.<br><br>https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 713571,
          "date": "Tue 08 Nov 2022 08:39",
          "username": "\t\t\t\tNigma\t\t\t",
          "content": "B<br><br>CPU utilization = target tracking",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 707625,
          "date": "Sun 30 Oct 2022 06:30",
          "username": "\t\t\t\tSimonPark\t\t\t",
          "content": "B is the answer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        }
      ]
    },
    {
      "question_id": "#131",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is developing a file-sharing application that will use an Amazon S3 bucket for storage. The company wants to serve all the files through an Amazon CloudFront distribution. The company does not want the files to be accessible through direct navigation to the S3 URL.<br>What should a solutions architect do to meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#131",
          "answers": [
            {
              "choice": "<p>A. Write individual policies for each S3 bucket to grant read permission for only CloudFront access.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an IAM user. Grant the user read permission to objects in the S3 bucket. Assign the user to CloudFront.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Write an S3 bucket policy that assigns the CloudFront distribution ID as the Principal and assigns the target S3 bucket as the Amazon Resource Name (ARN).<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an origin access identity (OAI). Assign the OAI to the CloudFront distribution. Configure the S3 bucket permissions so that only the OAI has read permission.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 699608,
          "date": "Thu 20 Oct 2022 08:36",
          "username": "\t\t\t\t123jhl0\t\t\t",
          "content": "I want to restrict access to my Amazon Simple Storage Service (Amazon S3) bucket so that objects can be accessed only through my Amazon CloudFront distribution. How can I do that?<br>Create a CloudFront origin access identity (OAI)<br>https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-access-to-amazon-s3/<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Thanks it convinces me</li></ul>",
          "upvote_count": "18",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 707626,
          "date": "Sun 30 Oct 2022 06:35",
          "username": "\t\t\t\tSimonPark\t\t\t",
          "content": "Thanks it convinces me",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 759452,
          "date": "Wed 28 Dec 2022 07:45",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The correct answer is D.  To meet the requirements, the solutions architect should create an origin access identity (OAI) and assign it to the CloudFront distribution. The S3 bucket permissions should be configured so that only the OAI has read permission.<br><br>An OAI is a special CloudFront user that is associated with a CloudFront distribution and is used to give CloudFront access to the files in an S3 bucket. By using an OAI, the company can serve the files through the CloudFront distribution while preventing direct access to the S3 bucket.<br><br>https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 748619,
          "date": "Sun 18 Dec 2022 06:19",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "D is the right answer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 744398,
          "date": "Tue 13 Dec 2022 20:21",
          "username": "\t\t\t\tgloritown\t\t\t",
          "content": "D is correct but instead of OAI using OAC would be better since OAI is legacy",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 724300,
          "date": "Tue 22 Nov 2022 13:11",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "D is correct",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#132",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company's website provides users with downloadable historical performance reports. The website needs a solution that will scale to meet the company's website demands globally. The solution should be cost-effective, limit the provisioning of infrastructure resources, and provide the fastest possible response time.<br>Which combination should a solutions architect recommend to meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#132",
          "answers": [
            {
              "choice": "<p>A. Amazon CloudFront and Amazon S3<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. AWS Lambda and Amazon DynamoDB<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Application Load Balancer with Amazon EC2 Auto Scaling<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Amazon Route 53 with internal Application Load Balancers<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 707602,
          "date": "Sun 30 Oct 2022 05:42",
          "username": "\t\t\t\tdokaedu\t\t\t",
          "content": "A is the correct answer<br> The solution should be cost-effective, limit the provisioning of infrastructure resources, and provide the fastest possible response time.",
          "upvote_count": "7",
          "selected_answers": ""
        },
        {
          "id": 792040,
          "date": "Sun 29 Jan 2023 21:30",
          "username": "\t\t\t\tG3\t\t\t",
          "content": "Historical reports = Static content = S3",
          "upvote_count": "5",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 759463,
          "date": "Wed 28 Dec 2022 08:06",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The correct answer is Option A.  To meet the requirements, the solutions architect should recommend using Amazon CloudFront and Amazon S3.<br><br>By combining Amazon CloudFront and Amazon S3, the solutions architect can provide a scalable and cost-effective solution that limits the provisioning of infrastructure resources and provides the fastest possible response time.<br><br>https://aws.amazon.com/cloudfront/<br><br>https://aws.amazon.com/s3/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 757704,
          "date": "Mon 26 Dec 2022 18:46",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 748620,
          "date": "Sun 18 Dec 2022 06:22",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "A is the best and most cost effective option if only download of the static pre-created report(no data processing before downloading) is a requirement.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 724301,
          "date": "Tue 22 Nov 2022 13:12",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 717410,
          "date": "Sun 13 Nov 2022 16:57",
          "username": "\t\t\t\tsdasdawa\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/27935-exam-aws-certified-solutions-architect-associate-saa-c02/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 717343,
          "date": "Sun 13 Nov 2022 15:08",
          "username": "\t\t\t\tNirmal3331\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/27935-exam-aws-certified-solutions-architect-associate-saa-c02/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 717166,
          "date": "Sun 13 Nov 2022 09:01",
          "username": "\t\t\t\tsamplunk\t\t\t",
          "content": "See this discussion:<br>https://www.examtopics.com/discussions/amazon/view/27935-exam-aws-certified-solutions-architect-associate-saa-c02/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 717054,
          "date": "Sun 13 Nov 2022 03:00",
          "username": "\t\t\t\tmanu427\t\t\t",
          "content": "load balancing + scalability + cost effective",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 716880,
          "date": "Sat 12 Nov 2022 20:05",
          "username": "\t\t\t\tMyNameIsJulien\t\t\t",
          "content": "I think the answer is B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        }
      ]
    },
    {
      "question_id": "#133",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company runs an Oracle database on premises. As part of the company's migration to AWS, the company wants to upgrade the database to the most recent available version. The company also wants to set up disaster recovery (DR) for the database. The company needs to minimize the operational overhead for normal operations and DR setup. The company also needs to maintain access to the database's underlying operating system.<br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#133",
          "answers": [
            {
              "choice": "<p>A. Migrate the Oracle database to an Amazon EC2 instance. Set up database replication to a different AWS Region.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Migrate the Oracle database to Amazon RDS for Oracle. Activate Cross-Region automated backups to replicate the snapshots to another AWS Region.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Migrate the Oracle database to Amazon RDS Custom for Oracle. Create a read replica for the database in another AWS Region.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Migrate the Oracle database to Amazon RDS for Oracle. Create a standby database in another Availability Zone.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 714519,
          "date": "Wed 09 Nov 2022 12:43",
          "username": "\t\t\t\tArielSchivo\t\t\t",
          "content": "Option C since RDS Custom has access to the underlying OS and it provides less operational overhead. Also, a read replica in another Region can be used for DR activities.<br><br>https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/",
          "upvote_count": "10",
          "selected_answers": ""
        },
        {
          "id": 694231,
          "date": "Thu 13 Oct 2022 21:20",
          "username": "\t\t\t\tbrushek\t\t\t",
          "content": "It should be C:<br><br>https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-custom.html<br>and<br>https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/working-with-custom-oracle.html",
          "upvote_count": "10",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 839539,
          "date": "Wed 15 Mar 2023 04:56",
          "username": "\t\t\t\tcegama543\t\t\t",
          "content": "If you read the question. At the end it says:The company also needs to maintain access to the database's underlying operating system. Only EC2 allows that.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 838025,
          "date": "Mon 13 Mar 2023 16:22",
          "username": "\t\t\t\tmell1222\t\t\t",
          "content": "chat GPT<br><br>Launch a new Amazon RDS database instance with the latest Oracle database engine version.<br>Use AWS DMS to migrate the on-premises Oracle database to the new Amazon RDS instance.<br>Create a standby Amazon RDS instance in a different AWS region to set up disaster recovery.<br>Use AWS CloudFormation to automate the setup of the DR environment and to deploy an Amazon EC2 instance to access the operating system of the database.<br>Configure AWS RDS Multi-AZ deployment for the primary database instance to provide high availability and failover capability.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 816217,
          "date": "Tue 21 Feb 2023 06:14",
          "username": "\t\t\t\tiGotBloodOnMyHat\t\t\t",
          "content": "There's no way to access the underlying OS of a managed service though.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 807636,
          "date": "Mon 13 Feb 2023 18:14",
          "username": "\t\t\t\tYelizaveta\t\t\t",
          "content": "After realy long reserching I came to the conclusion....<br>It just can be A. <br>It is possible to get an read replica in a other region for RDS Custom for Oracle:<br>https://aws.amazon.com/blogs/database/part-2-implement-multi-master-replication-with-rds-custom-for-oracle-high-availability-disaster-recovery/<br><br>BUT because of the sentence \\\"The company needs to minimize the operational overhead for normal operations and DR setup.\\\" now I am pretty shure it is A :D<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>You missed this part \\\"minimize the operational overhead\\\"</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 814420,
          "date": "Sun 19 Feb 2023 20:16",
          "username": "\t\t\t\tHelp2023\t\t\t",
          "content": "You missed this part \\\"minimize the operational overhead\\\"",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 801646,
          "date": "Wed 08 Feb 2023 05:05",
          "username": "\t\t\t\tJiyuKim\t\t\t",
          "content": "the answer should be A. <br>the comapny also need to maintain access to the database's underlying operating sysyem. -> A or C<br>The company also wants to set up DR for the database. -> A<br>Amazon RDS Custom does NOT support Cross-Region read relicas.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 791577,
          "date": "Sun 29 Jan 2023 12:19",
          "username": "\t\t\t\tTECHNOWARRIOR\t\t\t",
          "content": "Migrating an Oracle database to Amazon RDS Custom for Oracle is supported [1], but creating a read replica for the database in another region is not supported. Cross-Region Oracle replicas aren't supported for RDS Custom for Oracle DB<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>it is: <br>https://aws.amazon.com/blogs/database/part-2-implement-multi-master-replication-with-rds-custom-for-oracle-high-availability-disaster-recovery/</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 806169,
          "date": "Sun 12 Feb 2023 10:48",
          "username": "\t\t\t\tYelizaveta\t\t\t",
          "content": "it is: <br>https://aws.amazon.com/blogs/database/part-2-implement-multi-master-replication-with-rds-custom-for-oracle-high-availability-disaster-recovery/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 788456,
          "date": "Thu 26 Jan 2023 07:57",
          "username": "\t\t\t\tMohamed_Samir\t\t\t",
          "content": "\\\"Cross-Region Oracle replicas aren't supported.\\\" for \\\"RDS Custom for Oracle DB\\\"..<br>It is mentioned here in the \\\"General requirements and limitations\\\" section in the following URL:<br>>> https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/custom-rr.html<br>So, I am not sure which answer can work, but as access to the database's underlying operating system is required, then B & D are definitely incorrect.<br>Then, we have A & C, C would be correct if the cross-region replication is supported, but unfortunately, according to the above URL, it is not supported.<br>So, we have to go with A I think..",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 784467,
          "date": "Sun 22 Jan 2023 17:03",
          "username": "\t\t\t\tbullrem\t\t\t",
          "content": "D.  <br>Migrating the Oracle database to Amazon RDS for Oracle and creating a standby database in another availability zone will meet the requirement of upgrading the database to the most recent available version and setting up disaster recovery (DR) while minimizing operational overhead. RDS will handle the backups, software patching, and version upgrades for the databases. Additionally, creating a standby database in another availability zone will provide a highly available architecture with minimal operational overhead for normal operations and disaster recovery setup.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Migrating the Oracle database to an Amazon EC2 instance and setting up database replication to a different AWS Region would not provide the same level of availability and ease of management as RDS.<br>Migrating the Oracle database to Amazon RDS for Oracle and activating Cross-Region automated backups to replicate the snapshots to another AWS Region would not provide the same level of availability and ease of management as having a standby database in another availability zone.<br>Migrating the Oracle database to Amazon RDS Custom for Oracle and creating a read replica for the database in another AWS Region would not provide the same level of availability and ease of management as having a standby database in another availability zone.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 784468,
          "date": "Sun 22 Jan 2023 17:03",
          "username": "\t\t\t\tbullrem\t\t\t",
          "content": "Migrating the Oracle database to an Amazon EC2 instance and setting up database replication to a different AWS Region would not provide the same level of availability and ease of management as RDS.<br>Migrating the Oracle database to Amazon RDS for Oracle and activating Cross-Region automated backups to replicate the snapshots to another AWS Region would not provide the same level of availability and ease of management as having a standby database in another availability zone.<br>Migrating the Oracle database to Amazon RDS Custom for Oracle and creating a read replica for the database in another AWS Region would not provide the same level of availability and ease of management as having a standby database in another availability zone.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 759477,
          "date": "Wed 28 Dec 2022 08:14",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The correct answer is Option C.  To meet the requirements, the company should migrate the Oracle database to Amazon RDS Custom for Oracle and create a read replica for the database in another AWS Region.<br><br>Amazon RDS Custom for Oracle is a fully managed service that allows users to run Oracle databases on Amazon RDS. By using RDS Custom for Oracle, the company can minimize the operational overhead for normal operations and DR setup and maintain access to the database's underlying operating system.<br><br>Creating a read replica of the database in another AWS Region will provide the company with a disaster recovery solution that allows the company to failover to the replica if the primary database becomes unavailable. The read replica can also be used to offload read workloads from the primary database, which can improve the performance of the database.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 758573,
          "date": "Tue 27 Dec 2022 14:46",
          "username": "\t\t\t\tanonymouscloudguy\t\t\t",
          "content": "https://aws.amazon.com/about-aws/whats-new/2021/10/amazon-rds-custom-oracle/<br><br>\\\"Access to underlying OS and DB environment\\\"",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 752505,
          "date": "Wed 21 Dec 2022 16:32",
          "username": "\t\t\t\tSilvestr\t\t\t",
          "content": "Right answer is C : RDS Custom -for access to and customize the underlying instance (Oracle & SQL Server)",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 748637,
          "date": "Sun 18 Dec 2022 07:37",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option C- RDS Custom as the usecase needs access to underlying OS platform.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 746407,
          "date": "Thu 15 Dec 2022 19:11",
          "username": "\t\t\t\tmarkvonwaffen\t\t\t",
          "content": "Amazon Relational Database Service (Amazon RDS) Custom is a managed database service for legacy, custom, and packaged applications that require access to the underlying OS and DB environment. T<br>https://aws.amazon.com/about-aws/whats-new/2021/10/amazon-rds-custom-oracle/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 742488,
          "date": "Mon 12 Dec 2022 08:00",
          "username": "\t\t\t\tAlaN652\t\t\t",
          "content": "C is the answer since read replica can be promoted to a stand alone DB instance. see: <br>https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html#USER_ReadRepl.Promote<br>A: Can't be correct because of the operational overhead<br>B: Can't be correct because cross-region automated backups is not feasible,<br>only it is possible in same region, see the table in first page: https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/<br>D: can't be the correct answer because creating a standby DB in a different availability zone <br>is not considered as a DR. It should be in different region.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 737009,
          "date": "Tue 06 Dec 2022 18:00",
          "username": "\t\t\t\tAnto1973\t\t\t",
          "content": "Deffo C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        }
      ]
    },
    {
      "question_id": "#134",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company wants to move its application to a serverless solution. The serverless solution needs to analyze existing and new data by using SL. The company stores the data in an Amazon S3 bucket. The data requires encryption and must be replicated to a different AWS Region.<br>Which solution will meet these requirements with the LEAST operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#134",
          "answers": [
            {
              "choice": "<p>A. Create a new S3 bucket. Load the data into the new S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate encrypted objects to an S3 bucket in another Region. Use server-side encryption with AWS KMS multi-Region kays (SSE-KMS). Use Amazon Athena to query the data.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a new S3 bucket. Load the data into the new S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate encrypted objects to an S3 bucket in another Region. Use server-side encryption with AWS KMS multi-Region keys (SSE-KMS). Use Amazon RDS to query the data.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Load the data into the existing S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate encrypted objects to an S3 bucket in another Region. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Use Amazon Athena to query the data.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Load the data into the existing S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate encrypted objects to an S3 bucket in another Region. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Use Amazon RDS to query the data.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 699631,
          "date": "Thu 20 Oct 2022 09:18",
          "username": "\t\t\t\t123jhl0\t\t\t",
          "content": "SSE-KMS vs SSE-S3 - The last seems to have less overhead (as the keys are automatically generated by S3 and applied on data at upload, and don't require further actions. KMS provides more flexibility, but in turn involves a different service, which finally is more \\\"complex\\\" than just managing one (S3). So A and B are excluded. If you are in doubt, you are having 2 buckets in A and B, while just keeping one in C and D. <br>https://s3browser.com/server-side-encryption-types.aspx <br>Decide between C and D is deciding on Athena or RDS. RDS is a relational db, and we have documents on S3, which is the use case for Athena. Athena is also serverless, which eliminates the need of controlling the underlying infrastructure and capacity. So C is the answer.<br>https://aws.amazon.com/athena/",
          "upvote_count": "34",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 707618,
          "date": "Sun 30 Oct 2022 06:20",
          "username": "\t\t\t\tdokaedu\t\t\t",
          "content": "Answer is A: <br>Amazon S3 Bucket Keys reduce the cost of Amazon S3 server-side encryption using AWS Key Management Service (SSE-KMS). This new bucket-level key for SSE can reduce AWS KMS request costs by up to 99 percent by decreasing the request traffic from Amazon S3 to AWS KMS. With a few clicks in the AWS Management Console, and without any changes to your client applications, you can configure your bucket to use an S3 Bucket Key for AWS KMS-based encryption on new objects.<br>The Existing S3 bucket might have uncrypted data - encryption will apply new data received after the applying of encryption on the new bucket.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I didn't read anywhere in the question where cost was an issue of consideration, so how you made it a main issue here is beyond me.</li><li>Cost reduction is in comparison bet Bucket level KMS key and object level KMS key. Not between SSE-KMS and SSE-S3. Hence its a wrong comparison</li><li>Reducing cost was never the target, it's LEAST operational. In that regard SSE-S3 AWS fully managed.</li></ul>",
          "upvote_count": "10",
          "selected_answers": ""
        },
        {
          "id": 777087,
          "date": "Sun 15 Jan 2023 22:35",
          "username": "\t\t\t\tLuckyAro\t\t\t",
          "content": "I didn't read anywhere in the question where cost was an issue of consideration, so how you made it a main issue here is beyond me.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 739828,
          "date": "Fri 09 Dec 2022 06:59",
          "username": "\t\t\t\tRBSK\t\t\t",
          "content": "Cost reduction is in comparison bet Bucket level KMS key and object level KMS key. Not between SSE-KMS and SSE-S3. Hence its a wrong comparison",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 713354,
          "date": "Tue 08 Nov 2022 00:45",
          "username": "\t\t\t\tRODROSKAR\t\t\t",
          "content": "Reducing cost was never the target, it's LEAST operational. In that regard SSE-S3 AWS fully managed.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 847534,
          "date": "Wed 22 Mar 2023 22:18",
          "username": "\t\t\t\tErbug\t\t\t",
          "content": "why do we need SSE-S3 and not SSE-KMS for this solution? What are the differences between them?",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 844869,
          "date": "Mon 20 Mar 2023 13:43",
          "username": "\t\t\t\tasoli\t\t\t",
          "content": "it says existing objects and new objects. When you enable cross-region replication on an s3 bucket, it only replicates the new objects and you have to take care of the existing objects to copy them to the new bucket. which has more operational overhead.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 840479,
          "date": "Thu 16 Mar 2023 02:43",
          "username": "\t\t\t\tChandraPrabu\t\t\t",
          "content": "Existing objects in the source bucket will not be replicated to the destination bucket unless you manually copy them to the destination bucket or use another method such as Amazon S3 inventory and Amazon S3 batch operations.<br><br>In that case option A makes sense to copy the exiting data to new bucket & make them replicated in destination bucket.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 835433,
          "date": "Fri 10 Mar 2023 22:04",
          "username": "\t\t\t\trdss11\t\t\t",
          "content": "S3 buckets are ecrypted by SSE-S3 by default",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 834694,
          "date": "Fri 10 Mar 2023 07:51",
          "username": "\t\t\t\ttaehyeki\t\t\t",
          "content": "i would go with A<br>i dont understand what loading data into existing s3 means",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 828768,
          "date": "Sat 04 Mar 2023 10:06",
          "username": "\t\t\t\tathiha\t\t\t",
          "content": "The only reason why I choose option A is that the question states \\\"Serverless Solutions needs to analyze existing and new data\\\". And when you turn on the Cross-Region Replication (CRR), the existing data will not be replicated automatically. It only replicates the new data added to the source bucket from the point you turn on CRR. So it would make more sense to have a new bucket to load the data and then turn on the CRR.",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 825397,
          "date": "Wed 01 Mar 2023 01:05",
          "username": "\t\t\t\tKZM\t\t\t",
          "content": "Multi-Regions Key in AWS KMS<br>https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>A,<br>Sorry, I wrongly clicked on C.  I mean option A, AWS KMS multi-Region kays.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 825453,
          "date": "Wed 01 Mar 2023 02:23",
          "username": "\t\t\t\tKZM\t\t\t",
          "content": "A,<br>Sorry, I wrongly clicked on C.  I mean option A, AWS KMS multi-Region kays.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 822503,
          "date": "Sun 26 Feb 2023 15:15",
          "username": "\t\t\t\tJa13\t\t\t",
          "content": "It says that they already have a S3 bucket, option A indicates to create a new one. Why would they create a new bucket when they already have one? option c is better<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Cross region replication is not retro-active.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 838203,
          "date": "Mon 13 Mar 2023 20:11",
          "username": "\t\t\t\tCapJackSparrow\t\t\t",
          "content": "Cross region replication is not retro-active.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 819021,
          "date": "Thu 23 Feb 2023 10:30",
          "username": "\t\t\t\tLuckyAro\t\t\t",
          "content": "A is the best solution that meets the company's requirements with the least operational overhead.<br><br>A recommends creating a new S3 bucket, loading the data into the new S3 bucket, using S3 Cross-Region Replication (CRR) to replicate encrypted objects to an S3 bucket in another region, using server-side encryption with AWS KMS multi-Region keys (SSE-KMS), and using Amazon Athena to query the data.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 818664,
          "date": "Thu 23 Feb 2023 03:03",
          "username": "\t\t\t\tbdp123\t\t\t",
          "content": "It doesn't state whether the Existing S3 bucket might have uncrypted data - encryption will apply new data received after the applying of encryption on the new bucket.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 799867,
          "date": "Mon 06 Feb 2023 15:55",
          "username": "\t\t\t\tjoric\t\t\t",
          "content": "I vote C , key word existing data",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 799864,
          "date": "Mon 06 Feb 2023 15:52",
          "username": "\t\t\t\tjoric\t\t\t",
          "content": "The serverless solution needs to analyze existing and new data by using SL.<br>(SQL) there is a misstype.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 798442,
          "date": "Sat 04 Feb 2023 23:54",
          "username": "\t\t\t\tKZM\t\t\t",
          "content": "Amazon S3 now applies server-side encryption with Amazon S3 managed keys (SSE-S3) as the base level of encryption for every bucket in Amazon S3. Starting January 5, 2023, all new object uploads to Amazon S3 will be automatically encrypted at no additional cost and with no impact on performance.<br><br>Server-side encryption with AWS KMS keys (SSE-KMS) is similar to SSE-S3, but with some additional benefits and charges for using this service.<br><br>If we think about the LEAST operational overhead, SSE-S3 is more reasonable, I think.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 796559,
          "date": "Fri 03 Feb 2023 01:29",
          "username": "\t\t\t\tjoric\t\t\t",
          "content": "I mean I askesd whats SL? is it a typing mistake? admins unlock my comment please.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Its a TYPO for SQL</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 808067,
          "date": "Tue 14 Feb 2023 05:11",
          "username": "\t\t\t\tJoxtat\t\t\t",
          "content": "Its a TYPO for SQL",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 796558,
          "date": "Fri 03 Feb 2023 01:28",
          "username": "\t\t\t\tjoric\t\t\t",
          "content": "I asked what is ML. admins unlock my comment please.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Machine Language</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 803842,
          "date": "Fri 10 Feb 2023 01:50",
          "username": "\t\t\t\tLemmij\t\t\t",
          "content": "Machine Language",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#135",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company runs workloads on AWS. The company needs to connect to a service from an external provider. The service is hosted in the provider's VPC.  According to the company's security team, the connectivity must be private and must be restricted to the target service. The connection must be initiated only from the company's VPC. <br>Which solution will mast these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#135",
          "answers": [
            {
              "choice": "<p>A. Create a VPC peering connection between the company's VPC and the provider's VPC.  Update the route table to connect to the target service.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Ask the provider to create a virtual private gateway in its VPC.  Use AWS PrivateLink to connect to the target service.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a NAT gateway in a public subnet of the company's VPUpdate the route table to connect to the target service.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Ask the provider to create a VPC endpoint for the target service. Use AWS PrivateLink to connect to the target service.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 699637,
          "date": "Thu 20 Oct 2022 09:26",
          "username": "\t\t\t\t123jhl0\t\t\t",
          "content": "**AWS PrivateLink provides private connectivity between VPCs, AWS services, and your on-premises networks, without exposing your traffic to the public internet**. AWS PrivateLink makes it easy to connect services across different accounts and VPCs to significantly simplify your network architecture.<br>Interface **VPC endpoints**, powered by AWS PrivateLink, connect you to services hosted by AWS Partners and supported solutions available in AWS Marketplace. <br>https://aws.amazon.com/privatelink/",
          "upvote_count": "18",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 814436,
          "date": "Sun 19 Feb 2023 20:32",
          "username": "\t\t\t\tHelp2023\t\t\t",
          "content": "D.  Here you are the one initiating the connection",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 794837,
          "date": "Wed 01 Feb 2023 04:12",
          "username": "\t\t\t\tdevonwho\t\t\t",
          "content": "PrivateLink is a more generalized technology for linking VPCs to other services. This can include multiple potential endpoints: AWS services, such as Lambda or EC2; Services hosted in other VPCs; Application endpoints hosted on-premises.<br><br>https://www.tinystacks.com/blog-post/aws-vpc-peering-vs-privatelink-which-to-use-and-when/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 794694,
          "date": "Tue 31 Jan 2023 23:51",
          "username": "\t\t\t\tdevonwho\t\t\t",
          "content": "While VPC peering enables you to privately connect VPCs, AWS PrivateLink enables you to configure applications or services in VPCs as endpoints that your VPC peering connections can connect to.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 779285,
          "date": "Tue 17 Jan 2023 20:46",
          "username": "\t\t\t\tremand\t\t\t",
          "content": "The solution that meets these requirements best is option D. <br><br>By asking the provider to create a VPC endpoint for the target service, the company can use AWS PrivateLink to connect to the target service. This enables the company to access the service privately and securely over an Amazon VPC endpoint, without requiring a NAT gateway, VPN, or AWS Direct Connect. Additionally, this will restrict the connectivity only to the target service, as required by the company's security team.<br><br>Option A VPC peering connection may not meet security requirement as it can allow communication between all resources in both VPCs.<br>Option B, asking the provider to create a virtual private gateway in its VPC and use AWS PrivateLink to connect to the target service is not the optimal solution because it may require the provider to make changes and also you may face security issues.<br>Option C, creating a NAT gateway in a public subnet of the company's VPC can expose the target service to the internet, which would not meet the security requirements.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 759539,
          "date": "Wed 28 Dec 2022 08:59",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The solution that meets these requirements is Option D:<br><br>* Ask the provider to create a VPC endpoint for the target service.<br>* Use AWS PrivateLink to connect to the target service.<br><br>Option D involves asking the provider to create a VPC endpoint for the target service, which is a private connection to the service that is hosted in the provider's VPC.  This ensures that the connection is private and restricted to the target service, as required by the company's security team. The company can then use AWS PrivateLink to connect to the target service over the VPC endpoint. AWS PrivateLink is a fully managed service that enables you to privately access services hosted on AWS, on-premises, or in other VPCs. It provides secure and private connectivity to services by using private IP addresses, which ensures that traffic stays within the Amazon network and does not traverse the public internet. <br><br>Therefore, Option D is the solution that meets the requirements.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>AWS PrivateLink documentation: https://docs.aws.amazon.com/privatelink/latest/userguide/what-is-privatelink.html</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 759542,
          "date": "Wed 28 Dec 2022 09:00",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "AWS PrivateLink documentation: https://docs.aws.amazon.com/privatelink/latest/userguide/what-is-privatelink.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 757795,
          "date": "Mon 26 Dec 2022 20:52",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "D is right,if requirement was to be ok with public internet then option C was ok.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 755480,
          "date": "Sun 25 Dec 2022 08:23",
          "username": "\t\t\t\tk1kavi1\t\t\t",
          "content": "D (VPC endpoint) looks correct. Below are the differences between VPC Peering & VPC endpoints.<br><br>https://support.huaweicloud.com/intl/en-us/vpcep_faq/vpcep_04_0004.html#:~:text=You%20can%20create%20a%20VPC%20endpoint%20to%20connect%20your%20local,connection%20over%20an%20internal%20network.&text=VPC%20Peering%20supports%20only%20communications%20between%20two%20VPCs%20in%20the%20same%20region.&text=You%20can%20use%20Cloud%20Connect,between%20VPCs%20in%20different%20regions.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 748646,
          "date": "Sun 18 Dec 2022 07:59",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "D is the right answer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 741940,
          "date": "Sun 11 Dec 2022 18:43",
          "username": "\t\t\t\tSahilbhai\t\t\t",
          "content": "answer is D",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#136",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is migrating its on-premises PostgreSQL database to Amazon Aurora PostgreSQL. The on-premises database must remain online and accessible during the migration. The Aurora database must remain synchronized with the on-premises database.<br>Which combination of actions must a solutions architect take to meet these requirements? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: AC</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#136",
          "answers": [
            {
              "choice": "<p>A. Create an ongoing replication task.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a database backup of the on-premises database.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an AWS Database Migration Service (AWS DMS) replication server.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Convert the database schema by using the AWS Schema Conversion Tool (AWS SCT).<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Create an Amazon EventBridge (Amazon CloudWatch Events) rule to monitor the database synchronization.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 699673,
          "date": "Thu 20 Oct 2022 10:03",
          "username": "\t\t\t\t123jhl0\t\t\t",
          "content": "AWS Database Migration Service (AWS DMS) helps you migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database.<br>... With AWS Database Migration Service, you can also continuously replicate data with low latency from any supported source to any supported target.<br>https://aws.amazon.com/dms/",
          "upvote_count": "15",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 850573,
          "date": "Sun 26 Mar 2023 01:17",
          "username": "\t\t\t\tosmk\t\t\t",
          "content": "A->https://docs.aws.amazon.com/dms/latest/sbs/chap-manageddatabases.oracle2rds.replication.html<br>C->https://docs.aws.amazon.com/dms/latest/userguide/Welcome.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 848021,
          "date": "Thu 23 Mar 2023 10:38",
          "username": "\t\t\t\tErbug\t\t\t",
          "content": "This question is giving us two conditions to solve it. One of them is on-premise database must remain online and accessible during the migration and the second one is Aurora database must remain synchronized with the on-premises database. So to meet them all A and C will be the correct options for us.<br><br>PS: if the question was just asking us something related to the DB migration process alone, all options would be correct.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 792050,
          "date": "Sun 29 Jan 2023 21:45",
          "username": "\t\t\t\tG3\t\t\t",
          "content": "https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-an-on-premises-postgresql-database-to-aurora-postgresql.html<br><br>This link talks about using DMS .I saw the other link pointing to SCT - not sure which one is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 764812,
          "date": "Tue 03 Jan 2023 16:29",
          "username": "\t\t\t\taba2s\t\t\t",
          "content": "DMS for database migration<br>SCT for having the same scheme<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>The source and destination are both MySQL so schema is not needed.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: CD"
        },
        {
          "id": 810479,
          "date": "Thu 16 Feb 2023 09:58",
          "username": "\t\t\t\tHelp2023\t\t\t",
          "content": "The source and destination are both MySQL so schema is not needed.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 764428,
          "date": "Tue 03 Jan 2023 11:30",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "AWS Database Migration Service (AWS DMS)",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 763816,
          "date": "Mon 02 Jan 2023 16:43",
          "username": "\t\t\t\tgustavtd\t\t\t",
          "content": "AC, here it is clearly shownhttps://docs.aws.amazon.com/zh_cn/dms/latest/sbs/chap-manageddatabases.postgresql-rds-postgresql.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>You nailed it !</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 777138,
          "date": "Sun 15 Jan 2023 23:40",
          "username": "\t\t\t\tLuckyAro\t\t\t",
          "content": "You nailed it !",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 760363,
          "date": "Wed 28 Dec 2022 22:46",
          "username": "\t\t\t\tbamishr\t\t\t",
          "content": "A.  Create an ongoing replication task: An ongoing replication task can be used to continuously replicate data from the on-premises database to the Aurora database. This will ensure that the Aurora database remains in sync with the on-premises database.<br>D.  Convert the database schema by using the AWS Schema Conversion Tool (AWS SCT): The AWS SCT can be used to convert the schema of the on-premises database to a format that is compatible with Aurora. This will ensure that the data can be properly migrated and that the Aurora database can be used with the same applications and queries as the on-premises database.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>The source and destination are both MySQL so schema is not needed.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 810481,
          "date": "Thu 16 Feb 2023 09:58",
          "username": "\t\t\t\tHelp2023\t\t\t",
          "content": "The source and destination are both MySQL so schema is not needed.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 760234,
          "date": "Wed 28 Dec 2022 20:08",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "To meet the requirements of maintaining an online and accessible on-premises database while migrating to Amazon Aurora PostgreSQL and keeping the databases synchronized, a solutions architect should take the following actions:<br><br>Option A.  Create an ongoing replication task. This will allow the architect to continuously replicate data from the on-premises database to the Aurora database.<br><br>Option C.  Create an AWS Database Migration Service (AWS DMS) replication server. This will allow the architect to use AWS DMS to migrate data from the on-premises database to the Aurora database. AWS DMS can also be used to continuously replicate data between the two databases to keep them synchronized.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 757800,
          "date": "Mon 26 Dec 2022 21:00",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "C&D ,SCT is required,its a mandate not an option.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: CD"
        },
        {
          "id": 755353,
          "date": "Sun 25 Dec 2022 01:53",
          "username": "\t\t\t\tberks\t\t\t",
          "content": "Answer is CD.  Postgresql to Aurora Postgresql needed SCT.<br>https://aws.amazon.com/ko/dms/schema-conversion-tool/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: CD"
        },
        {
          "id": 755351,
          "date": "Sun 25 Dec 2022 01:52",
          "username": "\t\t\t\tberks\t\t\t",
          "content": "Answer is CD.  Postgresql to Aurora Postgresql needed SCT.<br>https://aws.amazon.com/ko/dms/schema-conversion-tool/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 752528,
          "date": "Wed 21 Dec 2022 16:50",
          "username": "\t\t\t\tSilvestr\t\t\t",
          "content": "You do not need to use SCT if you are migrating the same DB engine<br> Ex: On-Premise PostgreSQL => RDS PostgreSQL<br> The DB engine is still PostgreSQL (RDS is the platform)",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 748649,
          "date": "Sun 18 Dec 2022 08:04",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "A and C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 745824,
          "date": "Thu 15 Dec 2022 08:42",
          "username": "\t\t\t\tandreiushu\t\t\t",
          "content": "A & C<br>SCT is not needed here.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>You're going from Postgres to Postgres. What schema are you converting??</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 787177,
          "date": "Wed 25 Jan 2023 02:54",
          "username": "\t\t\t\tjwu413\t\t\t",
          "content": "You're going from Postgres to Postgres. What schema are you converting??",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 744740,
          "date": "Wed 14 Dec 2022 06:19",
          "username": "\t\t\t\twly_al\t\t\t",
          "content": "both source and target is PostgreSQL so SCT is not needed.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 744588,
          "date": "Wed 14 Dec 2022 01:28",
          "username": "\t\t\t\t333666999\t\t\t",
          "content": "i voted CD",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: CD"
        }
      ]
    },
    {
      "question_id": "#137",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company uses AWS Organizations to create dedicated AWS accounts for each business unit to manage each business unit's account independently upon request. The root email recipient missed a notification that was sent to the root user email address of one account. The company wants to ensure that all future notifications are not missed. Future notifications must be limited to account administrators.<br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#137",
          "answers": [
            {
              "choice": "<p>A. Configure the company's email server to forward notification email messages that are sent to the AWS account root user email address to all users in the organization.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Configure all AWS account root user email addresses as distribution lists that go to a few administrators who can respond to alerts. Configure AWS account alternate contacts in the AWS Organizations console or programmatically.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Configure all AWS account root user email messages to be sent to one administrator who is responsible for monitoring alerts and forwarding those alerts to the appropriate groups.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure all existing AWS accounts and all newly created accounts to use the same root user email address. Configure AWS account alternate contacts in the AWS Organizations console or programmatically.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 699683,
          "date": "Thu 20 Oct 2022 10:12",
          "username": "\t\t\t\t123jhl0\t\t\t",
          "content": "Use a group email address for the management account's root user<br>https://docs.aws.amazon.com/organizations/latest/userguide/orgs_best-practices_mgmt-acct.html#best-practices_mgmt-acct_email-address",
          "upvote_count": "19",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 848254,
          "date": "Thu 23 Mar 2023 14:29",
          "username": "\t\t\t\tjaswantn\t\t\t",
          "content": "Using the method of crossing out the option that does not fit....<br>Option A: address to all users of organization (wrong)<br>Option B: go to a few administration who can respond to alerts (question says to send notification to administrators not a selected few )<br>Option C: send to one administrator and giving him responsibility (wrong)<br>Option D: correct (as this is the one option left after checking all others).",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 763118,
          "date": "Sun 01 Jan 2023 10:04",
          "username": "\t\t\t\tZerotn3\t\t\t",
          "content": "Option B does not meet the requirements because it would require configuring all AWS account root user email addresses as distribution lists, which is not necessary to meet the requirements.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 761179,
          "date": "Thu 29 Dec 2022 16:14",
          "username": "\t\t\t\tmp165\t\t\t",
          "content": "Unless I am reading this wrong from AWS, it seems D is proper as it says to use a single account and then set to forward to other emails.<br>Use an email address that forwards received messages directly to a list of senior business managers. In the event that AWS needs to contact the owner of the account, for example, to confirm access, the email is distributed to multiple parties. This approach helps to reduce the risk of delays in responding, even if individuals are on vacation, out sick, or leave the business.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 760285,
          "date": "Wed 28 Dec 2022 21:02",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "To meet the requirements of ensuring that all future notifications are not missed and are limited to account administrators, the company should take the following action:<br><br>Option D.  Configure all existing AWS accounts and all newly created accounts to use the same root user email address. Configure AWS account alternate contacts in the AWS Organizations console or programmatically.<br><br>By configuring all AWS accounts to use the same root user email address and setting up AWS account alternate contacts, the company can ensure that all notifications are sent to a single email address that is monitored by one or more administrators. This will allow the company to ensure that all notifications are received and responded to promptly, without the risk of notifications being missed.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option D would not meet the requirement of limiting the notifications to account administrators. Instead, it is better to use option B, which is to configure all AWS account root user email addresses as distribution lists that go to a few administrators who can respond to alerts. This way, the company can ensure that the notifications are received by the appropriate people and that they are not missed. Additionally, AWS account alternate contacts can be configured in the AWS Organizations console or programmatically, which allows the company to have more granular control over who receives the notifications.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 784473,
          "date": "Sun 22 Jan 2023 17:18",
          "username": "\t\t\t\tbullrem\t\t\t",
          "content": "Option D would not meet the requirement of limiting the notifications to account administrators. Instead, it is better to use option B, which is to configure all AWS account root user email addresses as distribution lists that go to a few administrators who can respond to alerts. This way, the company can ensure that the notifications are received by the appropriate people and that they are not missed. Additionally, AWS account alternate contacts can be configured in the AWS Organizations console or programmatically, which allows the company to have more granular control over who receives the notifications.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 757803,
          "date": "Mon 26 Dec 2022 21:04",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "B makes more sense",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 748598,
          "date": "Sun 18 Dec 2022 05:13",
          "username": "\t\t\t\tSahilbhai\t\t\t",
          "content": "answer b is makes more sense",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 712922,
          "date": "Mon 07 Nov 2022 09:23",
          "username": "\t\t\t\tPS_R\t\t\t",
          "content": "B makes more sense and is a best practise",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 700227,
          "date": "Thu 20 Oct 2022 21:13",
          "username": "\t\t\t\tChunsli\t\t\t",
          "content": "B makes better sense in the context",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        }
      ]
    },
    {
      "question_id": "#138",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company runs its ecommerce application on AWS. Every new order is published as a massage in a RabbitMQ queue that runs on an Amazon EC2 instance in a single Availability Zone. These messages are processed by a different application that runs on a separate EC2 instance. This application stores the details in a PostgreSQL database on another EC2 instance. All the EC2 instances are in the same Availability Zone.<br>The company needs to redesign its architecture to provide the highest availability with the least operational overhead.<br>What should a solutions architect do to meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#138",
          "answers": [
            {
              "choice": "<p>A. Migrate the queue to a redundant pair (active/standby) of RabbitMQ instances on Amazon MQ. Create a Multi-AZ Auto Scaling group for EC2 instances that host the application. Create another Multi-AZ Auto Scaling group for EC2 instances that host the PostgreSQL database.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Migrate the queue to a redundant pair (active/standby) of RabbitMQ instances on Amazon MQ. Create a Multi-AZ Auto Scaling group for EC2 instances that host the application. Migrate the database to run on a Multi-AZ deployment of Amazon RDS for PostgreSQL.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a Multi-AZ Auto Scaling group for EC2 instances that host the RabbitMQ queue. Create another Multi-AZ Auto Scaling group for EC2 instances that host the application. Migrate the database to run on a Multi-AZ deployment of Amazon RDS for PostgreSQL.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a Multi-AZ Auto Scaling group for EC2 instances that host the RabbitMQ queue. Create another Multi-AZ Auto Scaling group for EC2 instances that host the application. Create a third Multi-AZ Auto Scaling group for EC2 instances that host the PostgreSQL database<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 699693,
          "date": "Thu 20 Oct 2022 10:27",
          "username": "\t\t\t\t123jhl0\t\t\t",
          "content": "Migrating to Amazon MQ reduces the overhead on the queue management. C and D are dismissed.<br>Deciding between A and B means deciding to go for an AutoScaling group for EC2 or an RDS for Postgress (both multi- AZ). The RDS option has less operational impact, as provide as a service the tools and software required. Consider for instance, the effort to add an additional node like a read replica, to the DB. <br>https://docs.aws.amazon.com/amazon-mq/latest/developer-guide/active-standby-broker-deployment.html<br>https://aws.amazon.com/rds/postgresql/<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>This also helps anyone in doubt; https://docs.aws.amazon.com/amazon-mq/latest/developer-guide/active-standby-broker-deployment.html</li><li>Yes but active/standby is fault tolerance, not HA. I would concede after thinking about it that B is probably the answer that will be marked correct but its not a great question.</li></ul>",
          "upvote_count": "15",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 732395,
          "date": "Thu 01 Dec 2022 09:22",
          "username": "\t\t\t\tEKA_CloudGod\t\t\t",
          "content": "This also helps anyone in doubt; https://docs.aws.amazon.com/amazon-mq/latest/developer-guide/active-standby-broker-deployment.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 705714,
          "date": "Thu 27 Oct 2022 17:58",
          "username": "\t\t\t\tUWSFish\t\t\t",
          "content": "Yes but active/standby is fault tolerance, not HA. I would concede after thinking about it that B is probably the answer that will be marked correct but its not a great question.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 826070,
          "date": "Wed 01 Mar 2023 17:33",
          "username": "\t\t\t\tGary_Phillips_2007\t\t\t",
          "content": "B for me.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 760497,
          "date": "Thu 29 Dec 2022 02:55",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "To meet the requirements of providing the highest availability with the least operational overhead, the solutions architect should take the following actions:<br><br>* By migrating the queue to Amazon MQ, the architect can take advantage of the built-in high availability and failover capabilities of the service, which will help ensure that messages are delivered reliably and without interruption.<br><br>* By creating a Multi-AZ Auto Scaling group for the EC2 instances that host the application, the architect can ensure that the application is highly available and able to handle increased traffic without the need for manual intervention.<br><br>* By migrating the database to a Multi-AZ deployment of Amazon RDS for PostgreSQL, the architect can take advantage of the built-in high availability and failover capabilities of the service, which will help ensure that the database is always available and able to handle increased traffic.<br><br>Therefore, the correct answer is Option B. ",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 757806,
          "date": "Mon 26 Dec 2022 21:09",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "B is right all explanations below are correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 748652,
          "date": "Sun 18 Dec 2022 08:14",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option B is right answer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 724316,
          "date": "Tue 22 Nov 2022 13:27",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "B for me",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#139",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A reporting team receives files each day in an Amazon S3 bucket. The reporting team manually reviews and copies the files from this initial S3 bucket to an analysis S3 bucket each day at the same time to use with Amazon QuickSight. Additional teams are starting to send more files in larger sizes to the initial S3 bucket.<br>The reporting team wants to move the files automatically analysis S3 bucket as the files enter the initial S3 bucket. The reporting team also wants to use AWS Lambda functions to run pattern-matching code on the copied data. In addition, the reporting team wants to send the data files to a pipeline in Amazon SageMaker Pipelines.<br>What should a solutions architect do to meet these requirements with the LEAST operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#139",
          "answers": [
            {
              "choice": "<p>A. Create a Lambda function to copy the files to the analysis S3 bucket. Create an S3 event notification for the analysis S3 bucket. Configure Lambda and SageMaker Pipelines as destinations of the event notification. Configure s3:ObjectCreated:Put as the event type.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a Lambda function to copy the files to the analysis S3 bucket. Configure the analysis S3 bucket to send event notifications to Amazon EventBridge (Amazon CloudWatch Events). Configure an ObjectCreated rule in EventBridge (CloudWatch Events). Configure Lambda and SageMaker Pipelines as targets for the rule.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Configure S3 replication between the S3 buckets. Create an S3 event notification for the analysis S3 bucket. Configure Lambda and SageMaker Pipelines as destinations of the event notification. Configure s3:ObjectCreated:Put as the event type.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure S3 replication between the S3 buckets. Configure the analysis S3 bucket to send event notifications to Amazon EventBridge (Amazon CloudWatch Events). Configure an ObjectCreated rule in EventBridge (CloudWatch Events). Configure Lambda and SageMaker Pipelines as targets for the rule.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 704868,
          "date": "Wed 26 Oct 2022 18:49",
          "username": "\t\t\t\tSix_Fingered_Jose\t\t\t",
          "content": "i go for D here<br>A and B says you are copying the file to another bucket using lambda,<br>C an D just uses S3 replication to copy the files,<br><br>They are doing exactly the same thing while C and D do not require setting up of lambda, which should be more efficient<br><br>The question says the team is manually copying the files, automatically replicating the files should be the most efficient method vs manually copying or copying with lambda.",
          "upvote_count": "15",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 699712,
          "date": "Thu 20 Oct 2022 10:55",
          "username": "\t\t\t\t123jhl0\t\t\t",
          "content": "C and D aren't answers as replicating the S3 bucket isn't efficient, as other teams are starting to use it to store larger docs not related to the reporting, making replication not useful.<br>As Amazon SageMaker Pipelines, ..., is now supported as a target for routing events in Amazon EventBridge, means the answer is B<br>https://aws.amazon.com/about-aws/whats-new/2021/04/new-options-trigger-amazon-sagemaker-pipeline-executions/<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I think you are mis-interpreting the question. I think you need to use all files, including the ones provided by other teams, otherwise how can you tell what files to copy? I think the point of this statement is to show that more files are in use, and being copied at different times, rather than suggesting you need to differentiate between the two sources of files.</li><li>Nowhere in the question did they mention that other files were unrelated to reporting ....<br>\\\"The reporting team wants to move the files automatically to analysis S3 bucket as the files enter the initial S3 bucket\\\" where did it say they were unrelated files ? except for conjecture.</li><li>Not sure how far lambda will cope up with larger files with the timelimit in place.</li></ul>",
          "upvote_count": "13",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 753278,
          "date": "Thu 22 Dec 2022 13:39",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "I think you are mis-interpreting the question. I think you need to use all files, including the ones provided by other teams, otherwise how can you tell what files to copy? I think the point of this statement is to show that more files are in use, and being copied at different times, rather than suggesting you need to differentiate between the two sources of files.",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 777191,
          "date": "Mon 16 Jan 2023 01:01",
          "username": "\t\t\t\tLuckyAro\t\t\t",
          "content": "Nowhere in the question did they mention that other files were unrelated to reporting ....<br>\\\"The reporting team wants to move the files automatically to analysis S3 bucket as the files enter the initial S3 bucket\\\" where did it say they were unrelated files ? except for conjecture.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 715773,
          "date": "Fri 11 Nov 2022 06:04",
          "username": "\t\t\t\tKADSM\t\t\t",
          "content": "Not sure how far lambda will cope up with larger files with the timelimit in place.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 844614,
          "date": "Mon 20 Mar 2023 08:27",
          "username": "\t\t\t\tSuketuKohli\t\t\t",
          "content": "only one destination type can be specified for each event notification in S3 event notifications",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 835545,
          "date": "Sat 11 Mar 2023 02:09",
          "username": "\t\t\t\tgmehra\t\t\t",
          "content": "Answer is A<br>The statement says move the file. Replication won't move the file it will just create a copy. so Obviously C and D are out. When you Event notification and Lambda why we need EVent bridge as more service. So answer is A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 829461,
          "date": "Sat 04 Mar 2023 23:28",
          "username": "\t\t\t\tSteve_4542636\t\t\t",
          "content": "Using lambda is one of the requirements.Sns, sqs, lambda, and event bridge are the only s3 notification destinations<br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/notification-how-to-event-types-and-destinations.html.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 783819,
          "date": "Sun 22 Jan 2023 00:39",
          "username": "\t\t\t\tbullrem\t\t\t",
          "content": "both A and D options can meet the requirements with the least operational overhead as they both use automatic event-driven mechanisms (S3 event notifications and EventBridge rules) to trigger the Lambda function and copy the files to the analysis S3 bucket. The Lambda function can then run the pattern-matching code, and the files can be sent to the SageMaker pipeline. <br>Option A, directly copying the files to the analysis S3 bucket using a Lambda function, is more straight forward, option D using S3 replication and EventBridge rules is more flexible and can be more powerful as it allows you to use more complex event-driven flows.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 773912,
          "date": "Thu 12 Jan 2023 22:28",
          "username": "\t\t\t\tAHUI\t\t\t",
          "content": "Ans : D<br><br>S3 event notification can only send notifications to SQS. SNS and Lambda, BUT not Sagamaker<br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html<br><br>S3 event notification can send notification to SNS, SQS and Lambda, but not SageMaker",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 771607,
          "date": "Tue 10 Jan 2023 17:18",
          "username": "\t\t\t\tRBKumaran\t\t\t",
          "content": "A and B are ruled out as it requires an extra Lambda job to do the copy while S3 replication will take care of it with little to no overhead.<br>C is incorrect because, S3 notifcations are not supported on Sagemake pipeline (https://docs.aws.amazon.com/AmazonS3/latest/userguide/notification-how-to-event-types-and-destinations.html#supported-notification-destinations)",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 768150,
          "date": "Sat 07 Jan 2023 01:04",
          "username": "\t\t\t\tMahadeva\t\t\t",
          "content": "Since we are working already on S3 buckets, configuring S3 event notification (with evet type: s3:ObjectCreated:Put) is much easier than doing the same through EventBridge (which is an additional service in this case). Less operational overhead.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 763832,
          "date": "Mon 02 Jan 2023 17:04",
          "username": "\t\t\t\tgustavtd\t\t\t",
          "content": "https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/userguide/NotificationHowTo.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 763125,
          "date": "Sun 01 Jan 2023 10:34",
          "username": "\t\t\t\tZerotn3\t\t\t",
          "content": "I would recommend option D as it is the most efficient way to meet the requirements with the least operational overhead.<br><br>Option D involves configuring S3 replication between the two S3 buckets, which will automatically copy the files from the initial S3 bucket to the analysis S3 bucket as they are added. This eliminates the need to manually copy the files every day and will ensure that the analysis S3 bucket always has the most recent data.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>In addition, configuring the analysis S3 bucket to send event notifications to Amazon EventBridge (CloudWatch Events) and creating an ObjectCreated rule allows you to trigger Lambda functions and SageMaker Pipelines when new objects are created in the analysis S3 bucket. This allows you to perform pattern-matching and data processing on the copied data automatically as it is added to the analysis S3 bucket.<br><br>Option A and option C involve manually copying the files to the analysis S3 bucket, which is not an efficient solution given the increased volume of data that the reporting team is expecting. Option B does not involve S3 replication, so it does not address the requirement to automatically copy the data to the analysis S3 bucket.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 763126,
          "date": "Sun 01 Jan 2023 10:35",
          "username": "\t\t\t\tZerotn3\t\t\t",
          "content": "In addition, configuring the analysis S3 bucket to send event notifications to Amazon EventBridge (CloudWatch Events) and creating an ObjectCreated rule allows you to trigger Lambda functions and SageMaker Pipelines when new objects are created in the analysis S3 bucket. This allows you to perform pattern-matching and data processing on the copied data automatically as it is added to the analysis S3 bucket.<br><br>Option A and option C involve manually copying the files to the analysis S3 bucket, which is not an efficient solution given the increased volume of data that the reporting team is expecting. Option B does not involve S3 replication, so it does not address the requirement to automatically copy the data to the analysis S3 bucket.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 760585,
          "date": "Thu 29 Dec 2022 05:48",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Options A and B are incorrect because it involves creating a Lambda function to copy the files to the analysis S3 bucket, which is unnecessary. The requirement is to move the files automatically to the analysis S3 bucket as soon as they are added to the initial S3 bucket. This can be achieved more efficiently using S3 replication, which is not mentioned in Options A and B. <br><br>Option C is incorrect because it involves configuring S3 replication between the S3 buckets, which is correct. However, it does not involve configuring the analysis S3 bucket to send event notifications to Amazon EventBridge (CloudWatch Events). This is necessary to trigger the subsequent actions (i.e., running pattern-matching code using Lambda functions and sending data files to a pipeline in SageMaker Pipelines).<br><br>Therefore, the correct answer is Option D. ",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 757811,
          "date": "Mon 26 Dec 2022 21:17",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "Going with D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 744782,
          "date": "Wed 14 Dec 2022 07:27",
          "username": "\t\t\t\twly_al\t\t\t",
          "content": "lambda function for copy the data between S3 bucket was overuse and produce some cost when we can just use S3 replication",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 743372,
          "date": "Mon 12 Dec 2022 23:09",
          "username": "\t\t\t\tQjb8m9h\t\t\t",
          "content": "B.  To review is the same as to analyze, that requires Lamba, and Lamba can be configure to copy to S3 after analysis. And it's serverless hence removes overhead.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>\\\"The reporting team wants to move the files automatically analysis S3 bucket as the files enter the initial S3 bucket\\\" Based on this line i think the believe the answer is D. They aren't willing to analysis the files before copying so Lamba is not required.. <br>IT's D</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 744718,
          "date": "Wed 14 Dec 2022 05:44",
          "username": "\t\t\t\tQjb8m9h\t\t\t",
          "content": "\\\"The reporting team wants to move the files automatically analysis S3 bucket as the files enter the initial S3 bucket\\\" Based on this line i think the believe the answer is D. They aren't willing to analysis the files before copying so Lamba is not required.. <br>IT's D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 742281,
          "date": "Mon 12 Dec 2022 02:20",
          "username": "\t\t\t\ttz1\t\t\t",
          "content": "I will go with B since enabling replication also requires versioning on the bucket to be enabled which adds more operational overhead eventually and cost structure<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>It might add cost but does not add operational overhead.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 753281,
          "date": "Thu 22 Dec 2022 13:40",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "It might add cost but does not add operational overhead.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 740783,
          "date": "Sat 10 Dec 2022 09:35",
          "username": "\t\t\t\tlapaki\t\t\t",
          "content": "B.  Team is reviewing then copying to the analysis bucket. Review implies the need for lambda before copying.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>\\\"The reporting team wants to move the files automatically analysis S3 bucket as the files enter the initial S3 bucket.\\\" implies they want the files copied immediately, i.e. before being reviewed</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 753286,
          "date": "Thu 22 Dec 2022 13:42",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "\\\"The reporting team wants to move the files automatically analysis S3 bucket as the files enter the initial S3 bucket.\\\" implies they want the files copied immediately, i.e. before being reviewed",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#140",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A solutions architect needs to help a company optimize the cost of running an application on AWS. The application will use Amazon EC2 instances, AWS Fargate, and AWS Lambda for compute within the architecture.<br>The EC2 instances will run the data ingestion layer of the application. EC2 usage will be sporadic and unpredictable. Workloads that run on EC2 instances can be interrupted at any time. The application front end will run on Fargate, and Lambda will serve the API layer. The front-end utilization and API layer utilization will be predictable over the course of the next year.<br>Which combination of purchasing options will provide the MOST cost-effective solution for hosting this application? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: AC</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#140",
          "answers": [
            {
              "choice": "<p>A. Use Spot Instances for the data ingestion layer<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use On-Demand Instances for the data ingestion layer<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Purchase a 1-year Compute Savings Plan for the front end and API layer.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Purchase 1-year All Upfront Reserved instances for the data ingestion layer.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Purchase a 1-year EC2 instance Savings Plan for the front end and API layer.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 707646,
          "date": "Sun 30 Oct 2022 07:31",
          "username": "\t\t\t\tSimonPark\t\t\t",
          "content": "EC2 instance Savings Plan saves 72% while Compute Savings Plans saves 66%. But according to link, it says \\\"Compute Savings Plans provide the most flexibility and help to reduce your costs by up to 66%. These plans automatically apply to EC2 instance usage regardless of instance family, size, AZ, region, OS or tenancy, and also apply to Fargate and Lambda usage.\\\"EC2 instance Savings Plans are not applied to Fargate or Lambda",
          "upvote_count": "8",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 841615,
          "date": "Fri 17 Mar 2023 06:21",
          "username": "\t\t\t\tNoviiiice\t\t\t",
          "content": "Why not B?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 765040,
          "date": "Tue 03 Jan 2023 21:51",
          "username": "\t\t\t\taba2s\t\t\t",
          "content": "Compute Savings Plans can be used for EC2 instances and Fargate. Whereas EC2 Savings Plans support EC2 only.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 760588,
          "date": "Thu 29 Dec 2022 05:55",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "To optimize the cost of running this application on AWS, you should consider the following options:<br>A.  Use Spot Instances for the data ingestion layerC.  Purchase a 1-year Compute Savings Plan for the front-end and API layer<br><br>Therefore, the most cost-effective solution for hosting this application would be to use Spot Instances for the data ingestion layer and to purchase either a 1-year Compute Savings Plan or a 1-year EC2 instance Savings Plan for the front-end and API layer.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 757812,
          "date": "Mon 26 Dec 2022 21:19",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "Too obvious answer.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 755359,
          "date": "Sun 25 Dec 2022 02:22",
          "username": "\t\t\t\tberks\t\t\t",
          "content": "AC<br> can be interrupted at any time => spot",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 754820,
          "date": "Sat 24 Dec 2022 12:01",
          "username": "\t\t\t\tTECHNOWARRIOR\t\t\t",
          "content": "A,E::<br>Savings Plan  EC2<br>Savings Plan offers almost the same savings from a cost as RIs and adds additional Automation around how the savings are being applied. One way to understand is to say that EC2 Savings Plan are Standard Reserved Instances with automatic switching depending on Instance types being used within the same instance family and additionally applied to ECS Fargate and Lambda.<br><br>Savings Plan  Compute<br>Savings Plan offers almost the same savings from a cost as RIs and adds additional Automation around how the savings are being applied. For example, they provide flexibility around instance types and regions so that you don't have to monitor new instance types that are being launched. It is also applied to Lambda and ECS Fargate workloads. One way to understand is to say that Compute Savings Plan are Convertible Reserved Instances with automatic switching depending on Instance types being used.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 748436,
          "date": "Sat 17 Dec 2022 22:40",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "A and C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 718669,
          "date": "Tue 15 Nov 2022 11:28",
          "username": "\t\t\t\trjam\t\t\t",
          "content": "its A and C . https://www.densify.com/finops/aws-savings-plan",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 705466,
          "date": "Thu 27 Oct 2022 13:06",
          "username": "\t\t\t\tbunnychip\t\t\t",
          "content": "api is not EC2.need to use compute savings plan",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 700308,
          "date": "Thu 20 Oct 2022 23:18",
          "username": "\t\t\t\tChunsli\t\t\t",
          "content": "E makes more sense than C.  See https://aws.amazon.com/savingsplans/faq/, EC2 instance Savings Plan (up to 72% saving) costs less than Compute Savings Plan (up to 66% saving)<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Isn't the EC2 Instance Savings Plan not applicable to Fargate and Lambda?<br>https://aws.amazon.com/savingsplans/compute-pricing/</li></ul>",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 705262,
          "date": "Thu 27 Oct 2022 07:37",
          "username": "\t\t\t\tcapepenguin\t\t\t",
          "content": "Isn't the EC2 Instance Savings Plan not applicable to Fargate and Lambda?<br>https://aws.amazon.com/savingsplans/compute-pricing/",
          "upvote_count": "6",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#141",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company runs a web-based portal that provides users with global breaking news, local alerts, and weather updates. The portal delivers each user a personalized view by using mixture of static and dynamic content. Content is served over HTTPS through an API server running on an Amazon EC2 instance behind an Application Load Balancer (ALB). The company wants the portal to provide this content to its users across the world as quickly as possible.<br>How should a solutions architect design the application to ensure the LEAST amount of latency for all users?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#141",
          "answers": [
            {
              "choice": "<p>A. Deploy the application stack in a single AWS Region. Use Amazon CloudFront to serve all static and dynamic content by specifying the ALB as an origin.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Deploy the application stack in two AWS Regions. Use an Amazon Route 53 latency routing policy to serve all content from the ALB in the closest Region.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Deploy the application stack in a single AWS Region. Use Amazon CloudFront to serve the static content. Serve the dynamic content directly from the ALB. <br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Deploy the application stack in two AWS Regions. Use an Amazon Route 53 geolocation routing policy to serve all content from the ALB in the closest Region.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 694483,
          "date": "Fri 14 Oct 2022 06:21",
          "username": "\t\t\t\thuiy\t\t\t",
          "content": "Answer is A. <br>Amazon CloudFront is a web service that speeds up distribution of your static and dynamic web content<br>https://www.examtopics.com/discussions/amazon/view/81081-exam-aws-certified-solutions-architect-associate-saa-c02/",
          "upvote_count": "17",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 704873,
          "date": "Wed 26 Oct 2022 18:55",
          "username": "\t\t\t\tSix_Fingered_Jose\t\t\t",
          "content": "Answer should be B,<br><br>CloudFront reduces latency if its only static content, which is not the case here.<br>For Dynamic content, CF cant cache the content so it sends the traffic through the AWS Network which does reduces latency, but it still has to travel through another region.<br><br>For the case with 2 region and Route 53 latency routing, Route 53 detects the nearest resouce (with lowest latency) and routes the traffic there. Because the traffic does not have to travel toresources far away, it should have the least latency in this case here.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Cf works for both static and dynamic content</li><li>CloudFront does not cache dynamic content. But Latency can be still low for dynamic content because the traffic is on the AWS global network which is faster than the internet.</li><li>Amazon CloudFront speeds up distribution of your static and dynamic web content, such as .html, .css, .php, image, and media files. When users request your content, CloudFront delivers it through a worldwide network of edge locations that provide low latency and high performance.</li><li>Can you pls. provide a ref. link from where this info. got extracted?</li></ul>",
          "upvote_count": "6",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 711725,
          "date": "Sat 05 Nov 2022 12:31",
          "username": "\t\t\t\tOnimole\t\t\t",
          "content": "Cf works for both static and dynamic content",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 768795,
          "date": "Sat 07 Jan 2023 18:58",
          "username": "\t\t\t\tMahadeva\t\t\t",
          "content": "CloudFront does not cache dynamic content. But Latency can be still low for dynamic content because the traffic is on the AWS global network which is faster than the internet.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Amazon CloudFront speeds up distribution of your static and dynamic web content, such as .html, .css, .php, image, and media files. When users request your content, CloudFront delivers it through a worldwide network of edge locations that provide low latency and high performance.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 779010,
          "date": "Tue 17 Jan 2023 15:17",
          "username": "\t\t\t\tJoxtat\t\t\t",
          "content": "Amazon CloudFront speeds up distribution of your static and dynamic web content, such as .html, .css, .php, image, and media files. When users request your content, CloudFront delivers it through a worldwide network of edge locations that provide low latency and high performance.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 732007,
          "date": "Wed 30 Nov 2022 22:37",
          "username": "\t\t\t\tAamee\t\t\t",
          "content": "Can you pls. provide a ref. link from where this info. got extracted?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 848571,
          "date": "Thu 23 Mar 2023 20:02",
          "username": "\t\t\t\tjaswantn\t\t\t",
          "content": "Having stack in two Regions is always better than one Region, when portal has to be used globally. This crosses out Option A and C.  <br>Requirement is to have LEAST amount of latency , so instead of choosing Route 53 Geolocation routing policy (Option D), we should go for Latency based routing; which is Option B. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 840137,
          "date": "Wed 15 Mar 2023 19:02",
          "username": "\t\t\t\tUnluckyDucky\t\t\t",
          "content": "Something is wrong with the question, or the answers.<br><br>The best way to do it is deploy the website in one region, use CloudFront to reduce latency and use a geolocation Route 53 routing policy as the application provides local alerts and weather alerts.<br><br>Without geolocation the application will provide local alerts in London to people living in Australia.<br><br>Answer D is the closet, however - it's wrong.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 838055,
          "date": "Mon 13 Mar 2023 17:07",
          "username": "\t\t\t\tmell1222\t\t\t",
          "content": "Use Amazon CloudFront as a content delivery network (CDN) to distribute static and dynamic content to edge locations around the world",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 826080,
          "date": "Wed 01 Mar 2023 17:40",
          "username": "\t\t\t\tGary_Phillips_2007\t\t\t",
          "content": "A for me.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 825484,
          "date": "Wed 01 Mar 2023 03:34",
          "username": "\t\t\t\tKZM\t\t\t",
          "content": "A is impossible I think. Because when using Amazon CloudFront to serve static content, the content should be stored in an Amazon S3 bucket, and CloudFront should be configured to use that S3 bucket as the origin instead of ALB. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 820205,
          "date": "Fri 24 Feb 2023 08:23",
          "username": "\t\t\t\tmoaaz86\t\t\t",
          "content": "For those who doubt the fact about CloudFront and dynamic content, see this video on how Slack utilized CloudFront for this purpose. Pretty interesting stuff.<br><br>https://aws.amazon.com/cloudfront/dynamic-content/",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 819613,
          "date": "Thu 23 Feb 2023 20:14",
          "username": "\t\t\t\tanthony2021\t\t\t",
          "content": "As its a new site even the static content will be frequently refeshed, requiring cloudfront to request the content, a two region solution looks best",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 812781,
          "date": "Sat 18 Feb 2023 09:31",
          "username": "\t\t\t\tRehan33\t\t\t",
          "content": "Why not going for option C<br>Use cloud front for static content <br>Use application load balancer for dynamic content",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 810619,
          "date": "Thu 16 Feb 2023 13:13",
          "username": "\t\t\t\tHelp2023\t\t\t",
          "content": "Cloudfront does static and dynamic. It is purpose is to provide common data in the shortest time possible.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 801860,
          "date": "Wed 08 Feb 2023 10:58",
          "username": "\t\t\t\tKZM\t\t\t",
          "content": "I think, it is A. <br>Option A, deploying the application stack in a single AWS Region and using Amazon CloudFront to serve all static and dynamic content, may not provide the least amount of latency for all users as users located far away from the single region may experience higher latency due to the distance between their location and the region hosting the application stack.<br><br>Option B, deploying the application stack in two AWS Regions and using an Amazon Route 53 latency routing policy to serve all content from the ALB in the closest region, is a better solution as it allows the application to be closer to the users, resulting in lower latency for users located in different regions of the world.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>CloudFront is not designed to cache dynamic content, but it can cache static content, such as images, videos, or JavaScript and CSS files. Dynamic content is content that changes frequently, such as news articles or weather updates, and is generated by a server in real-time in response to each user's request.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 802768,
          "date": "Thu 09 Feb 2023 04:05",
          "username": "\t\t\t\tKZM\t\t\t",
          "content": "CloudFront is not designed to cache dynamic content, but it can cache static content, such as images, videos, or JavaScript and CSS files. Dynamic content is content that changes frequently, such as news articles or weather updates, and is generated by a server in real-time in response to each user's request.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 779025,
          "date": "Tue 17 Jan 2023 15:30",
          "username": "\t\t\t\tJoxtat\t\t\t",
          "content": "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/HowCloudFrontWorks.html#CloudFrontRegionaledgecaches",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 774948,
          "date": "Sat 14 Jan 2023 00:52",
          "username": "\t\t\t\tmj61\t\t\t",
          "content": "Option A is incorrect because it deploys the application stack in a single AWS region and uses Amazon CloudFront to serve all static and dynamic content. While this approach will cache the static content at edge locations, it does not take into account the geographical location of the users, and therefore will not minimize the latency for all users. The dynamic content will still be served from the origin which is the ALB, so users far from the region where the ALB is deployed will have high latency.<br>It also does not provide redundancy and fault tolerance as it only deployed in single region.<br><br>In summary, deploying the application stack in a single region and using CloudFront to serve all content may improve performance for users in close proximity to the region, but it will not minimize latency for all users globally, while option B takes into account the geographical location of the users and serves them the content from the closest region which results in low latency.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 765065,
          "date": "Tue 03 Jan 2023 22:23",
          "username": "\t\t\t\taba2s\t\t\t",
          "content": "https://aws.amazon.com/blogs/networking-and-content-delivery/deliver-your-apps-dynamic-content-using-amazon-cloudfront-getting-started-template/",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 760597,
          "date": "Thu 29 Dec 2022 06:14",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "I would go for Option B as the correct answer.<br><br>By deploying the application stack in two regions and using an Amazon Route 53 latency routing policy, you can ensure that users are served from the ALB in the region that is closest to them, reducing latency. Amazon Route 53 latency routing works by monitoring the latency between the users and the different regions and routing traffic to the region with the lowest latency.<br><br>Option A is incorrect, while using Amazon CloudFront to serve static and dynamic content can improve the performance of the application, deploying the application stack in a single region may not be sufficient to reduce latency for users located in different parts of the world.<br><br>Therefore, the correct solution to ensure the least amount of latency for all users is to deploy the application stack in two AWS Regions and use either an Amazon Route 53 latency routing policy or an Amazon Route 53 geolocation routing policy to serve all content from the ALB in the closest region.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Answer is A. <br>https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/HowCloudFrontWorks.html#CloudFrontRegionaledgecaches</li><li>Link to the documentation for Amazon Route 53 Latency-Based Routing:<br><br>https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-latency<br><br>This routing policy allows you to route traffic to the Amazon EC2 instance, Amazon S3 bucket, Amazon CloudFront distribution, or other resources with the lowest latency. It is useful when you want to serve users the content from the location that provides the lowest latency.</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 779022,
          "date": "Tue 17 Jan 2023 15:29",
          "username": "\t\t\t\tJoxtat\t\t\t",
          "content": "Answer is A. <br>https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/HowCloudFrontWorks.html#CloudFrontRegionaledgecaches",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 760600,
          "date": "Thu 29 Dec 2022 06:18",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Link to the documentation for Amazon Route 53 Latency-Based Routing:<br><br>https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-latency<br><br>This routing policy allows you to route traffic to the Amazon EC2 instance, Amazon S3 bucket, Amazon CloudFront distribution, or other resources with the lowest latency. It is useful when you want to serve users the content from the location that provides the lowest latency.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 748442,
          "date": "Sat 17 Dec 2022 22:49",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "A is the best option as Cloudfront can deliver the content from the Edge location that will be lowest latency for static content to all users across the globe. Deploying in two region is not sufficient for the users that might be still far away from two regions. So option B will will not provide lowest possible latency to \\\"ALL users\\\" which is the key here.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Read following two articles on why A is right option.<br>https://aws.amazon.com/blogs/networking-and-content-delivery/deliver-your-apps-dynamic-content-using-amazon-cloudfront-getting-started-template/<br>https://aws.amazon.com/cloudfront/dynamic-content/</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 754605,
          "date": "Sat 24 Dec 2022 00:37",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Read following two articles on why A is right option.<br>https://aws.amazon.com/blogs/networking-and-content-delivery/deliver-your-apps-dynamic-content-using-amazon-cloudfront-getting-started-template/<br>https://aws.amazon.com/cloudfront/dynamic-content/",
          "upvote_count": "4",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#142",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A gaming company is designing a highly available architecture. The application runs on a modified Linux kernel and supports only UDP-based traffic. The company needs the front-end tier to provide the best possible user experience. That tier must have low latency, route traffic to the nearest edge location, and provide static IP addresses for entry into the application endpoints.<br>What should a solutions architect do to meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#142",
          "answers": [
            {
              "choice": "<p>A. Configure Amazon Route 53 to forward requests to an Application Load Balancer. Use AWS Lambda for the application in AWS Application Auto Scaling.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Configure Amazon CloudFront to forward requests to a Network Load Balancer. Use AWS Lambda for the application in an AWS Application Auto Scaling group.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Configure AWS Global Accelerator to forward requests to a Network Load Balancer. Use Amazon EC2 instances for the application in an EC2 Auto Scaling group.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure Amazon API Gateway to forward requests to an Application Load Balancer. Use Amazon EC2 instances for the application in an EC2 Auto Scaling group.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 707684,
          "date": "Sun 30 Oct 2022 08:59",
          "username": "\t\t\t\tdokaedu\t\t\t",
          "content": "Correct Answer: C <br>AWS Global Accelerator and Amazon CloudFront are separate services that use the AWS global network and its edge locations around the world. CloudFront improves performance for both cacheable content (such as images and videos) and dynamic content (such as API acceleration and dynamic site delivery). Global Accelerator improves performance for a wide range of applications over TCP or UDP by proxying packets at the edge to applications running in one or more AWS Regions. Global Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP, as well as for HTTP use cases that specifically require static IP addresses or deterministic, fast regional failover. Both services integrate with AWS Shield for DDoS protection.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Explained very well. ty</li><li>On top of this, lambda would not be able to run application that is running on a modified Linux kernel. The answer is C .</li><li>Thank you, your explanation helped me to better understand even the answer of question 29</li></ul>",
          "upvote_count": "33",
          "selected_answers": ""
        },
        {
          "id": 764913,
          "date": "Tue 03 Jan 2023 18:54",
          "username": "\t\t\t\tpraveenas400\t\t\t",
          "content": "Explained very well. ty",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 739858,
          "date": "Fri 09 Dec 2022 07:56",
          "username": "\t\t\t\tstepman\t\t\t",
          "content": "On top of this, lambda would not be able to run application that is running on a modified Linux kernel. The answer is C .",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 724888,
          "date": "Wed 23 Nov 2022 05:46",
          "username": "\t\t\t\tiCcma\t\t\t",
          "content": "Thank you, your explanation helped me to better understand even the answer of question 29",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 833691,
          "date": "Thu 09 Mar 2023 08:57",
          "username": "\t\t\t\tDevsin2000\t\t\t",
          "content": "C - https://aws.amazon.com/global-accelerator/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 770457,
          "date": "Mon 09 Jan 2023 14:42",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "To meet the requirements of providing low latency, routing traffic to the nearest edge location, and providing static IP addresses for entry into the application endpoints, the best solution would be to use AWS Global Accelerator. This service routes traffic to the nearest edge location and provides static IP addresses for the application endpoints. The front-end tier should be configured with a Network Load Balancer, which can handle UDP-based traffic and provide high availability. Option C, \\\"Configure AWS Global Accelerator to forward requests to a Network Load Balancer. Use Amazon EC2 instances for the application in an EC2 Auto Scaling group,\\\" is the correct answer.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 760606,
          "date": "Thu 29 Dec 2022 06:25",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The correct answer is Option C.  To meet the requirements;<br><br>* AWS Global Accelerator is a service that routes traffic to the nearest edge location, providing low latency and static IP addresses for the front-end tier. It supports UDP-based traffic, which is required by the application.<br><br>* A Network Load Balancer is a layer 4 load balancer that can handle UDP traffic and provide static IP addresses for the application endpoints.<br><br>* An EC2 Auto Scaling group ensures that the required number of Amazon EC2 instances is available to meet the demand of the application. This will help the front-end tier to provide the best possible user experience.<br><br>Option A is not a valid solution because Amazon Route 53 does not support UDP traffic.<br>Option B is not a valid solution because Amazon CloudFront does not support UDP traffic.<br>Option D is not a valid solution because Amazon API Gateway does not support UDP traffic.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>My mistake, correction on Option A, it is the Application Load Balancers do not support UDP traffic. They are designed to load balance HTTP and HTTPS traffic, and they do not support other protocols such as UDP.</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 760608,
          "date": "Thu 29 Dec 2022 06:32",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "My mistake, correction on Option A, it is the Application Load Balancers do not support UDP traffic. They are designed to load balance HTTP and HTTPS traffic, and they do not support other protocols such as UDP.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 757840,
          "date": "Mon 26 Dec 2022 21:52",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "C is obvious choice here.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 748445,
          "date": "Sat 17 Dec 2022 22:54",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "C as Global Accelerator is the best choice for UDP based traffic needing static IP address.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 745385,
          "date": "Wed 14 Dec 2022 20:05",
          "username": "\t\t\t\tCertified101\t\t\t",
          "content": "c correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 743447,
          "date": "Tue 13 Dec 2022 01:34",
          "username": "\t\t\t\tQjb8m9h\t\t\t",
          "content": "CloudFront is designed to handle HTTP protocol meanwhile Global Accelerator is best used for both HTTP and non-HTTP protocols such as TCP and UDP. HENCE C is the ANSWER!",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 726093,
          "date": "Thu 24 Nov 2022 18:53",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 712936,
          "date": "Mon 07 Nov 2022 09:42",
          "username": "\t\t\t\tPS_R\t\t\t",
          "content": "Cloud Fronts supports both Static and Dynamic and Global Accelerator means low latency over UDP",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        }
      ]
    },
    {
      "question_id": "#143",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company wants to migrate its existing on-premises monolithic application to AWS. The company wants to keep as much of the front-end code and the backend code as possible. However, the company wants to break the application into smaller applications. A different team will manage each application. The company needs a highly scalable solution that minimizes operational overhead.<br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#143",
          "answers": [
            {
              "choice": "<p>A. Host the application on AWS Lambda. Integrate the application with Amazon API Gateway.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Host the application with AWS Amplify. Connect the application to an Amazon API Gateway API that is integrated with AWS Lambda.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Host the application on Amazon EC2 instances. Set up an Application Load Balancer with EC2 instances in an Auto Scaling group as targets.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Host the application on Amazon Elastic Container Service (Amazon ECS). Set up an Application Load Balancer with Amazon ECS as the target.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 707404,
          "date": "Sat 29 Oct 2022 20:41",
          "username": "\t\t\t\tKen701\t\t\t",
          "content": "I think the answer here is \\\"D\\\" because usually when you see terms like \\\"monolithic\\\" the answer will likely refer to microservices.",
          "upvote_count": "18",
          "selected_answers": ""
        },
        {
          "id": 714174,
          "date": "Wed 09 Nov 2022 01:31",
          "username": "\t\t\t\tBevemo\t\t\t",
          "content": "D is organic pattern, lift and shift, decompose to containers, first making most use of existing code, whilst new features can be added over time with lambda+api gw later.<br>A is leapfrog pattern. requiring refactoring all code up front.",
          "upvote_count": "11",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 833700,
          "date": "Thu 09 Mar 2023 09:11",
          "username": "\t\t\t\tDevsin2000\t\t\t",
          "content": "I think the answer is A <br>B is wrong because the requirement is not for the backend. C and D are not suitable because the ALB Is not best suited for middle tier applications.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 784372,
          "date": "Sun 22 Jan 2023 15:06",
          "username": "\t\t\t\taws4myself\t\t\t",
          "content": "I will go with A because - less operational and High availability (Lambda has these)<br><br>If it is ECS, operational overhead and can only be scaled up to an EC2 assigned under it.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 770476,
          "date": "Mon 09 Jan 2023 14:49",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "To meet the requirement of breaking the application into smaller applications that can be managed by different teams, while minimizing operational overhead and providing high scalability, the best solution would be to host the applications on Amazon Elastic Container Service (Amazon ECS). Amazon ECS is a fully managed container orchestration service that makes it easy to run, scale, and maintain containerized applications. Additionally, setting up an Application Load Balancer with Amazon ECS as the target will allow the company to easily scale the application as needed. Option D, \\\"Host the application on Amazon Elastic Container Service (Amazon ECS). Set up an Application Load Balancer with Amazon ECS as the target,\\\" is the correct answer.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 763136,
          "date": "Sun 01 Jan 2023 11:03",
          "username": "\t\t\t\tZerotn3\t\t\t",
          "content": ". Host the application on Amazon Elastic Container Service (Amazon ECS). Set up an Application Load Balancer with Amazon ECS as the target.<br><br>Hosting the application on Amazon ECS would allow the company to break the monolithic application into smaller, more manageable applications that can be managed by different teams. Amazon ECS is a fully managed container orchestration service that makes it easy to deploy, run, and scale containerized applications. By setting up an Application Load Balancer with Amazon ECS as the target, the company can ensure that the solution is highly scalable and minimizes operational overhead.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 760633,
          "date": "Thu 29 Dec 2022 07:05",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The correct answer is Option D.  To meet the requirements, the company should host the application on Amazon Elastic Container Service (Amazon ECS) and set up an Application Load Balancer with Amazon ECS as the target.<br><br>Option A is not a valid solution because AWS Lambda is not suitable for hosting long-running applications.<br><br>Option B is not a valid solution because AWS Amplify is a framework for building, deploying, and managing web applications, not a hosting solution.<br><br>Option C is not a valid solution because Amazon EC2 instances are not fully managed container orchestration services. The company will need to manage the EC2 instances, which will increase operational overhead.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 748602,
          "date": "Sun 18 Dec 2022 05:20",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "It can be C or D depending on how easy it would be to containerize the application. If application needs persistent local data store then C would be a better choice. <br>Also from the usecase description it is not clear whether application is http based application or not though all options uses ALB only so we can safely assume that this is http based application only.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>After reading this question again A will be minimum operational overhead. <br>D has higher operational overhead as D will have operational overhead of scaling EC2 servers up/down for running ECS containers.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 754595,
          "date": "Sat 24 Dec 2022 00:14",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "After reading this question again A will be minimum operational overhead. <br>D has higher operational overhead as D will have operational overhead of scaling EC2 servers up/down for running ECS containers.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 726095,
          "date": "Thu 24 Nov 2022 18:54",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "D is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 712109,
          "date": "Sun 06 Nov 2022 02:11",
          "username": "\t\t\t\tbackbencher2022\t\t\t",
          "content": "I think D is the right choice as they want application to be managed by different people which could be enabled by breaking it into different containers",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 707652,
          "date": "Sun 30 Oct 2022 07:43",
          "username": "\t\t\t\tSimonPark\t\t\t",
          "content": "imho, it's D because \\\"break the application into smaller applications\\\" doesn't mean it has to be 'serverless'. Rather it can be divided into smaller application running on containers.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 704875,
          "date": "Wed 26 Oct 2022 19:00",
          "username": "\t\t\t\tSix_Fingered_Jose\t\t\t",
          "content": "I think A is the answer here, breaking into smaller pieces so lambda makes the most sense.<br>I don't see any restrictions in the question that forbids the usage of lambda<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>The reason for not choosing A: \\\"The company wants to keep as much of the front-end code and the backend code as possible\\\"</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 725466,
          "date": "Thu 24 Nov 2022 01:32",
          "username": "\t\t\t\tNewptone\t\t\t",
          "content": "The reason for not choosing A: \\\"The company wants to keep as much of the front-end code and the backend code as possible\\\"",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#144",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company recently started using Amazon Aurora as the data store for its global ecommerce application. When large reports are run, developers report that the ecommerce application is performing poorly. After reviewing metrics in Amazon CloudWatch, a solutions architect finds that the ReadIOPS and CPUUtilizalion metrics are spiking when monthly reports run.<br>What is the MOST cost-effective solution?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#144",
          "answers": [
            {
              "choice": "<p>A. Migrate the monthly reporting to Amazon Redshift.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Migrate the monthly reporting to an Aurora Replica.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Migrate the Aurora database to a larger instance class.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Increase the Provisioned IOPS on the Aurora instance.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 770478,
          "date": "Mon 09 Jan 2023 14:50",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "The most cost-effective solution for addressing high ReadIOPS and CPU utilization when running large reports would be to migrate the monthly reporting to an Aurora Replica. An Aurora Replica is a read-only copy of an Aurora database that is updated in real-time with the primary database. By using an Aurora Replica for running large reports, the primary database will be relieved of the additional read load, improving performance for the ecommerce application. Option B, \\\"Migrate the monthly reporting to an Aurora Replica,\\\" is the correct answer.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 753013,
          "date": "Thu 22 Dec 2022 07:23",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option B: Migrating the monthly reporting to an Aurora Replica may be the most cost-effective solution because it involves creating a read-only copy of the database that can be used specifically for running large reports without impacting the performance of the primary database. This solution allows the company to scale the read capacity of the database without incurring additional hardware or I/O costs.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>The incorrect solutions are:<br><br>Option A: Migrating the monthly reporting to Amazon Redshift may not be cost-effective because it involves creating a new data store and potentially significant data migration and ETL costs.<br><br>Option C: Migrating the Aurora database to a larger instance class may not be cost-effective because it involves changing the underlying hardware of the database and potentially incurring additional costs for the larger instance.<br><br>Option D: Increasing the Provisioned IOPS on the Aurora instance may not be cost-effective because it involves paying for additional I/O capacity that may not be necessary for other workloads on the database.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 753014,
          "date": "Thu 22 Dec 2022 07:23",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The incorrect solutions are:<br><br>Option A: Migrating the monthly reporting to Amazon Redshift may not be cost-effective because it involves creating a new data store and potentially significant data migration and ETL costs.<br><br>Option C: Migrating the Aurora database to a larger instance class may not be cost-effective because it involves changing the underlying hardware of the database and potentially incurring additional costs for the larger instance.<br><br>Option D: Increasing the Provisioned IOPS on the Aurora instance may not be cost-effective because it involves paying for additional I/O capacity that may not be necessary for other workloads on the database.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 748452,
          "date": "Sat 17 Dec 2022 23:09",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "B is the best option",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 742446,
          "date": "Mon 12 Dec 2022 07:10",
          "username": "\t\t\t\tsanket1990\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 724395,
          "date": "Tue 22 Nov 2022 16:14",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 712111,
          "date": "Sun 06 Nov 2022 02:14",
          "username": "\t\t\t\tbackbencher2022\t\t\t",
          "content": "ReadIOPS issue inclining towards Read Replica as the most cost effective solution here",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 709035,
          "date": "Tue 01 Nov 2022 11:31",
          "username": "\t\t\t\trjam\t\t\t",
          "content": "Answer B",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#145",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company hosts a website analytics application on a single Amazon EC2 On-Demand Instance. The analytics software is written in PHP and uses a MySQL database. The analytics software, the web server that provides PHP, and the database server are all hosted on the EC2 instance. The application is showing signs of performance degradation during busy times and is presenting 5xx errors. The company needs to make the application scale seamlessly.<br>Which solution will meet these requirements MOST cost-effectively?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#145",
          "answers": [
            {
              "choice": "<p>A. Migrate the database to an Amazon RDS for MySQL DB instance. Create an AMI of the web application. Use the AMI to launch a second EC2 On-Demand Instance. Use an Application Load Balancer to distribute the load to each EC2 instance.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Migrate the database to an Amazon RDS for MySQL DB instance. Create an AMI of the web application. Use the AMI to launch a second EC2 On-Demand Instance. Use Amazon Route 53 weighted routing to distribute the load across the two EC2 instances.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Migrate the database to an Amazon Aurora MySQL DB instance. Create an AWS Lambda function to stop the EC2 instance and change the instance type. Create an Amazon CloudWatch alarm to invoke the Lambda function when CPU utilization surpasses 75%.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Migrate the database to an Amazon Aurora MySQL DB instance. Create an AMI of the web application. Apply the AMI to a launch template. Create an Auto Scaling group with the launch template Configure the launch template to use a Spot Fleet. Attach an Application Load Balancer to the Auto Scaling group.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 844837,
          "date": "Mon 20 Mar 2023 12:55",
          "username": "\t\t\t\tSuketuKohli\t\t\t",
          "content": "A Spot Fleet is a set of Spot Instances and optionally On-Demand Instances that is launched based on criteria that you specify. The Spot Fleet selects the Spot capacity pools that meet your needs and launches Spot Instances to meet the target capacity for the fleet. By default, Spot Fleets are set to maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated. You can submit a Spot Fleet as a one-time request, which does not persist after the instances have been terminated. You can include On-Demand Instance requests in a Spot Fleet request.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 803064,
          "date": "Thu 09 Feb 2023 11:12",
          "username": "\t\t\t\tKZM\t\t\t",
          "content": "Ans: D<br>Both Amazon RDS for MySQL and Amazon Aurora MySQL are designed to provide customers with fully managed relational database services, but Amazon Aurora MySQL is designed to provide better performance, scalability, and reliability, making it a better option for customers who need high-performance database services.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 783745,
          "date": "Sat 21 Jan 2023 22:42",
          "username": "\t\t\t\tbullrem\t\t\t",
          "content": "Using an Auto Scaling group with a launch template and a Spot Fleet allows the company to scale the application seamlessly and cost-effectively, by automatically adding or removing instances based on the demand, and using Spot instances which are spare compute capacity available in the AWS region at a lower price than On-Demand instances. And also by migrating the database to Amazon Aurora MySQL DB instance, it provides higher scalability, availability, and performance than traditional MySQL databases.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 777190,
          "date": "Mon 16 Jan 2023 01:01",
          "username": "\t\t\t\tBakedBacon\t\t\t",
          "content": "The answer is D:<br>Migrate the database to Amazon Aurora MySQL - this will let the DB scale on it's own; it'll scale automaticallywithout needing adjustment.<br>Create AMI of the web app and using a launch template - this will make the creating of any future instances of the app seamless. They can then be added to the auto scaling group which will save them money as it will scale up and down based on demand. <br>Using a spot fleet to launch instances- This solves the \\\"MOST cost-effective\\\" portion of the question as spot instances come at a huge discount at the cost of being terminated at any time Amazon deems fit. I think this is why there's a bit of disagreement on this. While it's the most cost effective, it would be a terrible choice if amazon were to terminate that spot instance during a busy period.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 763860,
          "date": "Mon 02 Jan 2023 17:53",
          "username": "\t\t\t\tgustavtd\t\t\t",
          "content": "But I have a question,<br>For Spot instance, is it possible that at some time there is no spot resources available at all? because it is not guaranteed, right?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Spot fleet not spot instance mentioned over there. Spot fleet = Spot instance + on-demand instance. If we cannot manage the spot instance then we can use an on-demand instance.</li></ul>",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 806008,
          "date": "Sun 12 Feb 2023 06:55",
          "username": "\t\t\t\tRupak10\t\t\t",
          "content": "Spot fleet not spot instance mentioned over there. Spot fleet = Spot instance + on-demand instance. If we cannot manage the spot instance then we can use an on-demand instance.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 763140,
          "date": "Sun 01 Jan 2023 11:09",
          "username": "\t\t\t\tZerotn3\t\t\t",
          "content": "Option D is the most cost-effective solution that meets the requirements.<br><br>Migrating the database to Amazon Aurora MySQL will allow the database to scale automatically, so it can handle an increase in traffic without manual intervention. Creating an AMI of the web application and using a launch template will allow the company to quickly and easily launch new instances of the application, which can then be added to an Auto Scaling group. This will allow the application to automatically scale up and down based on demand, ensuring that there are enough resources to handle busy times without incurring the cost of running idle resources.<br><br>Using a Spot Fleet to launch the instances will allow the company to take advantage of Amazon's spare capacity and get a discount on their EC2 instances. Attaching an Application Load Balancer to the Auto Scaling group will allow the load to be distributed across all of the available instances, improving the performance and reliability of the application.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 760643,
          "date": "Thu 29 Dec 2022 07:16",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option D is the most cost-effective solution because;<br><br>* it uses an Auto Scaling group with a launch template and a Spot Fleet to automatically scale the number of EC2 instances based on the workload.<br><br>* using a Spot Fleet allows the company to take advantage of the lower prices of Spot Instances while still providing the required performance and availability for the application.<br><br>* using an Aurora MySQL database instance allows the company to take advantage of the scalability and performance of Aurora.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 757845,
          "date": "Mon 26 Dec 2022 22:01",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "D ,as only this has auto scaling",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 754943,
          "date": "Sat 24 Dec 2022 15:45",
          "username": "\t\t\t\tSahilbhai\t\t\t",
          "content": "ANSWER IS D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 748606,
          "date": "Sun 18 Dec 2022 05:27",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "D is the right option. A is possible but it will have high cost due to on-demand instances. It is not mentioned that 24x7 application availability is high priority goal.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 745464,
          "date": "Wed 14 Dec 2022 22:28",
          "username": "\t\t\t\tsanyoc\t\t\t",
          "content": "correct is D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 745392,
          "date": "Wed 14 Dec 2022 20:16",
          "username": "\t\t\t\tCertified101\t\t\t",
          "content": "\\\"You can submit a Spot Fleet as a one-time request, which does not persist after the instances have been terminated. You can include On-Demand Instance requests in a Spot Fleet request.\\\"<br><br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-fleet.html",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 741217,
          "date": "Sat 10 Dec 2022 21:08",
          "username": "\t\t\t\tlapaki\t\t\t",
          "content": "D. other answers don't deal with scaling.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 725805,
          "date": "Thu 24 Nov 2022 12:57",
          "username": "\t\t\t\tds0321\t\t\t",
          "content": "D is correct",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 724396,
          "date": "Tue 22 Nov 2022 16:15",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "D is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 707708,
          "date": "Sun 30 Oct 2022 09:41",
          "username": "\t\t\t\tdokaedu\t\t\t",
          "content": "Answer: D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 707657,
          "date": "Sun 30 Oct 2022 07:49",
          "username": "\t\t\t\tSimonPark\t\t\t",
          "content": "I think D is the answer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        }
      ]
    },
    {
      "question_id": "#146",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company runs a stateless web application in production on a group of Amazon EC2 On-Demand Instances behind an Application Load Balancer. The application experiences heavy usage during an 8-hour period each business day. Application usage is moderate and steady overnight. Application usage is low during weekends.<br>The company wants to minimize its EC2 costs without affecting the availability of the application.<br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#146",
          "answers": [
            {
              "choice": "<p>A. Use Spot Instances for the entire workload.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use Reserved Instances for the baseline level of usage. Use Spot instances for any additional capacity that the application needs.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use On-Demand Instances for the baseline level of usage. Use Spot Instances for any additional capacity that the application needs.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use Dedicated Instances for the baseline level of usage. Use On-Demand Instances for any additional capacity that the application needs.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 709261,
          "date": "Tue 01 Nov 2022 16:48",
          "username": "\t\t\t\trob74\t\t\t",
          "content": "In the Question is mentioned that it has oDemand instances...so I think is more cheapest reserved and spot",
          "upvote_count": "10",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 760649,
          "date": "Thu 29 Dec 2022 07:22",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option B is the most cost-effective solution that meets the requirements.<br><br>* Using Reserved Instances for the baseline level of usage will provide a discount on the EC2 costs for steady overnight and weekend usage. <br><br>* Using Spot Instances for any additional capacity that the application needs during peak usage times will allow the company to take advantage of spare capacity in the region at a lower cost than On-Demand Instances.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 757850,
          "date": "Mon 26 Dec 2022 22:03",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 748607,
          "date": "Sun 18 Dec 2022 05:30",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option B is most cost effective without compromising the availability for baseline load.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 743467,
          "date": "Tue 13 Dec 2022 02:04",
          "username": "\t\t\t\tQjb8m9h\t\t\t",
          "content": "Answer is B: Reserved is cheaper than on demand the company has.And it's meet the availabilty (HA) requirement as to spot instance that can be disrupted at any time.<br>PRICING BELOW.<br>On-Demand: 0% There's no commitment from you. You pay the most with this option.<br>Reserved :40%-60%1-year or 3-year commitment from you. You save money from that commitment.<br>Spot 50%-90% Ridiculously inexpensive because there's no commitment from the AWS side.",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 724414,
          "date": "Tue 22 Nov 2022 16:38",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "B IS CORRECT",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 714614,
          "date": "Wed 09 Nov 2022 14:49",
          "username": "\t\t\t\tArielSchivo\t\t\t",
          "content": "They are currently using On Demand instances, so option C is out.<br>A uses Spot instances which is not recommended for PROD and D uses Dedicated instances which are expensive.<br>So option B should be the one.",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 708918,
          "date": "Tue 01 Nov 2022 08:05",
          "username": "\t\t\t\tDsouzaf\t\t\t",
          "content": "If we select B, Spot instance are reliable though it saves cost.<br>In D: base line & additional capacity is also On-Demand. Expensive than Reserve Instance but will not bring down Production",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 708873,
          "date": "Tue 01 Nov 2022 06:44",
          "username": "\t\t\t\tTaiTran1994\t\t\t",
          "content": "I think C should be corrected.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>C costs more</li></ul>",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 757849,
          "date": "Mon 26 Dec 2022 22:03",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "C costs more",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#147",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company needs to retain application log files for a critical application for 10 years. The application team regularly accesses logs from the past month for troubleshooting, but logs older than 1 month are rarely accessed. The application generates more than 10 TB of logs per month.<br>Which storage option meets these requirements MOST cost-effectively?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#147",
          "answers": [
            {
              "choice": "<p>A. Store the logs in Amazon S3. Use AWS Backup to move logs more than 1 month old to S3 Glacier Deep Archive.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Store the logs in Amazon S3. Use S3 Lifecycle policies to move logs more than 1 month old to S3 Glacier Deep Archive.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Store the logs in Amazon CloudWatch Logs. Use AWS Backup to move logs more than 1 month old to S3 Glacier Deep Archive.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Store the logs in Amazon CloudWatch Logs. Use Amazon S3 Lifecycle policies to move logs more than 1 month old to S3 Glacier Deep Archive.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 775442,
          "date": "Sat 14 Jan 2023 14:35",
          "username": "\t\t\t\tMamiololo\t\t\t",
          "content": "B is correct..",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 760654,
          "date": "Thu 29 Dec 2022 07:25",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option B (Store the logs in Amazon S3. Use S3 Lifecycle policies to move logs more than 1-month-old to S3 Glacier Deep Archive) would meet these requirements in the most cost-effective manner.<br><br>This solution would allow the application team to quickly access the logs from the past month for troubleshooting, while also providing a cost-effective storage solution for the logs that are rarely accessed and need to be retained for 10 years.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 748608,
          "date": "Sun 18 Dec 2022 05:33",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option B is most cost effective. Moving logs to Cloudwatch logs may incure additional cost.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 724415,
          "date": "Tue 22 Nov 2022 16:39",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 720000,
          "date": "Wed 16 Nov 2022 22:02",
          "username": "\t\t\t\trjam\t\t\t",
          "content": "Why not AwsBackup? No Glacier Deep is supported by AWS Backup<br><br>https://docs.aws.amazon.com/aws-backup/latest/devguide/s3-backups.html <br>AWS Backup allows you to backup your S3 data stored in the following S3 Storage Classes:<br> S3 Standard<br> S3 Standard - Infrequently Access (IA)<br> S3 One Zone-IA<br> S3 Glacier Instant Retrieval<br> S3 Intelligent-Tiering (S3 INT)<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>AWS BackUp costs something, setting up S3 LCP doesn't.</li></ul>",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 727787,
          "date": "Sat 26 Nov 2022 20:50",
          "username": "\t\t\t\ttdkcumberland\t\t\t",
          "content": "AWS BackUp costs something, setting up S3 LCP doesn't.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 714617,
          "date": "Wed 09 Nov 2022 14:51",
          "username": "\t\t\t\tArielSchivo\t\t\t",
          "content": "S3 + Glacier is the most cost effective.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 714179,
          "date": "Wed 09 Nov 2022 01:59",
          "username": "\t\t\t\tBevemo\t\t\t",
          "content": "D works, archive cloudwatch logs to S3 .... but is an additional service to pay for over B. <br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>CloudWatch logs can't store around 10 TB of data per month I believe so both C and Doptions are ruled out already.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 732178,
          "date": "Thu 01 Dec 2022 03:28",
          "username": "\t\t\t\tAamee\t\t\t",
          "content": "CloudWatch logs can't store around 10 TB of data per month I believe so both C and Doptions are ruled out already.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 710005,
          "date": "Wed 02 Nov 2022 20:01",
          "username": "\t\t\t\tmasetromain\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/80772-exam-aws-certified-solutions-architect-associate-saa-c02/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        }
      ]
    },
    {
      "question_id": "#148",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has a data ingestion workflow that includes the following components:<br>An Amazon Simple Notification Service (Amazon SNS) topic that receives notifications about new data deliveries<br>An AWS Lambda function that processes and stores the data<br>The ingestion workflow occasionally fails because of network connectivity issues. When failure occurs, the corresponding data is not ingested unless the company manually reruns the job.<br>What should a solutions architect do to ensure that all notifications are eventually processed?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#148",
          "answers": [
            {
              "choice": "<p>A. Configure the Lambda function for deployment across multiple Availability Zones.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Modify the Lambda function's configuration to increase the CPU and memory allocations for the function.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Configure the SNS topic's retry strategy to increase both the number of retries and the wait time between retries.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure an Amazon Simple Queue Service (Amazon SQS) queue as the on-failure destination. Modify the Lambda function to process messages in the queue.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 705462,
          "date": "Thu 27 Oct 2022 12:58",
          "username": "\t\t\t\tbunnychip\t\t\t",
          "content": "*ensure that all notifications are eventually processed*",
          "upvote_count": "8",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 810648,
          "date": "Thu 16 Feb 2023 13:42",
          "username": "\t\t\t\tHelp2023\t\t\t",
          "content": "This is why https://docs.aws.amazon.com/sns/latest/dg/sns-message-delivery-retries.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 799588,
          "date": "Mon 06 Feb 2023 10:59",
          "username": "\t\t\t\tCaoMengde09\t\t\t",
          "content": "C is not the right answer since after several retries SNS discard the message which doesn't align with the reqirement. D is the right answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 799584,
          "date": "Mon 06 Feb 2023 10:58",
          "username": "\t\t\t\tCaoMengde09\t\t\t",
          "content": "Best solution to process failed SNS notifications is using sns-dead-letter-queues (SQS Queue for reprocessing)<br>https://docs.aws.amazon.com/sns/latest/dg/sns-dead-letter-queues.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 770490,
          "date": "Mon 09 Jan 2023 15:05",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "To ensure that all notifications are eventually processed, the best solution would be to configure an Amazon Simple Queue Service (SQS) queue as the on-failure destination for the SNS topic. This will allow the notifications to be retried until they are successfully processed. The Lambda function can then be modified to process messages in the queue, ensuring that all notifications are eventually processed. Option D, \\\"Configure an Amazon Simple Queue Service (Amazon SQS) queue as the on-failure destination. Modify the Lambda function to process messages in the queue,\\\" is the correct answer.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 760671,
          "date": "Thu 29 Dec 2022 07:36",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "I choose Option D as the correct answer.<br><br>To ensure that all notifications are eventually processed, the solutions architect can set up an Amazon SQS queue as the on-failure destination for the Amazon SNS topic. This way, when the Lambda function fails due to network connectivity issues, the notification will be sent to the queue instead of being lost. The Lambda function can then be modified to process messages in the queue, ensuring that all notifications are eventually processed.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 757861,
          "date": "Mon 26 Dec 2022 22:26",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "Option Dto ensure that all notifications are eventually processed you need to use SQS.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 748616,
          "date": "Sun 18 Dec 2022 06:11",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option C is right option. <br>SNS does not have any \\\"On Failure\\\" delivery destination. One need to configure dead-letter queue and configure SQS to read from there.So given this option D is incorrect.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I don't think that's right <br>\\\"A dead-letter queue is an Amazon SQS queue that an Amazon SNS subscription can target for messages that can't be delivered to subscribers successfully. Messages that can't be delivered due to client errors or server errors are held in the dead-letter queue for further analysis or reprocessing\\\" from https://docs.aws.amazon.com/sns/latest/dg/sns-dead-letter-queues.html.<br>This is pretty much what is being described in D. <br>Plus C will only retry message processing, and network problems could still prevent the message from being processed, but the question states \\\"ensure that all notifications are eventually processed\\\". So C does not meet the requirements but D does look to do this.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 754151,
          "date": "Fri 23 Dec 2022 12:32",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "I don't think that's right <br>\\\"A dead-letter queue is an Amazon SQS queue that an Amazon SNS subscription can target for messages that can't be delivered to subscribers successfully. Messages that can't be delivered due to client errors or server errors are held in the dead-letter queue for further analysis or reprocessing\\\" from https://docs.aws.amazon.com/sns/latest/dg/sns-dead-letter-queues.html.<br>This is pretty much what is being described in D. <br>Plus C will only retry message processing, and network problems could still prevent the message from being processed, but the question states \\\"ensure that all notifications are eventually processed\\\". So C does not meet the requirements but D does look to do this.",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 747952,
          "date": "Sat 17 Dec 2022 10:39",
          "username": "\t\t\t\tNikaCZ\t\t\t",
          "content": "Is correct.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 747951,
          "date": "Sat 17 Dec 2022 10:39",
          "username": "\t\t\t\tNikaCZ\t\t\t",
          "content": "If you want to ensure that all notifications are eventually processed you need to use SQS.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 734860,
          "date": "Sun 04 Dec 2022 06:29",
          "username": "\t\t\t\tWajif\t\t\t",
          "content": "C isnt specific. Hence D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 724461,
          "date": "Tue 22 Nov 2022 17:23",
          "username": "\t\t\t\tLeGloupier\t\t\t",
          "content": "\\\"on-failure destination\\\" doesn't exist, only dead letter queue exist.<br>that's why I am leaning for C<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Dead letter queue doesnt exist in SNS. They are specifically saying a new queue will be configured for failures from SNS. Hence D</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 734859,
          "date": "Sun 04 Dec 2022 06:29",
          "username": "\t\t\t\tWajif\t\t\t",
          "content": "Dead letter queue doesnt exist in SNS. They are specifically saying a new queue will be configured for failures from SNS. Hence D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 724418,
          "date": "Tue 22 Nov 2022 16:44",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "D is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 721349,
          "date": "Fri 18 Nov 2022 16:10",
          "username": "\t\t\t\tds0321\t\t\t",
          "content": "D is the answer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 714631,
          "date": "Wed 09 Nov 2022 15:04",
          "username": "\t\t\t\tArielSchivo\t\t\t",
          "content": "Option C could work but the max retries attempts is 23 days. After that messages are deleted. And you do not want that to happen! So, Option D. ",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 707658,
          "date": "Sun 30 Oct 2022 07:56",
          "username": "\t\t\t\tSimonPark\t\t\t",
          "content": "imho, D is the answer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 694238,
          "date": "Thu 13 Oct 2022 21:43",
          "username": "\t\t\t\tbrushek\t\t\t",
          "content": "should be C:<br><br>https://docs.aws.amazon.com/sns/latest/dg/sns-message-delivery-retries.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>And should D in this case. In the URL you referred, there is a statement as follows :- \\\"With the exception of HTTP/S, you can't change Amazon SNS-defined delivery policies. Only HTTP/S supports custom policies. See Creating an HTTP/S delivery policy.\\\"Hence you cantcustomise the retry for Lamda and option D is more relevant</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 739987,
          "date": "Fri 09 Dec 2022 11:17",
          "username": "\t\t\t\tRBSK\t\t\t",
          "content": "And should D in this case. In the URL you referred, there is a statement as follows :- \\\"With the exception of HTTP/S, you can't change Amazon SNS-defined delivery policies. Only HTTP/S supports custom policies. See Creating an HTTP/S delivery policy.\\\"Hence you cantcustomise the retry for Lamda and option D is more relevant",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#149",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has a service that produces event data. The company wants to use AWS to process the event data as it is received. The data is written in a specific order that must be maintained throughout processing. The company wants to implement a solution that minimizes operational overhead.<br>How should a solutions architect accomplish this?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#149",
          "answers": [
            {
              "choice": "<p>A. Create an Amazon Simple Queue Service (Amazon SQS) FIFO queue to hold messages. Set up an AWS Lambda function to process messages from the queue.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an Amazon Simple Notification Service (Amazon SNS) topic to deliver notifications containing payloads to process. Configure an AWS Lambda function as a subscriber.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an Amazon Simple Queue Service (Amazon SQS) standard queue to hold messages. Set up an AWS Lambda function to process messages from the queue independently.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an Amazon Simple Notification Service (Amazon SNS) topic to deliver notifications containing payloads to process. Configure an Amazon Simple Queue Service (Amazon SQS) queue as a subscriber.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 825502,
          "date": "Wed 01 Mar 2023 04:41",
          "username": "\t\t\t\tWherecanIstart\t\t\t",
          "content": "Option A is correct...data is processed in the correct order",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 760691,
          "date": "Thu 29 Dec 2022 07:49",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The correct solution is Option A.  Creating an Amazon Simple Queue Service (Amazon SQS) FIFO queue to hold messages and setting up an AWS Lambda function to process messages from the queue will ensure that the event data is processed in the correct order and minimize operational overhead.<br><br>Option B is incorrect because using Amazon Simple Notification Service (Amazon SNS) does not guarantee the order in which messages are delivered.<br><br>Option C is incorrect because using an Amazon SQS standard queue does not guarantee the order in which messages are processed.<br><br>Option D is incorrect because using an Amazon SQS queue as a subscriber to an Amazon SNS topic does not guarantee the order in which messages are processed.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 757862,
          "date": "Mon 26 Dec 2022 22:28",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "Only A is right option here.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 748617,
          "date": "Sun 18 Dec 2022 06:14",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option A is the best option.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 747994,
          "date": "Sat 17 Dec 2022 11:47",
          "username": "\t\t\t\talect096\t\t\t",
          "content": "\\\"The data is written in a specific order that must be maintained throughout processing\\\" --> FIFO",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 747948,
          "date": "Sat 17 Dec 2022 10:37",
          "username": "\t\t\t\tNikaCZ\t\t\t",
          "content": "specific order = FIFO",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 745879,
          "date": "Thu 15 Dec 2022 10:23",
          "username": "\t\t\t\tk1kavi1\t\t\t",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 740199,
          "date": "Fri 09 Dec 2022 15:26",
          "username": "\t\t\t\tdavid76x\t\t\t",
          "content": "Definitely A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 724420,
          "date": "Tue 22 Nov 2022 16:45",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 714635,
          "date": "Wed 09 Nov 2022 15:09",
          "username": "\t\t\t\tArielSchivo\t\t\t",
          "content": "FIFO means order, so Option A. ",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 709052,
          "date": "Tue 01 Nov 2022 11:44",
          "username": "\t\t\t\trjam\t\t\t",
          "content": "Order --- meansFIFO option A",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#150",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is migrating an application from on-premises servers to Amazon EC2 instances. As part of the migration design requirements, a solutions architect must implement infrastructure metric alarms. The company does not need to take action if CPU utilization increases to more than 50% for a short burst of time. However, if the CPU utilization increases to more than 50% and read IOPS on the disk are high at the same time, the company needs to act as soon as possible. The solutions architect also must reduce false alarms.<br>What should the solutions architect do to meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#150",
          "answers": [
            {
              "choice": "<p>A. Create Amazon CloudWatch composite alarms where possible.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create Amazon CloudWatch dashboards to visualize the metrics and react to issues quickly.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create Amazon CloudWatch Synthetics canaries to monitor the application and raise an alarm.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create single Amazon CloudWatch metric alarms with multiple metric thresholds where possible.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 699894,
          "date": "Thu 20 Oct 2022 14:14",
          "username": "\t\t\t\t123jhl0\t\t\t",
          "content": "Composite alarms determine their states by monitoring the states of other alarms. You can **use composite alarms to reduce alarm noise**. For example, you can create a composite alarm where the underlying metric alarms go into ALARM when they meet specific conditions. You then can set up your composite alarm to go into ALARM and send you notifications when the underlying metric alarms go into ALARM by configuring the underlying metric alarms never to take actions. Currently, composite alarms can take the following actions:<br>https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Create_Composite_Alarm.html",
          "upvote_count": "20",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 753039,
          "date": "Thu 22 Dec 2022 07:55",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A, creating Amazon CloudWatch composite alarms, is correct because it allows the solutions architect to create an alarm that is triggered only when both CPU utilization is above 50% and read IOPS on the disk are high at the same time. This meets the requirement to act as soon as possible if both conditions are met, while also reducing the number of false alarms by ensuring that the alarm is triggered only when both conditions are met.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>The incorrect solutions are:<br><br>In contrast, Option B, creating Amazon CloudWatch dashboards, would not directly address the requirement to trigger an alarm when both CPU utilization is high and read IOPS on the disk are high at the same time. Dashboards can be useful for visualizing metric data and identifying trends, but they do not have the capability to trigger alarms based on multiple metric thresholds.<br><br>Option C, using Amazon CloudWatch Synthetics canaries, may not be the best choice for this scenario, as canaries are used for synthetic testing rather than for monitoring live traffic. Canaries can be useful for monitoring the availability and performance of an application, but they may not be the most effective way to monitor the specific metric thresholds and conditions described in this scenario.</li><li>Option D, creating single Amazon CloudWatch metric alarms with multiple metric thresholds, would not allow the solutions architect to create an alarm that is triggered only when both CPU utilization and read IOPS on the disk are high at the same time. Instead, the alarm would be triggered whenever any of the specified metric thresholds are exceeded, which may result in a higher number of false alarms.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 753041,
          "date": "Thu 22 Dec 2022 07:56",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The incorrect solutions are:<br><br>In contrast, Option B, creating Amazon CloudWatch dashboards, would not directly address the requirement to trigger an alarm when both CPU utilization is high and read IOPS on the disk are high at the same time. Dashboards can be useful for visualizing metric data and identifying trends, but they do not have the capability to trigger alarms based on multiple metric thresholds.<br><br>Option C, using Amazon CloudWatch Synthetics canaries, may not be the best choice for this scenario, as canaries are used for synthetic testing rather than for monitoring live traffic. Canaries can be useful for monitoring the availability and performance of an application, but they may not be the most effective way to monitor the specific metric thresholds and conditions described in this scenario.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option D, creating single Amazon CloudWatch metric alarms with multiple metric thresholds, would not allow the solutions architect to create an alarm that is triggered only when both CPU utilization and read IOPS on the disk are high at the same time. Instead, the alarm would be triggered whenever any of the specified metric thresholds are exceeded, which may result in a higher number of false alarms.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 753042,
          "date": "Thu 22 Dec 2022 07:56",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option D, creating single Amazon CloudWatch metric alarms with multiple metric thresholds, would not allow the solutions architect to create an alarm that is triggered only when both CPU utilization and read IOPS on the disk are high at the same time. Instead, the alarm would be triggered whenever any of the specified metric thresholds are exceeded, which may result in a higher number of false alarms.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 750485,
          "date": "Tue 20 Dec 2022 05:12",
          "username": "\t\t\t\tBENICE\t\t\t",
          "content": "A is correct answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 748382,
          "date": "Sat 17 Dec 2022 21:12",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 743481,
          "date": "Tue 13 Dec 2022 02:32",
          "username": "\t\t\t\tQjb8m9h\t\t\t",
          "content": "The AWS::CloudWatch::CompositeAlarm type creates or updates a composite alarm. When you create a composite alarm, you specify a rule expression for the alarm that takes into account the alarm states of other alarms that you have created. The composite alarm goes into ALARM state only if all conditions of the rule are met.<br><br>The alarms specified in a composite alarm's rule expression can include metric alarms and other composite alarms.Using composite alarms can reduce alarm noise.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 724421,
          "date": "Tue 22 Nov 2022 16:46",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#151",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company wants to migrate its on-premises data center to AWS. According to the company's compliance requirements, the company can use only the ap-northeast-3 Region. Company administrators are not permitted to connect VPCs to the internet.<br>Which solutions will meet these requirements? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: AC</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#151",
          "answers": [
            {
              "choice": "<p>A. Use AWS Control Tower to implement data residency guardrails to deny internet access and deny access to all AWS Regions except ap-northeast-3.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use rules in AWS WAF to prevent internet access. Deny access to all AWS Regions except ap-northeast-3 in the AWS account settings.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use AWS Organizations to configure service control policies (SCPS) that prevent VPCs from gaining internet access. Deny access to all AWS Regions except ap-northeast-3.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an outbound rule for the network ACL in each VPC to deny all traffic from 0.0.0.0/0. Create an IAM policy for each user to prevent the use of any AWS Region other than ap-northeast-3.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Use AWS Config to activate managed rules to detect and alert for internet gateways and to detect and alert for new resources deployed outside of ap-northeast-3.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 704889,
          "date": "Wed 26 Oct 2022 19:24",
          "username": "\t\t\t\tSix_Fingered_Jose\t\t\t",
          "content": "agree with A and C<br>https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples_vpc.html#example_vpc_2",
          "upvote_count": "11",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 718680,
          "date": "Tue 15 Nov 2022 12:01",
          "username": "\t\t\t\trjam\t\t\t",
          "content": "https://aws.amazon.com/blogs/aws/new-for-aws-control-tower-region-deny-and-guardrails-to-help-you-meet-data-residency-requirements/<br>*Disallow internet access for an Amazon VPC instance managed by a customer<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A and C</li><li>*You can use data-residency guardrails to control resources in any AWS Region.</li></ul>",
          "upvote_count": "7",
          "selected_answers": ""
        },
        {
          "id": 718681,
          "date": "Tue 15 Nov 2022 12:01",
          "username": "\t\t\t\trjam\t\t\t",
          "content": "Option A and C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 718682,
          "date": "Tue 15 Nov 2022 12:02",
          "username": "\t\t\t\trjam\t\t\t",
          "content": "*You can use data-residency guardrails to control resources in any AWS Region.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 848808,
          "date": "Fri 24 Mar 2023 01:26",
          "username": "\t\t\t\tWherecanIstart\t\t\t",
          "content": "AWS Control tower is not available in ap-northeast-3!<br><br>https://www.aws-services.info/controltower.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: CE"
        },
        {
          "id": 848801,
          "date": "Fri 24 Mar 2023 01:20",
          "username": "\t\t\t\twarioverde\t\t\t",
          "content": "What's wrong with B?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 839949,
          "date": "Wed 15 Mar 2023 15:08",
          "username": "\t\t\t\tAlessandraSAA\t\t\t",
          "content": "A - CANNOT BE!!! AWS Control Tower is not available in ap-northeast-3! Check your consolle.",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: CE"
        },
        {
          "id": 820430,
          "date": "Fri 24 Feb 2023 12:46",
          "username": "\t\t\t\tmoaaz86\t\t\t",
          "content": "From ChatGPT :) <br><br>Control Tower: Can<br>Yes, AWS Control Tower can implement data residency guardrails to deny internet access and restrict access to AWS Regions except for one.<br>To restrict access to AWS regions, you can create a guardrail using AWS Organizations to deny access to all AWS regions except for the one that you want to allow. This can be done by creating an organizational policy that restricts access to specific AWS services and resources based on region.<br><br>Config: Can(not).<br>Yes, AWS Config can help you enforce restrictions on internet access and control access to specific AWS Regions using AWS Config Rules.<br>It's worth noting that AWS Config is a monitoring service that provides continuous assessment of your AWS resources against desired configurations. While AWS Config can alert you when a configuration change occurs, it cannot directly restrict access to resources or enforce specific policies. For that, you may need to use other AWS services such as AWS Identity and Access Management (IAM), AWS Firewall Manager, or AWS Organizations.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 804157,
          "date": "Fri 10 Feb 2023 10:11",
          "username": "\t\t\t\tKZM\t\t\t",
          "content": "Option A uses AWS Control Tower to implement data residency guardrails, but it does not prevent internet access by itself. It only denies access to all AWS Regions except ap-northeast-3. The requirement states that administrators are not permitted to connect VPCs to the internet, so Option A does not meet this requirement.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 784334,
          "date": "Sun 22 Jan 2023 14:25",
          "username": "\t\t\t\tbullrem\t\t\t",
          "content": "Option A is not a valid solution because AWS Control Tower is a service that helps customers set up and govern a new, secure, multi-account AWS environment based on best practices. It does not provide specific guardrails that would prevent internet access or restrict access to a specific region.<br>Option C is a valid solution because AWS Organizations can be used to configure service control policies (SCPs) that can prevent VPCs from gaining internet access, and this can be done by denying access to all AWS Regions except ap-northeast-3.<br>Option E is also a valid solution because AWS Config can be used to activate managed rules to detect and alert for internet gateways and to detect and alert for new resources deployed outside of ap-northeast-3. This can help to ensure compliance with the company's requirements to prevent internet access and to limit access to a specific region.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>The most interesting guardrail is probably the one denying access to AWS based on the requested AWS Region. I choose it from the list and find that it is different from the other guardrails because it affects all Organizational Units (OUs) and cannot be activated here but must be activated in the landing zone settings.<br><br>https://aws.amazon.com/blogs/aws/new-for-aws-control-tower-region-deny-and-guardrails-to-help-you-meet-data-residency-requirements/#:~:text=AWS%20Control%20Tower%20also%20offers,the%20creation%20of%20internet%20gateway</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: CE"
        },
        {
          "id": 795770,
          "date": "Thu 02 Feb 2023 06:32",
          "username": "\t\t\t\tLuckyAro\t\t\t",
          "content": "The most interesting guardrail is probably the one denying access to AWS based on the requested AWS Region. I choose it from the list and find that it is different from the other guardrails because it affects all Organizational Units (OUs) and cannot be activated here but must be activated in the landing zone settings.<br><br>https://aws.amazon.com/blogs/aws/new-for-aws-control-tower-region-deny-and-guardrails-to-help-you-meet-data-residency-requirements/#:~:text=AWS%20Control%20Tower%20also%20offers,the%20creation%20of%20internet%20gateway",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 766902,
          "date": "Thu 05 Jan 2023 19:01",
          "username": "\t\t\t\tmhmt4438\t\t\t",
          "content": "C and E<br><br>To meet the requirements of not allowing VPCs to connect to the internet and limiting the AWS Region to ap-northeast-3, you can use the following solutions:<br><br>C: Use AWS Organizations to configure service control policies (SCPs) that prevent VPCs from gaining internet access. Deny access to all AWS Regions except ap-northeast-3. This will ensure that VPCs cannot access the internet and can only be created in the ap-northeast-3 Region.<br><br>E: Use AWS Config to activate managed rules to detect and alert for internet gateways and to detect and alert for new resources deployed outside of ap-northeast-3. This will allow you to monitor for any attempts to connect VPCs to the internet or to deploy resources outside of the ap-northeast-3 Region, and alert you if any such attempts are detected.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Not E.  \\\"Company administrators are not permitted...\\\", an alert detect a connection an send an alert, not prevent the connection</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 780423,
          "date": "Wed 18 Jan 2023 21:54",
          "username": "\t\t\t\tegmiranda\t\t\t",
          "content": "Not E.  \\\"Company administrators are not permitted...\\\", an alert detect a connection an send an alert, not prevent the connection",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 765514,
          "date": "Wed 04 Jan 2023 11:56",
          "username": "\t\t\t\taba2s\t\t\t",
          "content": "You can now use AWS Control Tower guardrails to deny services and operations for AWS Region(s) of your choice in your AWS Control Tower environments. The Region deny capabilities complement existing AWS Control Tower Region selection and Region deselection features, providing you with the capabilities to address compliance and regulatory requirements while improving cost efficiency of expanding into additional Regions.<br><br>Along with the Region Deny feature, a set of data residency guardrails are released to help customers with data residency requirements. You can use these guardrails to choose the AWS Region that is in your desired location and have complete control and ownership over the region in which your data is physically located, making it easy to meet regional compliance and data residency requirements. https://controltower.aws-management.tools/security/restrict_regions/<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I mean A and C not D.  Please allow editing post after submitted</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 765516,
          "date": "Wed 04 Jan 2023 11:58",
          "username": "\t\t\t\taba2s\t\t\t",
          "content": "I mean A and C not D.  Please allow editing post after submitted",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 748385,
          "date": "Sat 17 Dec 2022 21:17",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "A and C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 724437,
          "date": "Tue 22 Nov 2022 17:02",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "A and C",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#152",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company uses a three-tier web application to provide training to new employees. The application is accessed for only 12 hours every day. The company is using an Amazon RDS for MySQL DB instance to store information and wants to minimize costs.<br>What should a solutions architect do to meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#152",
          "answers": [
            {
              "choice": "<p>A. Configure an IAM policy for AWS Systems Manager Session Manager. Create an IAM role for the policy. Update the trust relationship of the role. Set up automatic start and stop for the DB instance.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an Amazon ElastiCache for Redis cache cluster that gives users the ability to access the data from the cache when the DB instance is stopped. Invalidate the cache after the DB instance is started.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Launch an Amazon EC2 instance. Create an IAM role that grants access to Amazon RDS. Attach the role to the EC2 instance. Configure a cron job to start and stop the EC2 instance on the desired schedule.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create AWS Lambda functions to start and stop the DB instance. Create Amazon EventBridge (Amazon CloudWatch Events) scheduled rules to invoke the Lambda functions. Configure the Lambda functions as event targets for the rules.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 707852,
          "date": "Sun 30 Oct 2022 15:18",
          "username": "\t\t\t\tstudy_aws1\t\t\t",
          "content": "https://aws.amazon.com/blogs/database/schedule-amazon-rds-stop-and-start-using-aws-lambda/<br><br>It is option D.  Option A could have been applicable had it been AWS Systems Manager State Manager & not AWS Systems Manager Session Manager",
          "upvote_count": "19",
          "selected_answers": ""
        },
        {
          "id": 714646,
          "date": "Wed 09 Nov 2022 15:22",
          "username": "\t\t\t\tArielSchivo\t\t\t",
          "content": "Option D is the one. Option A could be as well if it referred to State Manager instead of Session Manager.",
          "upvote_count": "5",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 842696,
          "date": "Sat 18 Mar 2023 12:19",
          "username": "\t\t\t\ttest_devops_aws\t\t\t",
          "content": "https://docs.aws.amazon.com/systems-manager-automation-runbooks/latest/userguide/automation-ref-rds.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 765534,
          "date": "Wed 04 Jan 2023 12:11",
          "username": "\t\t\t\taba2s\t\t\t",
          "content": "AWS Lambda and Amazon EventBridge that allows you to schedule a Lambda function to stop and start the idle databases with specific tags to save on compute costs. https://aws.amazon.com/blogs/database/schedule-amazon-rds-stop-and-start-using-aws-lambda/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 763386,
          "date": "Mon 02 Jan 2023 00:12",
          "username": "\t\t\t\tZerotn3\t\t\t",
          "content": "The correct answer is D.  Creating AWS Lambda functions to start and stop the DB instance and using Amazon EventBridge (Amazon CloudWatch Events) scheduled rules to invoke the Lambda functions is the most cost-effective way to meet the requirements. The Lambda functions can be configured as event targets for the scheduled rules, which will allow the DB instance to be started and stopped on the desired schedule.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 750945,
          "date": "Tue 20 Dec 2022 14:42",
          "username": "\t\t\t\tjupa\t\t\t",
          "content": "Its D.  confirmed via others exam test pages",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 748399,
          "date": "Sat 17 Dec 2022 21:36",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option D is the best option. Session Manager access can not be used to start and stop DB instances.It is used for the Brower based SSH access to instances.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 709237,
          "date": "Tue 01 Nov 2022 16:10",
          "username": "\t\t\t\trob74\t\t\t",
          "content": "I think A or D but D is cheaper (mimimize costs) because you pay Lambda only if you use it.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 709235,
          "date": "Tue 01 Nov 2022 16:10",
          "username": "\t\t\t\trob74\t\t\t",
          "content": "I think A or D but D is cheaper (mimimize costs) because you pay Lambda only if you use it.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 707744,
          "date": "Sun 30 Oct 2022 10:42",
          "username": "\t\t\t\tSimonPark\t\t\t",
          "content": "voted d",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 706521,
          "date": "Fri 28 Oct 2022 16:06",
          "username": "\t\t\t\tKien048\t\t\t",
          "content": "Vote D",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 704891,
          "date": "Wed 26 Oct 2022 19:26",
          "username": "\t\t\t\tSix_Fingered_Jose\t\t\t",
          "content": "agreed with A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 699963,
          "date": "Thu 20 Oct 2022 15:17",
          "username": "\t\t\t\t123jhl0\t\t\t",
          "content": "A is true for sure. \\\"Schedule Amazon RDS stop and start using AWS Systems Manager\\\" Steps in the documentation:<br> 1. Configure an AWS Identity and Access Management (IAM) policy for State Manager.<br> 2. Create an IAM role for the new policy.<br> 3. Update the trust relationship of the role so Systems Manager can use it.<br> 4. Set up the automatic stop with State Manager.<br> 5. Set up the automatic start with State Manager.<br>https://aws.amazon.com/blogs/database/schedule-amazon-rds-stop-and-start-using-aws-systems-manager/<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A refers to Session Manager, not State Manager as you pointed, so it is wrong. Option D is valid.</li><li>Agree A, free to use state manager within limits, and don't need to code or manage lambda.</li><li>Look like State manager and Session manager use for difference purpose even both in same dashboard console.</li><li>And ofcause, D is working, so if A also right, the question is wrong.</li></ul>",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 714649,
          "date": "Wed 09 Nov 2022 15:23",
          "username": "\t\t\t\tArielSchivo\t\t\t",
          "content": "Option A refers to Session Manager, not State Manager as you pointed, so it is wrong. Option D is valid.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 714217,
          "date": "Wed 09 Nov 2022 02:51",
          "username": "\t\t\t\tBevemo\t\t\t",
          "content": "Agree A, free to use state manager within limits, and don't need to code or manage lambda.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 706517,
          "date": "Fri 28 Oct 2022 16:04",
          "username": "\t\t\t\tKien048\t\t\t",
          "content": "Look like State manager and Session manager use for difference purpose even both in same dashboard console.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 706519,
          "date": "Fri 28 Oct 2022 16:05",
          "username": "\t\t\t\tKien048\t\t\t",
          "content": "And ofcause, D is working, so if A also right, the question is wrong.",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#153",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company sells ringtones created from clips of popular songs. The files containing the ringtones are stored in Amazon S3 Standard and are at least 128 KB in size. The company has millions of files, but downloads are infrequent for ringtones older than 90 days. The company needs to save money on storage while keeping the most accessed files readily available for its users.<br>Which action should the company take to meet these requirements MOST cost-effectively?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#153",
          "answers": [
            {
              "choice": "<p>A. Configure S3 Standard-Infrequent Access (S3 Standard-IA) storage for the initial storage tier of the objects.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Move the files to S3 Intelligent-Tiering and configure it to move objects to a less expensive storage tier after 90 days.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Configure S3 inventory to manage objects and move them to S3 Standard-Infrequent Access (S3 Standard-1A) after 90 days.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Implement an S3 Lifecycle policy that moves the objects from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-1A) after 90 days.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 718790,
          "date": "Tue 15 Nov 2022 14:52",
          "username": "\t\t\t\trjam\t\t\t",
          "content": "Answer D<br>Why Optoin D ?<br>The Question talks about downloads are infrequent older than 90 days which means files less than 90 days are accessed frequently. Standard-Infrequent Access (S3 Standard-IA) needs a minimum 30 days if accessed before, it costs more.<br>So to access the files frequently you need aS3 Standard .After 90 days you can move it toStandard-Infrequent Access (S3 Standard-IA) as its going to be less frequently accessed",
          "upvote_count": "22",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 724877,
          "date": "Wed 23 Nov 2022 05:12",
          "username": "\t\t\t\tzeronine75\t\t\t",
          "content": "B/D seems possible answer. But, I'll go with \\\"B\\\". <br>In the following table, S3 Intelligent-Tiering seems not so expansive than S3 Standard.<br>https://aws.amazon.com/s3/pricing/?nc1=h_ls<br>And, in the question \\\"128KB\\\" size is talking about S3 Intelligent-Tiering stuff.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>S3 Intelligent tiering is used when the access frequency is not known. I think 128KB is a deflector.</li><li>also, there are probably several ringtones which aren't popular/used. Why keep them in S3 standard? The company would save money if s3 intelligent-tiering moves the unpopular ringtones to a more cost-effective tier than s3 standard.</li><li>This link also has me going with B.  Specifying 128 KB in size is not a coincidence. https://aws.amazon.com/s3/storage-classes/intelligent-tiering/</li><li>because of tha link it is D.  <br>There are no retrieval charges in S3 Intelligent-Tiering. S3 Intelligent-Tiering has no minimum eligible object size, but objects smaller than 128 KB are not eligible for auto tiering. These smaller objects may be stored, but they'll always be charged at the Frequent Access tier</li><li>oh sorry it states objects are bigger than 128 KB.  B is correct</li></ul>",
          "upvote_count": "8",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 734856,
          "date": "Sun 04 Dec 2022 06:22",
          "username": "\t\t\t\tWajif\t\t\t",
          "content": "S3 Intelligent tiering is used when the access frequency is not known. I think 128KB is a deflector.",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 761422,
          "date": "Thu 29 Dec 2022 20:42",
          "username": "\t\t\t\tFNJ1111\t\t\t",
          "content": "also, there are probably several ringtones which aren't popular/used. Why keep them in S3 standard? The company would save money if s3 intelligent-tiering moves the unpopular ringtones to a more cost-effective tier than s3 standard.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 725573,
          "date": "Thu 24 Nov 2022 06:29",
          "username": "\t\t\t\tWilson_S\t\t\t",
          "content": "This link also has me going with B.  Specifying 128 KB in size is not a coincidence. https://aws.amazon.com/s3/storage-classes/intelligent-tiering/<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>because of tha link it is D.  <br>There are no retrieval charges in S3 Intelligent-Tiering. S3 Intelligent-Tiering has no minimum eligible object size, but objects smaller than 128 KB are not eligible for auto tiering. These smaller objects may be stored, but they'll always be charged at the Frequent Access tier</li><li>oh sorry it states objects are bigger than 128 KB.  B is correct</li></ul>",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 737558,
          "date": "Wed 07 Dec 2022 09:06",
          "username": "\t\t\t\tjavitech83\t\t\t",
          "content": "because of tha link it is D.  <br>There are no retrieval charges in S3 Intelligent-Tiering. S3 Intelligent-Tiering has no minimum eligible object size, but objects smaller than 128 KB are not eligible for auto tiering. These smaller objects may be stored, but they'll always be charged at the Frequent Access tier<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>oh sorry it states objects are bigger than 128 KB.  B is correct</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 737562,
          "date": "Wed 07 Dec 2022 09:08",
          "username": "\t\t\t\tjavitech83\t\t\t",
          "content": "oh sorry it states objects are bigger than 128 KB.  B is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 825508,
          "date": "Wed 01 Mar 2023 04:53",
          "username": "\t\t\t\tKZM\t\t\t",
          "content": "S3 Intelligent-Tiering is designed for data with unknown or changing access patterns and automatically moves data between two access tiers based on access frequency, while S3 Standard-IA is designed for infrequently accessed data that still requires low latency access times when accessed.<br>In this scenario, already mentioned that \\\"the files are infrequent for ringtones older than 90 days and keeping the most access files readily available for the users\\\". So, it is sure that S3-AI.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 810259,
          "date": "Thu 16 Feb 2023 05:46",
          "username": "\t\t\t\tAlmeroSenior\t\t\t",
          "content": "Requirement is > The company needs to save money on storage while keeping the most accessed files readily available for its user . ( So after 90 days , they can wait for access ) .<br><br>Looking at AI by default it will auto move between > Frequent Access > Infrequent Access ><br>Archive Instant Access with an OPTIONAL param to park after 90 daysto ><br><br>Archive Access  S3 Intelligent-Tiering provides you with the option to activate the Archive Access tier for data that can be accessed asynchronously. After activation, the Archive Access tier automatically archives objects that have not been accessed for a minimum of 90 consecutive days.<br><br>So B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 808520,
          "date": "Tue 14 Feb 2023 16:31",
          "username": "\t\t\t\tJoxtat\t\t\t",
          "content": "To manage your objects so that they are stored cost effectively throughout their lifecycle, configure their Amazon S3 Lifecycle. An S3 Lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects. <br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 806390,
          "date": "Sun 12 Feb 2023 14:54",
          "username": "\t\t\t\tYelizaveta\t\t\t",
          "content": "Intelligent Tiering: Monitoring and Automation, All Storage / Month (Objects > 128 KB) $0.0025 per 1,000 objects",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 804194,
          "date": "Fri 10 Feb 2023 10:51",
          "username": "\t\t\t\tKZM\t\t\t",
          "content": "I think it is D.  <br>S3 Lifecycle policy to move the files to S3 Standard-IA after 90 days is more cost-effected.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 801674,
          "date": "Wed 08 Feb 2023 06:09",
          "username": "\t\t\t\tProfXsamson\t\t\t",
          "content": "Keeping most accessed file readily available.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 790061,
          "date": "Sat 28 Jan 2023 00:21",
          "username": "\t\t\t\tegmiranda\t\t\t",
          "content": "I think that the cost of transition from the Intelligent to the Standard infrequent should be considered. In option D, going from standard to standard infrequent is free. In option B, the transfer of the files after 90 days has a cost. The question asks for most-cost effectly, I think it is D",
          "upvote_count": "5",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 790060,
          "date": "Sat 28 Jan 2023 00:21",
          "username": "\t\t\t\tegmiranda\t\t\t",
          "content": "I think that the cost of transition from the Intelligent to the Standard infrequent should be considered. In option D, going from standard to standard infrequent is free. In option B, the transfer of the files after 90 days has a cost. The question asks for most-cost effectively, I think it is D. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 770589,
          "date": "Mon 09 Jan 2023 16:39",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "Option D suggests implementing an Amazon S3 Lifecycle policy that moves objects from the S3 Standard storage class to the S3 Standard-Infrequent Access (S3 Standard-IA) storage class after 90 days. This would allow the company to save money on storage costs while keeping the most accessed files readily available for its users.<br><br>S3 Standard-IA is a storage class that is designed for objects that are accessed less frequently, but still require rapid access when needed. It is generally less expensive than S3 Standard, but has higher retrieval fees. By implementing an S3 Lifecycle policy to move objects to S3 Standard-IA after 90 days, the company would be able to take advantage of the lower storage costs for less frequently accessed objects while still being able to access the files quickly when needed.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 770569,
          "date": "Mon 09 Jan 2023 16:27",
          "username": "\t\t\t\tlfrad\t\t\t",
          "content": "If the ringtones are accessed from the Archive Instant Access or Infrequent Access through Intelligent-Tiering, they will be put back on the Frequent Access tier. <br>Yet we know these ringtones, while being accessed sometime, do not need to move up again as it will be a very rare access. Therefore D preserving their status as Infrequent Access will prevent paying 90 days of Frequent Access rate for a ringtone accessed once every 6 months.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 765533,
          "date": "Wed 04 Jan 2023 12:11",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "I you compare costs for a file that is infrequently used, it's very clear B is the correct answer:<br>S3 Intelligent-Tiering<br>---------------------------------<br>0 --------------------> 30 ---------------------------> 90<br>S3 StandardInfrequent Access Archive Instant Access tier<br>$0.023 $0.0125$0.004<br><br>LifeCycle<br>--------------<br>0 ---------------------------------------------------> 90<br>S3 StandardS3 Standard - Infrequent Access<br>$0.023 $0.0125<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Try again<br><br>S3 Intelligent-Tiering<br><br>0 ----- &gt; 30-----&gt; 90<br>S3 StdS3 IA S3 Arch IA<br>$0.023 $0.0125$0.004<br><br>LifeCycle<br>0 ----------------&gt; 90<br>S3 StdS3 IA<br>$0.023 $0.0125</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 766329,
          "date": "Thu 05 Jan 2023 08:36",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "Try again<br><br>S3 Intelligent-Tiering<br><br>0 ----- > 30-----> 90<br>S3 StdS3 IA S3 Arch IA<br>$0.023 $0.0125$0.004<br><br>LifeCycle<br>0 ----------------> 90<br>S3 StdS3 IA<br>$0.023 $0.0125",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 765026,
          "date": "Tue 03 Jan 2023 21:39",
          "username": "\t\t\t\tdan80\t\t\t",
          "content": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html , both S3-Standard and S3-IA provide millisecond access.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 756012,
          "date": "Sun 25 Dec 2022 22:13",
          "username": "\t\t\t\tMegaMax\t\t\t",
          "content": "D S3 IA minimum size 128kb",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 755799,
          "date": "Sun 25 Dec 2022 16:50",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "its D as objects larger then 128 kb,auto tiering,here are no retrieval charges in S3 Intelligent-Tiering. S3 Intelligent-Tiering has no minimum eligible object size, but objects smaller than 128 KB are not eligible for auto tiering. These smaller objects may be stored, but they'll always be charged at the Frequent Access tier rates and don't incur the monitoring and automation charge.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 752582,
          "date": "Wed 21 Dec 2022 17:55",
          "username": "\t\t\t\tduriselvan\t\t\t",
          "content": "Ans -D : -question itself infrequent access",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#154",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company needs to save the results from a medical trial to an Amazon S3 repository. The repository must allow a few scientists to add new files and must restrict all other users to read-only access. No users can have the ability to modify or delete any files in the repository. The company must keep every file in the repository for a minimum of 1 year after its creation date.<br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#154",
          "answers": [
            {
              "choice": "<p>A. Use S3 Object Lock in governance mode with a legal hold of 1 year.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use S3 Object Lock in compliance mode with a retention period of 365 days.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use an IAM role to restrict all users from deleting or changing objects in the S3 bucket. Use an S3 bucket policy to only allow the IAM role.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure the S3 bucket to invoke an AWS Lambda function every time an object is added. Configure the function to track the hash of the saved object so that modified objects can be marked accordingly.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 718491,
          "date": "Tue 15 Nov 2022 05:13",
          "username": "\t\t\t\tQjb8m9h\t\t\t",
          "content": "Answer : B<br>Reason: Compliance Mode. The key difference between Compliance Mode and Governance Mode is that there are NO users that can override the retention periods set or delete an object, and that also includes your AWS root account which has the highest privileges.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>How about:The repository must allow a few scientists to add new files</li><li>Adding is not the same as changing :)</li></ul>",
          "upvote_count": "14",
          "selected_answers": ""
        },
        {
          "id": 763397,
          "date": "Mon 02 Jan 2023 00:55",
          "username": "\t\t\t\tZerotn3\t\t\t",
          "content": "How about:The repository must allow a few scientists to add new files<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Adding is not the same as changing :)</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 766333,
          "date": "Thu 05 Jan 2023 08:40",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "Adding is not the same as changing :)",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 791465,
          "date": "Sun 29 Jan 2023 09:01",
          "username": "\t\t\t\tProfXsamson\t\t\t",
          "content": "B.  Compliance mode helps ensure that an object version can't be overwritten or deleted for the duration of the retention period.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 765595,
          "date": "Wed 04 Jan 2023 13:06",
          "username": "\t\t\t\taba2s\t\t\t",
          "content": "users can have the ability to modify or delete any files in the repository ==> Compliance Mode<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>users cannot have the ability to modify or delete any files in the repository ==&gt; Compliance Mode</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 769501,
          "date": "Sun 08 Jan 2023 15:14",
          "username": "\t\t\t\taba2s\t\t\t",
          "content": "users cannot have the ability to modify or delete any files in the repository ==> Compliance Mode",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 763391,
          "date": "Mon 02 Jan 2023 00:29",
          "username": "\t\t\t\tZerotn3\t\t\t",
          "content": "B would also meet the requirement to keep every file in the repository for at least 1 year after its creation date, as you can specify a retention period of 365 days. However, it would not meet the requirement to restrict all users except a few scientists to read-only access. S3 Object Lock in compliance mode only allows you to specify retention periods and does not have any options for controlling access to objects in the bucket.<br><br>To meet all the requirements, you should use S3 Object Lock in governance mode and use IAM policies to control access to the objects in the bucket. This would allow you to specify a legal hold with a retention period of at least 1 year and to restrict all users except a few scientists to read-only access.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 755800,
          "date": "Sun 25 Dec 2022 16:54",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "No users can have the ability to modify or delete any files in the repository. hence it must be compliance mode.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 751465,
          "date": "Tue 20 Dec 2022 21:58",
          "username": "\t\t\t\tlazyyoung\t\t\t",
          "content": "Answer is B<br>Compliance: <br>- Object versions can't be overwritten or deleted by any user, including the root user <br>- Objects retention modes can't be changed, and retention periods can't be shortened <br><br>Governance: <br>- Most users can't overwrite or delete an object version or alter its lock settings <br>- Some users have special permissions to change the retention or delete the object",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 748409,
          "date": "Sat 17 Dec 2022 21:53",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "B is best answer but I feel none of the answers covers the requirement for only few users(scientiest) are able to upload(create) the file in the bucket and all other users has Read only access.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 747883,
          "date": "Sat 17 Dec 2022 09:31",
          "username": "\t\t\t\tSteveD15\t\t\t",
          "content": "It is B per \\\"No users can have the ability to modify or delete any files in the repository. \\\". Compliance mode supports that requirement whereas Governance mode does not as defined via https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 726038,
          "date": "Thu 24 Nov 2022 17:25",
          "username": "\t\t\t\tCizzla7049\t\t\t",
          "content": "ANSWER IS DEFINITELY A<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Why is it not B?</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 766334,
          "date": "Thu 05 Jan 2023 08:42",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "Why is it not B?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 724446,
          "date": "Tue 22 Nov 2022 17:07",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "B i think. im not sure..thougts?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 715462,
          "date": "Thu 10 Nov 2022 19:31",
          "username": "\t\t\t\tmabotega\t\t\t",
          "content": "https://cloudacademy.com/course/using-amazon-s3-bucket-properties-management-features-maintain-data/object-lock/#:~:text=be%20deleted%20again.-,Compliance%20Mode.,which%20has%20the%20highest%20privileges.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 714659,
          "date": "Wed 09 Nov 2022 15:35",
          "username": "\t\t\t\tArielSchivo\t\t\t",
          "content": "\\\"No users can have the ability to modify or delete any files in the repository\\\" = Compliance mode.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 710728,
          "date": "Thu 03 Nov 2022 19:43",
          "username": "\t\t\t\tUSalo\t\t\t",
          "content": "B.  Due to compliance",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 710278,
          "date": "Thu 03 Nov 2022 06:58",
          "username": "\t\t\t\tnikerlas\t\t\t",
          "content": "A is Correct <br><br>\\\"In governance mode, users can't overwrite or delete an object version or alter its lock settings unless they have special permissions. With governance mode, you protect objects against being deleted by most users, but you can still grant some users permission to alter the retention settings or delete the object if necessary.\\\"<br><br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>if you have very specific permissions, including s3:BypassGovernanceMode, s3:GetObjectLockConfiguration, s3:GetObjectRetention, then a user will still be able to delete an object version within the retention period or change any retention settings set on the bucket.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 724110,
          "date": "Tue 22 Nov 2022 06:34",
          "username": "\t\t\t\tBobbybash\t\t\t",
          "content": "if you have very specific permissions, including s3:BypassGovernanceMode, s3:GetObjectLockConfiguration, s3:GetObjectRetention, then a user will still be able to delete an object version within the retention period or change any retention settings set on the bucket.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 705571,
          "date": "Thu 27 Oct 2022 15:10",
          "username": "\t\t\t\tbunnychip\t\t\t",
          "content": "'No users\\\" can have the ability to modify or delete any files in the repository",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 704897,
          "date": "Wed 26 Oct 2022 19:33",
          "username": "\t\t\t\tSix_Fingered_Jose\t\t\t",
          "content": "Answer should be A because a few scientist must be able to edit the file<br>> In governance mode, users can't overwrite or delete an object version or alter its lock settings unless they have special permissions.<br><br>It cant be B because in compliance mode, absolutely nobody can touch the file during its period<br>> In compliance mode, a protected object version can't be overwritten or deleted by any user, including the root user in your AWS account<br><br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html#object-lock-retention-modes<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>actually i read the question again<br>&gt; No users can have the ability to modify or delete any files in the repository.<br><br>answer should be B ignore my comment</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 704898,
          "date": "Wed 26 Oct 2022 19:34",
          "username": "\t\t\t\tSix_Fingered_Jose\t\t\t",
          "content": "actually i read the question again<br>> No users can have the ability to modify or delete any files in the repository.<br><br>answer should be B ignore my comment",
          "upvote_count": "9",
          "selected_answers": ""
        },
        {
          "id": 703447,
          "date": "Tue 25 Oct 2022 02:29",
          "username": "\t\t\t\tdave9994\t\t\t",
          "content": "Compliance mode is more restrictive : https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#155",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A large media company hosts a web application on AWS. The company wants to start caching confidential media files so that users around the world will have reliable access to the files. The content is stored in Amazon S3 buckets. The company must deliver the content quickly, regardless of where the requests originate geographically.<br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#155",
          "answers": [
            {
              "choice": "<p>A. Use AWS DataSync to connect the S3 buckets to the web application.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Deploy AWS Global Accelerator to connect the S3 buckets to the web application.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Deploy Amazon CloudFront to connect the S3 buckets to CloudFront edge servers.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use Amazon Simple Queue Service (Amazon SQS) to connect the S3 buckets to the web application.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 709184,
          "date": "Tue 01 Nov 2022 14:49",
          "username": "\t\t\t\trjam\t\t\t",
          "content": "key :caching<br>Option C",
          "upvote_count": "8",
          "selected_answers": ""
        },
        {
          "id": 847305,
          "date": "Wed 22 Mar 2023 17:54",
          "username": "\t\t\t\twarioverde\t\t\t",
          "content": "As far as I understand, Global Accelerator does not have caching features, so CloudFront would be the recommended service for that purpose",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 809734,
          "date": "Wed 15 Feb 2023 17:30",
          "username": "\t\t\t\tAmerico32\t\t\t",
          "content": "C correto",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 791466,
          "date": "Sun 29 Jan 2023 09:05",
          "username": "\t\t\t\tProfXsamson\t\t\t",
          "content": "C, Caching == Edge location == CloudFront",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 748411,
          "date": "Sat 17 Dec 2022 21:55",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "C right answer",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 745895,
          "date": "Thu 15 Dec 2022 10:42",
          "username": "\t\t\t\tk1kavi1\t\t\t",
          "content": "Agreed",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 724447,
          "date": "Tue 22 Nov 2022 17:08",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 716946,
          "date": "Sat 12 Nov 2022 22:35",
          "username": "\t\t\t\tMyNameIsJulien\t\t\t",
          "content": "Answer is C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        }
      ]
    },
    {
      "question_id": "#156",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company produces batch data that comes from different databases. The company also produces live stream data from network sensors and application APIs. The company needs to consolidate all the data into one place for business analytics. The company needs to process the incoming data and then stage the data in different Amazon S3 buckets. Teams will later run one-time queries and import the data into a business intelligence tool to show key performance indicators (KPIs).<br>Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: AE</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#156",
          "answers": [
            {
              "choice": "<p>A. Use Amazon Athena for one-time queries. Use Amazon QuickSight to create dashboards for KPIs.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use Amazon Kinesis Data Analytics for one-time queries. Use Amazon QuickSight to create dashboards for KPIs.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create custom AWS Lambda functions to move the individual records from the databases to an Amazon Redshift cluster.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use an AWS Glue extract, transform, and load (ETL) job to convert the data into JSON format. Load the data into multiple Amazon OpenSearch Service (Amazon Elasticsearch Service) clusters.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Use blueprints in AWS Lake Formation to identify the data that can be ingested into a data lake. Use AWS Glue to crawl the source, extract the data, and load the data into Amazon S3 in Apache Parquet format.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 697932,
          "date": "Tue 18 Oct 2022 07:28",
          "username": "\t\t\t\tWazhija\t\t\t",
          "content": "I believe AE makes the most sense",
          "upvote_count": "9",
          "selected_answers": "Selected Answer: AE"
        },
        {
          "id": 704901,
          "date": "Wed 26 Oct 2022 19:38",
          "username": "\t\t\t\tSix_Fingered_Jose\t\t\t",
          "content": "yeah AE makes sense, only E is working with S3 here and questions wants them to be in S3",
          "upvote_count": "7",
          "selected_answers": "Selected Answer: AE"
        },
        {
          "id": 803880,
          "date": "Fri 10 Feb 2023 02:28",
          "username": "\t\t\t\tJiyuKim\t\t\t",
          "content": "Can anyone please explain me why B cannot be an answer?",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 785665,
          "date": "Mon 23 Jan 2023 18:36",
          "username": "\t\t\t\tashishvineetlko\t\t\t",
          "content": "can anyone help me in below question<br>36. A company has a Java application that uses Amazon Simple Queue Service (Amazon SOS) to parse messages. The application cannot parse messages that are large on 256KB size. The company wants to implement a solution to give the application the ability to parse messages as large as 50 MB. <br>Which solution will meet these requirements with the FEWEST changes to the code?<br>a) Use the Amazon SOS Extended Client Library for Java to host messages that are larger than 256 KB in Amazon S3.<br>b) Use Amazon EventBridge to post large messages from the application instead of Aaron SOS<br>c) Change the limit in Amazon SQS to handle messages that are larger than 256 KB<br>d) Store messages that are larger than 256 KB in Amazon Elastic File System (Amazon EFS) Configure AmazonSQS to reference this location in the messages.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I will do \\\"A\\\" as well.</li><li>A would probably be the best answer. Sqs extended client library is for Java apps.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 817573,
          "date": "Wed 22 Feb 2023 08:27",
          "username": "\t\t\t\tskondey\t\t\t",
          "content": "I will do \\\"A\\\" as well.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 791471,
          "date": "Sun 29 Jan 2023 09:14",
          "username": "\t\t\t\tProfXsamson\t\t\t",
          "content": "A would probably be the best answer. Sqs extended client library is for Java apps.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 784485,
          "date": "Sun 22 Jan 2023 17:33",
          "username": "\t\t\t\tbullrem\t\t\t",
          "content": "I believe DE makes the most sense",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: DE"
        },
        {
          "id": 782462,
          "date": "Fri 20 Jan 2023 17:07",
          "username": "\t\t\t\tShinobiGrappler\t\t\t",
          "content": "stored in s3 -> data lake -> athena (process the SQL parquet format)-> quicksight visualize",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: AE"
        },
        {
          "id": 763401,
          "date": "Mon 02 Jan 2023 01:17",
          "username": "\t\t\t\tZerotn3\t\t\t",
          "content": "While Amazon Athena is a fully managed service that makes it easy to analyze data stored in Amazon S3 using SQL, it is primarily designed for running ad-hoc queries on data stored in Amazon S3. It may not be the best choice for running one-time queries on streaming data, as it is not designed to process data in real-time.<br><br>Additionally, using Amazon Athena for one-time queries on streaming data could potentially lead to higher operational overhead, as you would need to set up and maintain the necessary infrastructure to stream the data into Amazon S3, and then query the data using Athena.<br><br>Using Amazon Kinesis Data Analytics, as mentioned in option B, would be a better choice for running one-time queries on streaming data, as it is specifically designed to process data in real-time and can automatically scale to match the incoming data rate.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>\\\"Company needs to consolidate all the data into one place\\\" -&gt; S3 bucket, which ishappening in E, which means Athena would not have an issue, so A is ok.</li><li>Absolutely, querying data is after staging and so Athena fits perfectly.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: BE"
        },
        {
          "id": 766339,
          "date": "Thu 05 Jan 2023 08:48",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "\\\"Company needs to consolidate all the data into one place\\\" -> S3 bucket, which ishappening in E, which means Athena would not have an issue, so A is ok.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Absolutely, querying data is after staging and so Athena fits perfectly.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 785152,
          "date": "Mon 23 Jan 2023 09:59",
          "username": "\t\t\t\tjainparag1\t\t\t",
          "content": "Absolutely, querying data is after staging and so Athena fits perfectly.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 755803,
          "date": "Sun 25 Dec 2022 17:00",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "C can work it out ,but has additional overhead.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: AE"
        },
        {
          "id": 748414,
          "date": "Sat 17 Dec 2022 21:57",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "A and E",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: AE"
        },
        {
          "id": 737582,
          "date": "Wed 07 Dec 2022 09:24",
          "username": "\t\t\t\tjavitech83\t\t\t",
          "content": "I would go for AE as information needs to be stored in S3",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 736922,
          "date": "Tue 06 Dec 2022 15:56",
          "username": "\t\t\t\tSwagata23\t\t\t",
          "content": "Anser is AE : https://aws.amazon.com/blogs/big-data/enhance-analytics-with-google-trends-data-using-aws-glue-amazon-athena-and-amazon-quicksight/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 728379,
          "date": "Sun 27 Nov 2022 16:35",
          "username": "\t\t\t\tDivaLight\t\t\t",
          "content": "Option AE",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AE"
        },
        {
          "id": 726044,
          "date": "Thu 24 Nov 2022 17:28",
          "username": "\t\t\t\tCizzla7049\t\t\t",
          "content": "A and C are correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 712391,
          "date": "Sun 06 Nov 2022 16:06",
          "username": "\t\t\t\tbackbencher2022\t\t\t",
          "content": "A&E is the correct answer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AE"
        },
        {
          "id": 708950,
          "date": "Tue 01 Nov 2022 09:01",
          "username": "\t\t\t\tDsouzaf\t\t\t",
          "content": "AC is correct. Ans E is also correctBut in ans E: since Apache Parquer format is used, this is not correct answer as per AWS exam answer<br>Six_Fingered_Jose<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>https://aws.amazon.com/tw/about-aws/whats-new/2018/12/amazon-s3-announces-parquet-output-format-for-inventory/</li></ul>",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 741738,
          "date": "Sun 11 Dec 2022 14:31",
          "username": "\t\t\t\tkmliuy73\t\t\t",
          "content": "https://aws.amazon.com/tw/about-aws/whats-new/2018/12/amazon-s3-announces-parquet-output-format-for-inventory/",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#157",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company stores data in an Amazon Aurora PostgreSQL DB cluster. The company must store all the data for 5 years and must delete all the data after 5 years. The company also must indefinitely keep audit logs of actions that are performed within the database. Currently, the company has automated backups configured for Aurora.<br><br>Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: DE</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#157",
          "answers": [
            {
              "choice": "<p>A. Take a manual snapshot of the DB cluster.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a lifecycle policy for the automated backups.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Configure automated backup retention for 5 years.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure an Amazon CloudWatch Logs export for the DB cluster.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. Use AWS Backup to take the backups and to keep the backups for 5 years.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 766371,
          "date": "Thu 05 Jan 2023 09:20",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "I tend to agree D and E. ..<br><br>A - Manual task that can be automated, so why make life difficult?<br>B - The maximum retention period is 35 days, so would not help<br>C - The maximum retention period is 35 days, so would not help<br>D - Only option that deals with logs, so makes sense<br>E - Partially manual but only option that achieves the 5 year goal",
          "upvote_count": "8",
          "selected_answers": ""
        },
        {
          "id": 729013,
          "date": "Mon 28 Nov 2022 12:14",
          "username": "\t\t\t\tkmaneith\t\t\t",
          "content": "dude trust me<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>No, please show your reasoning, you may be wrong. Remember, no one thinks they are wrong, but some always are :)</li></ul>",
          "upvote_count": "8",
          "selected_answers": "Selected Answer: DE"
        },
        {
          "id": 766351,
          "date": "Thu 05 Jan 2023 09:03",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "No, please show your reasoning, you may be wrong. Remember, no one thinks they are wrong, but some always are :)",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 848511,
          "date": "Thu 23 Mar 2023 18:37",
          "username": "\t\t\t\tneverdie\t\t\t",
          "content": "Automated backup is limited 35 days",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 787149,
          "date": "Wed 25 Jan 2023 02:28",
          "username": "\t\t\t\tTraining4aBetterLife\t\t\t",
          "content": "Previously, you had to create custom scripts to automate backup scheduling, enforce retention policies, or consolidate backup activity for manual Aurora cluster snapshots, especially when coordinating backups across AWS services. With AWS Backup, you gain a fully managed, policy-based backup solution with snapshot scheduling and snapshot retention management. You can now create, manage, and restore Aurora backups directly from the AWS Backup console for both PostgreSQL-compatible and MySQL-compatible versions of Aurora.<br>To get started, select an Amazon Aurora cluster from the AWS Backup console and take an on-demand backup or simply assign the cluster to a backup plan.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>https://aws.amazon.com/about-aws/whats-new/2020/06/amazon-aurora-snapshots-can-be-managed-via-aws-backup/?nc1=h_ls</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: DE"
        },
        {
          "id": 787151,
          "date": "Wed 25 Jan 2023 02:29",
          "username": "\t\t\t\tTraining4aBetterLife\t\t\t",
          "content": "https://aws.amazon.com/about-aws/whats-new/2020/06/amazon-aurora-snapshots-can-be-managed-via-aws-backup/?nc1=h_ls",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 763407,
          "date": "Mon 02 Jan 2023 01:31",
          "username": "\t\t\t\tZerotn3\t\t\t",
          "content": "A is not a valid option for meeting the requirements. A manual snapshot of the DB cluster is a point-in-time copy of the data in the cluster. While taking manual snapshots can be useful for creating backups of the data, it is not a reliable or efficient way to meet the requirement of storing all the data for 5 years and deleting it after 5 years. It would be difficult to ensure that manual snapshots are taken regularly and retained for the required period of time. It is recommended to use a fully managed backup service like AWS Backup, which can automate and centralize the process of taking and retaining backups.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Sorry, B and E that correctB.  Create a lifecycle policy for the automated backups.<br>This would ensure that the backups taken using AWS Backup are retained for the desired period of time.</li><li>I think a lifecycle policy would only keep backups for 35 days</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: DE"
        },
        {
          "id": 763410,
          "date": "Mon 02 Jan 2023 01:33",
          "username": "\t\t\t\tZerotn3\t\t\t",
          "content": "Sorry, B and E that correctB.  Create a lifecycle policy for the automated backups.<br>This would ensure that the backups taken using AWS Backup are retained for the desired period of time.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I think a lifecycle policy would only keep backups for 35 days</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 766374,
          "date": "Thu 05 Jan 2023 09:22",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "I think a lifecycle policy would only keep backups for 35 days",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 755817,
          "date": "Sun 25 Dec 2022 17:26",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "D andE only",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: DE"
        },
        {
          "id": 755037,
          "date": "Sat 24 Dec 2022 18:15",
          "username": "\t\t\t\tChirantan\t\t\t",
          "content": "AD<br> is correct as you can keep backup of snapshot indifferently.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 748416,
          "date": "Sat 17 Dec 2022 22:00",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "D and E",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: DE"
        },
        {
          "id": 744752,
          "date": "Wed 14 Dec 2022 06:38",
          "username": "\t\t\t\tQjb8m9h\t\t\t",
          "content": "Aurora backups are continuous and incremental so you can quickly restore to any point within the backup retention period. No performance impact or interruption of database service occurs as backup data is being written. You can specify a backup retention period, from 1 to 35 days, when you create or modify a DB cluster.<br><br>If you want to retain a backup beyond the backup retention period, you can also take a snapshot of the data in your cluster volume. Because Aurora retains incremental restore data for the entire backup retention period, you only need to create a snapshot for data that you want to retain beyond the backup retention period. You can create a new DB cluster from the snapshot.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 743634,
          "date": "Tue 13 Dec 2022 06:55",
          "username": "\t\t\t\tMarge_Simpson\t\t\t",
          "content": "D is the only one that resolves the logging situation<br>\\\"automated backup\\\" = AWS Backup<br>https://aws.amazon.com/backup/faqs/?nc=sn&loc=6<br>AWS Backup provides a centralized console, automated backup scheduling, backup retention management, and backup monitoring and alerting. AWS Backup offers advanced features such as lifecycle policies to transition backups to a low-cost storage tier. It also includes backup storage and encryption independent from its source data, audit and compliance reporting capabilities with AWS Backup Audit Manager, and delete protection with AWS Backup Vault Lock.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: DE"
        },
        {
          "id": 743540,
          "date": "Tue 13 Dec 2022 04:01",
          "username": "\t\t\t\tQjb8m9h\t\t\t",
          "content": "AD<br>Reason: When creating Aurora back up, you will need to specify the retention period which is between 1-35days. This does not meet the 5years retention requirement in this case. <br>Hence taking a snap manual snap shot is the best solution. <br><br>https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Backups.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 737284,
          "date": "Wed 07 Dec 2022 00:40",
          "username": "\t\t\t\tHeyang\t\t\t",
          "content": "no more than 35 days<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>https://aws.amazon.com/about-aws/whats-new/2020/06/amazon-aurora-snapshots-can-be-managed-via-aws-backup/?nc1=h_ls AWS Backup</li></ul>",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 741759,
          "date": "Sun 11 Dec 2022 14:57",
          "username": "\t\t\t\tkmliuy73\t\t\t",
          "content": "https://aws.amazon.com/about-aws/whats-new/2020/06/amazon-aurora-snapshots-can-be-managed-via-aws-backup/?nc1=h_ls AWS Backup",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 737254,
          "date": "Tue 06 Dec 2022 23:27",
          "username": "\t\t\t\tVicBucket1996\t\t\t",
          "content": "We all are agree with letter D but based in this documentation I think A could be the other correct answer:<br>https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Backups.html<br><br>But if I wrong, let me know, please :)<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>https://aws.amazon.com/about-aws/whats-new/2020/06/amazon-aurora-snapshots-can-be-managed-via-aws-backup/?nc1=h_ls AWS Backup</li></ul>",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 741760,
          "date": "Sun 11 Dec 2022 14:57",
          "username": "\t\t\t\tkmliuy73\t\t\t",
          "content": "https://aws.amazon.com/about-aws/whats-new/2020/06/amazon-aurora-snapshots-can-be-managed-via-aws-backup/?nc1=h_ls AWS Backup",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 728382,
          "date": "Sun 27 Nov 2022 16:38",
          "username": "\t\t\t\tDivaLight\t\t\t",
          "content": "DE Option",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: DE"
        },
        {
          "id": 727994,
          "date": "Sun 27 Nov 2022 08:24",
          "username": "\t\t\t\tPhinx\t\t\t",
          "content": "D and E is the most sensible options here.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: DE"
        },
        {
          "id": 727423,
          "date": "Sat 26 Nov 2022 10:46",
          "username": "\t\t\t\tjusttry\t\t\t",
          "content": "https://aws.amazon.com/about-aws/whats-new/2020/06/amazon-aurora-snapshots-can-be-managed-via-aws-backup/?nc1=h_ls<br>AWS Backup adds Amazon Aurora database cluster snapshots as its latest protected resource",
          "upvote_count": "5",
          "selected_answers": "Selected Answer: DE"
        },
        {
          "id": 726495,
          "date": "Fri 25 Nov 2022 09:16",
          "username": "\t\t\t\tNightducky\t\t\t",
          "content": "There is no sense with A if you can use AWS backup and keep snapshot for 5 years.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>https://aws.amazon.com/about-aws/whats-new/2020/06/amazon-aurora-snapshots-can-be-managed-via-aws-backup/?nc1=h_ls%20AWS%20Backup</li><li>But the retention period is between 1-35 went creating Aurora backup using AWS backup.</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: DE"
        },
        {
          "id": 765645,
          "date": "Wed 04 Jan 2023 13:58",
          "username": "\t\t\t\tHayLLlHuK\t\t\t",
          "content": "https://aws.amazon.com/about-aws/whats-new/2020/06/amazon-aurora-snapshots-can-be-managed-via-aws-backup/?nc1=h_ls%20AWS%20Backup",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 744747,
          "date": "Wed 14 Dec 2022 06:30",
          "username": "\t\t\t\tQjb8m9h\t\t\t",
          "content": "But the retention period is between 1-35 went creating Aurora backup using AWS backup.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#158",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A solutions architect is optimizing a website for an upcoming musical event. Videos of the performances will be streamed in real time and then will be available on demand. The event is expected to attract a global online audience.<br><br>Which service will improve the performance of both the real-time and on-demand streaming?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#158",
          "answers": [
            {
              "choice": "<p>A. Amazon CloudFront<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. AWS Global Accelerator<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Amazon Route 53<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Amazon S3 Transfer Acceleration<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 718763,
          "date": "Tue 15 Nov 2022 14:43",
          "username": "\t\t\t\tNigma\t\t\t",
          "content": "A is right<br><br>You can use CloudFront to deliver video on demand (VOD) or live streaming video using any HTTP origin<br><br>Global Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP, as well as for HTTP use cases that specifically require static IP addresses",
          "upvote_count": "14",
          "selected_answers": ""
        },
        {
          "id": 847519,
          "date": "Wed 22 Mar 2023 21:54",
          "username": "\t\t\t\twarioverde\t\t\t",
          "content": "How can Cloudfront help with real-time use case?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 775478,
          "date": "Sat 14 Jan 2023 15:05",
          "username": "\t\t\t\tMamiololo\t\t\t",
          "content": "Amazon CloudFront",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 765679,
          "date": "Wed 04 Jan 2023 14:21",
          "username": "\t\t\t\taba2s\t\t\t",
          "content": "CloudFront offers several options for streaming your media to global viewersboth pre-recorded files and live events. https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/IntroductionUseCases.html#IntroductionUseCasesStreaming",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 748418,
          "date": "Sat 17 Dec 2022 22:07",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "A Cloudfront",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 745331,
          "date": "Wed 14 Dec 2022 18:37",
          "username": "\t\t\t\tBaba_Eni\t\t\t",
          "content": "Cloudfront is used for live streaming and video on-demand<br><br>https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/IntroductionUseCases.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 729081,
          "date": "Mon 28 Nov 2022 13:32",
          "username": "\t\t\t\tleonnnn\t\t\t",
          "content": "I thought the real-time streaming comes with rtsp protocol for which B is better.<br>But I realized now real-time streaming also has http way now (like HLS, etc.).<br>So the answer should be A. ",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 724450,
          "date": "Tue 22 Nov 2022 17:12",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 719468,
          "date": "Wed 16 Nov 2022 09:18",
          "username": "\t\t\t\tbabaxoxo\t\t\t",
          "content": "CloudFront for sure",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        }
      ]
    },
    {
      "question_id": "#159",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is running a publicly accessible serverless application that uses Amazon API Gateway and AWS Lambda. The application's traffic recently spiked due to fraudulent requests from botnets.<br><br>Which steps should a solutions architect take to block requests from unauthorized users? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: AC</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#159",
          "answers": [
            {
              "choice": "<p>A. Create a usage plan with an API key that is shared with genuine users only.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Integrate logic within the Lambda function to ignore the requests from fraudulent IP addresses.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Implement an AWS WAF rule to target malicious requests and trigger actions to filter them out.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Convert the existing public API to a private API. Update the DNS records to redirect users to the new API endpoint.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Create an IAM role for each user attempting to access the API. A user will assume the role when making the API call.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 828699,
          "date": "Sat 04 Mar 2023 06:47",
          "username": "\t\t\t\tsachin\t\t\t",
          "content": "It should be A and C <br>But API Key alone can not help <br><br>API keys are alphanumeric string values that you distribute to application developer customers to grant access to your API. You can use API keys together with Lambda authorizers, IAM roles, or Amazon Cognito to control access to your APIs.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 823954,
          "date": "Mon 27 Feb 2023 17:38",
          "username": "\t\t\t\tSteve_4542636\t\t\t",
          "content": "Here https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html it says this:<br><br>Don't use API keys for authentication or authorization for your APIs. If you have multiple APIs in a usage plan, a user with a valid API key for one API in that usage plan can access all APIs in that usage plan. Instead, use an IAM role, a Lambda authorizer, or an Amazon Cognito user pool.<br><br>API keys are intended for software developers wanting to access an API from their application.This link then goes on to say an IAM role should be used instead.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Nevermind my answer.I switch it to A/C because the question states the application is *using* the API Gateway so A will make sense</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: CE"
        },
        {
          "id": 823957,
          "date": "Mon 27 Feb 2023 17:42",
          "username": "\t\t\t\tSteve_4542636\t\t\t",
          "content": "Nevermind my answer.I switch it to A/C because the question states the application is *using* the API Gateway so A will make sense",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 781746,
          "date": "Fri 20 Jan 2023 02:07",
          "username": "\t\t\t\tsimplimarvelous\t\t\t",
          "content": "A/C for security to prevent anonymous access",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 766394,
          "date": "Thu 05 Jan 2023 09:35",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "I'm thinking A and C<br>A - the API is publicly accessible but there is nothing to stop the company requiring users to register for access.<br>B - you can do this with Lambda, AWS Network Firewall and Amazon GuardDuty, see https://aws.amazon.com/blogs/security/automatically-block-suspicious-traffic-with-aws-network-firewall-and-amazon-guardduty/, but these components are not mentioned<br>C - a WAF is the logical choice with it's bot detection capabilities<br>D - a private API is only accessible within a VPC, so this would not work<br>E- would be even more work than A",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 765664,
          "date": "Wed 04 Jan 2023 14:11",
          "username": "\t\t\t\tHayLLlHuK\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/61082-exam-aws-certified-solutions-architect-associate-saa-c02/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 760155,
          "date": "Wed 28 Dec 2022 18:33",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html<br>https://medium.com/@tshemku/aws-waf-vs-firewall-manager-vs-shield-vs-shield-advanced-4c86911e94c6",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 758074,
          "date": "Tue 27 Dec 2022 03:06",
          "username": "\t\t\t\tSoluAWS\t\t\t",
          "content": "I do not agree with A as it mentioned the application is publically accessible. \\\"A company is running a publicly accessible serverless application that uses Amazon API Gateway and AWS Lambda\\\". If this is public how can we ensure that genuine user?<br><br>I will go with CD",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 755824,
          "date": "Sun 25 Dec 2022 17:34",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "A and C ,C is obivious ,however A is the only other which seems to put quota API keys are alphanumeric string values that you distribute to application developer customers to grant access to your API. You can use API keys together with Lambda authorizers, IAM roles, or Amazon Cognito to control access to your APIs",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 748422,
          "date": "Sat 17 Dec 2022 22:12",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "A and C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 727995,
          "date": "Sun 27 Nov 2022 08:27",
          "username": "\t\t\t\tPhinx\t\t\t",
          "content": "A and C are the correct choices.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 727425,
          "date": "Sat 26 Nov 2022 10:53",
          "username": "\t\t\t\tjusttry\t\t\t",
          "content": "https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 726492,
          "date": "Fri 25 Nov 2022 09:11",
          "username": "\t\t\t\t5up3rm4n\t\t\t",
          "content": "Only answer C is an obviouis choice.B and D are clearly not right and A is the only remotely viable other answer but even then the documentation on API Keys and Usage quotas states not to rely on it to block API requests;<br><br>Usage plan throttling and quotas are not hard limits, and are applied on a best-effort basis. In some cases, clients can exceed the quotas that you set. Don't rely on usage plan quotas or throttling to control costs or block access to an API. Consider using AWS Budgets to monitor costs and AWS WAF to manage API requests.<br><br>https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 725787,
          "date": "Thu 24 Nov 2022 12:41",
          "username": "\t\t\t\tds0321\t\t\t",
          "content": "A and C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 719472,
          "date": "Wed 16 Nov 2022 09:24",
          "username": "\t\t\t\tbabaxoxo\t\t\t",
          "content": "use usage plan API key",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 718766,
          "date": "Tue 15 Nov 2022 14:45",
          "username": "\t\t\t\tNigma\t\t\t",
          "content": "A and C",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#160",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>An ecommerce company hosts its analytics application in the AWS Cloud. The application generates about 300 MB of data each month. The data is stored in JSON format. The company is evaluating a disaster recovery solution to back up the data. The data must be accessible in milliseconds if it is needed, and the data must be kept for 30 days.<br><br>Which solution meets these requirements MOST cost-effectively?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#160",
          "answers": [
            {
              "choice": "<p>A. Amazon OpenSearch Service (Amazon Elasticsearch Service)<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Amazon S3 Glacier<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Amazon S3 Standard<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Amazon RDS for PostgreSQL<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 719482,
          "date": "Wed 16 Nov 2022 09:30",
          "username": "\t\t\t\tbabaxoxo\t\t\t",
          "content": "Ans C:<br>Cost-effective solution with milliseconds of retrieval -> it should be s3 standard",
          "upvote_count": "6",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 805326,
          "date": "Sat 11 Feb 2023 16:04",
          "username": "\t\t\t\tKZM\t\t\t",
          "content": "A.  Incorrect<br>Amazon OpenSearch Service (Amazon Elasticsearch Service) is designed for full-text search and analytics, but it may not be the most cost-effective solution for this use caseB.  Incorrect<br>S3 Glacier is a cold storage solution that is designed for long-term data retention and infrequent access.C.  Correct<br>S3 standard is cost-effective and meets the requirement. S3 Standard allows for data retention for a specific number of days.<br>D.  PostgreSQL is a relational database service and may not be the most cost-effective solution.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 767808,
          "date": "Fri 06 Jan 2023 15:47",
          "username": "\t\t\t\tngochieu276\t\t\t",
          "content": "Selected Answer: B<br>S3 Glacier Instant Retrieval  Use for archiving data that is rarely accessed and requires milliseconds retrieval.<br>https://docs.aws.amazon.com/amazonglacier/latest/dev/introduction.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 748354,
          "date": "Sat 17 Dec 2022 20:49",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 742453,
          "date": "Mon 12 Dec 2022 07:19",
          "username": "\t\t\t\tlapaki\t\t\t",
          "content": "JSON is object notation.S3 stores objects.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 734314,
          "date": "Sat 03 Dec 2022 10:25",
          "username": "\t\t\t\thpipit\t\t\t",
          "content": "c IS correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 724453,
          "date": "Tue 22 Nov 2022 17:14",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 719579,
          "date": "Wed 16 Nov 2022 12:37",
          "username": "\t\t\t\tsdasdawa\t\t\t",
          "content": "IMHO <br>Normally ElasticSearch would be ideal here, however as question states \\\"Most cost-effective\\\" <br>S3 is the best choice in this case<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>ElasticSearch is a search service, the question states here about the backup service reqd. for the DR scenario.</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 733109,
          "date": "Thu 01 Dec 2022 22:26",
          "username": "\t\t\t\tAamee\t\t\t",
          "content": "ElasticSearch is a search service, the question states here about the backup service reqd. for the DR scenario.",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#161",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has a small Python application that processes JSON documents and outputs the results to an on-premises SQL database. The application runs thousands of times each day. The company wants to move the application to the AWS Cloud. The company needs a highly available solution that maximizes scalability and minimizes operational overhead.<br><br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#161",
          "answers": [
            {
              "choice": "<p>A. Place the JSON documents in an Amazon S3 bucket. Run the Python code on multiple Amazon EC2 instances to process the documents. Store the results in an Amazon Aurora DB cluster.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Place the JSON documents in an Amazon S3 bucket. Create an AWS Lambda function that runs the Python code to process the documents as they arrive in the S3 bucket. Store the results in an Amazon Aurora DB cluster.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Place the JSON documents in an Amazon Elastic Block Store (Amazon EBS) volume. Use the EBS Multi-Attach feature to attach the volume to multiple Amazon EC2 instances. Run the Python code on the EC2 instances to process the documents. Store the results on an Amazon RDS DB instance.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Place the JSON documents in an Amazon Simple Queue Service (Amazon SQS) queue as messages. Deploy the Python code as a container on an Amazon Elastic Container Service (Amazon ECS) cluster that is configured with the Amazon EC2 launch type. Use the container to process the SQS messages. Store the results on an Amazon RDS DB instance.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 719484,
          "date": "Wed 16 Nov 2022 09:33",
          "username": "\t\t\t\tbabaxoxo\t\t\t",
          "content": "solution should remove operation overhead -> s3 -> lambda -> aurora",
          "upvote_count": "7",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 823299,
          "date": "Mon 27 Feb 2023 07:47",
          "username": "\t\t\t\tperception\t\t\t",
          "content": "does somebody had contributor access and want to share. i would really appreciate it.<br>here's my email <br>367501tab@gmail.com<br>Thanks",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 818719,
          "date": "Thu 23 Feb 2023 04:06",
          "username": "\t\t\t\tkerin\t\t\t",
          "content": "B is the best option. https://aws.amazon.com/rds/aurora/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 763445,
          "date": "Mon 02 Jan 2023 02:42",
          "username": "\t\t\t\tZerotn3\t\t\t",
          "content": "By placing the JSON documents in an S3 bucket, the documents will be stored in a highly durable and scalable object storage service. The use of AWS Lambda allows the company to run their Python code to process the documents as they arrive in the S3 bucket without having to worry about the underlying infrastructure. This also allows for horizontal scalability, as AWS Lambda will automatically scale the number of instances of the function based on the incoming rate of requests. The results can be stored in an Amazon Aurora DB cluster, which is a fully-managed, high-performance database service that is compatible with MySQL and PostgreSQL. This will provide the necessary durability and scalability for the results of the processing.",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 761256,
          "date": "Thu 29 Dec 2022 17:26",
          "username": "\t\t\t\tmp165\t\t\t",
          "content": "agree...B is the best optionS3, Lambda , Aurora.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 755837,
          "date": "Sun 25 Dec 2022 17:55",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "Choosing B as \\\"The company needs a highly available solution that maximizes scalability and minimizes operational overhead\\\"",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 748900,
          "date": "Sun 18 Dec 2022 14:19",
          "username": "\t\t\t\tstudis\t\t\t",
          "content": "B is tempting but this sentence \\\"runs thousands of times each day.\\\" If we use lambda as in B, won't this incur a high bill at the end?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Agree,but question doesnt have Cost as criteria to choose solution, Criteria is \\\"The company needs a highly available solution that maximizes scalability and minimizes operational overhead\\\". Hence B</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 755834,
          "date": "Sun 25 Dec 2022 17:55",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "Agree,but question doesnt have Cost as criteria to choose solution, Criteria is \\\"The company needs a highly available solution that maximizes scalability and minimizes operational overhead\\\". Hence B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 748356,
          "date": "Sat 17 Dec 2022 20:52",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 727998,
          "date": "Sun 27 Nov 2022 08:30",
          "username": "\t\t\t\tPhinx\t\t\t",
          "content": "D is incorrect because using ECS entails a lot of admin overhead. so B is the correct one.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 724458,
          "date": "Tue 22 Nov 2022 17:21",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 722986,
          "date": "Sun 20 Nov 2022 21:28",
          "username": "\t\t\t\tEKA_CloudGod\t\t\t",
          "content": "B is the answer<br>https://aws.amazon.com/rds/aurora/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 719702,
          "date": "Wed 16 Nov 2022 14:51",
          "username": "\t\t\t\tBENICE\t\t\t",
          "content": "D is correct option<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>ehhhhhh</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 720476,
          "date": "Thu 17 Nov 2022 14:29",
          "username": "\t\t\t\tNightducky\t\t\t",
          "content": "ehhhhhh",
          "upvote_count": "4",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#162",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company wants to use high performance computing (HPC) infrastructure on AWS for financial risk modeling. The company's HPC workloads run on Linux. Each HPC workflow runs on hundreds of Amazon EC2 Spot Instances, is short-lived, and generates thousands of output files that are ultimately stored in persistent storage for analytics and long-term future use.<br><br>The company seeks a cloud storage solution that permits the copying of on-premises data to long-term persistent storage to make data available for processing by all EC2 instances. The solution should also be a high performance file system that is integrated with persistent storage to read and write datasets and output files.<br><br>Which combination of AWS services meets these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#162",
          "answers": [
            {
              "choice": "<p>A. Amazon FSx for Lustre integrated with Amazon S3<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Amazon FSx for Windows File Server integrated with Amazon S3<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Amazon S3 Glacier integrated with Amazon Elastic Block Store (Amazon EBS)<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Amazon S3 bucket with a VPC endpoint integrated with an Amazon Elastic Block Store (Amazon EBS) General Purpose SSD (gp2) volume<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 743645,
          "date": "Tue 13 Dec 2022 07:07",
          "username": "\t\t\t\tMarge_Simpson\t\t\t",
          "content": "If you see HPC and Linux both in the question.. Pick Amazon FSx for Lustre<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>yeap, you're right!</li></ul>",
          "upvote_count": "5",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 764940,
          "date": "Tue 03 Jan 2023 19:55",
          "username": "\t\t\t\tHayLLlHuK\t\t\t",
          "content": "yeap, you're right!",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 818726,
          "date": "Thu 23 Feb 2023 04:11",
          "username": "\t\t\t\tkerin\t\t\t",
          "content": "FSx for Lustre makes it easy and cost-effective to launch and run the popular, high-performance Lustre file system. You use Lustre for workloads where speed matters, such as machine learning, high performance computing (HPC), video processing, and financial modeling. <br>Amazon Fsx for Lustre is integrated with Amazon S3.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 765716,
          "date": "Wed 04 Jan 2023 14:58",
          "username": "\t\t\t\taba2s\t\t\t",
          "content": "Additional keywords:make data available for processing by all EC2 instances ==> FSx<br><br>In absence of EFS, it should be FSx. Amazon FSx For Lustre provides a high-performance, parallel file system for hot data",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 764754,
          "date": "Tue 03 Jan 2023 15:38",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "Amazon FSx for Lustre integrated with Amazon S3",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 755838,
          "date": "Sun 25 Dec 2022 17:58",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "A is right choice here.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 748360,
          "date": "Sat 17 Dec 2022 20:55",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option A is the best high performance storage with integration to S3",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 745496,
          "date": "Wed 14 Dec 2022 23:18",
          "username": "\t\t\t\twly_al\t\t\t",
          "content": "requirement is File System and workload running on linux. so S3 and FSx for windows is not an option",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 741665,
          "date": "Sun 11 Dec 2022 13:11",
          "username": "\t\t\t\tShasha1\t\t\t",
          "content": "A <br>The Amazon FSx for Lustre service is a fully managed, high-performance file system that makes it easy to move and process large amounts of data quickly and cost-effectively. It provides a fully managed, cloud-native file system with low operational overhead, designed for massively parallel processing and high-performance workloads. The Lustre file system is a popular, open source parallel file system that is well-suited for a variety of applications such as HPC, image processing, AI/ML, media processing, data analytics, and financial modeling, among others. With Amazon FSx for Lustre, you can quickly create and configure new file systems in minutes, and easily scale the size of your file system up or down",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 724460,
          "date": "Tue 22 Nov 2022 17:23",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 719704,
          "date": "Wed 16 Nov 2022 14:56",
          "username": "\t\t\t\tBENICE\t\t\t",
          "content": "A - for HPC \\\"Amazon FSx for Lustre\\\" and long-term persistence \\\"S3\\\"",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 719565,
          "date": "Wed 16 Nov 2022 12:06",
          "username": "\t\t\t\trjam\t\t\t",
          "content": "Amazon FSx for Lustre: <br> HPC optimized distributed file system, millions of IOPS<br> Backed by S3<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Answer A</li></ul>",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 719566,
          "date": "Wed 16 Nov 2022 12:06",
          "username": "\t\t\t\trjam\t\t\t",
          "content": "Answer A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 719491,
          "date": "Wed 16 Nov 2022 09:49",
          "username": "\t\t\t\tbabaxoxo\t\t\t",
          "content": "FxS Lustre integrated with S3",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        }
      ]
    },
    {
      "question_id": "#163",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is building a containerized application on premises and decides to move the application to AWS. The application will have thousands of users soon after it is deployed. The company is unsure how to manage the deployment of containers at scale. The company needs to deploy the containerized application in a highly available architecture that minimizes operational overhead.<br><br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#163",
          "answers": [
            {
              "choice": "<p>A. Store container images in an Amazon Elastic Container Registry (Amazon ECR) repository. Use an Amazon Elastic Container Service (Amazon ECS) cluster with the AWS Fargate launch type to run the containers. Use target tracking to scale automatically based on demand.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Store container images in an Amazon Elastic Container Registry (Amazon ECR) repository. Use an Amazon Elastic Container Service (Amazon ECS) cluster with the Amazon EC2 launch type to run the containers. Use target tracking to scale automatically based on demand.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Store container images in a repository that runs on an Amazon EC2 instance. Run the containers on EC2 instances that are spread across multiple Availability Zones. Monitor the average CPU utilization in Amazon CloudWatch. Launch new EC2 instances as needed.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an Amazon EC2 Amazon Machine Image (AMI) that contains the container image. Launch EC2 instances in an Auto Scaling group across multiple Availability Zones. Use an Amazon CloudWatch alarm to scale out EC2 instances when the average CPU utilization threshold is breached.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 718746,
          "date": "Tue 15 Nov 2022 14:28",
          "username": "\t\t\t\tgoatbernard\t\t\t",
          "content": "AWS Fargate",
          "upvote_count": "7",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 759731,
          "date": "Wed 28 Dec 2022 11:45",
          "username": "\t\t\t\tCheckpointMaster\t\t\t",
          "content": "Option A<br><br>AWS Fargate is a technology that you can use with Amazon ECS to run containers without having to manage servers or clusters of Amazon EC2 instances. With Fargate, you no longer have to provision, configure, or scale clusters of virtual machines to run containers. This removes the need to choose server types, decide when to scale your clusters, or optimize cluster packing.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 748362,
          "date": "Sat 17 Dec 2022 20:57",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 748116,
          "date": "Sat 17 Dec 2022 15:27",
          "username": "\t\t\t\talect096\t\t\t",
          "content": "\\\"minimizes operational overhead\\\" --> Fargate is serverless",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 741667,
          "date": "Sun 11 Dec 2022 13:13",
          "username": "\t\t\t\tShasha1\t\t\t",
          "content": "A<br>AWS Fargate is a serverless experience for user applications, allowing the user to concentrate on building applications instead of configuring and managing servers. Fargate also automates resource management, allowing users to easily scale their applications in response to demand.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 728001,
          "date": "Sun 27 Nov 2022 08:33",
          "username": "\t\t\t\tPhinx\t\t\t",
          "content": "Fargate is the only serverless option.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 724462,
          "date": "Tue 22 Nov 2022 17:24",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 721192,
          "date": "Fri 18 Nov 2022 10:21",
          "username": "\t\t\t\tds0321\t\t\t",
          "content": "AWS Fargate",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 719719,
          "date": "Wed 16 Nov 2022 15:08",
          "username": "\t\t\t\tBENICE\t\t\t",
          "content": "I think A is the correct option. AWS Farget",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 719271,
          "date": "Wed 16 Nov 2022 03:00",
          "username": "\t\t\t\tmricee9\t\t\t",
          "content": "A seems right",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: A"
        }
      ]
    },
    {
      "question_id": "#164",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has two applications: a sender application that sends messages with payloads to be processed and a processing application intended to receive the messages with payloads. The company wants to implement an AWS service to handle messages between the two applications. The sender application can send about 1,000 messages each hour. The messages may take up to 2 days to be processed: If the messages fail to process, they must be retained so that they do not impact the processing of any remaining messages.<br><br>Which solution meets these requirements and is the MOST operationally efficient?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#164",
          "answers": [
            {
              "choice": "<p>A. Set up an Amazon EC2 instance running a Redis database. Configure both applications to use the instance. Store, process, and delete the messages, respectively.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use an Amazon Kinesis data stream to receive the messages from the sender application. Integrate the processing application with the Kinesis Client Library (KCL).<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Integrate the sender and processor applications with an Amazon Simple Queue Service (Amazon SQS) queue. Configure a dead-letter queue to collect the messages that failed to process.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Subscribe the processing application to an Amazon Simple Notification Service (Amazon SNS) topic to receive notifications to process. Integrate the sender application to write to the SNS topic.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 827102,
          "date": "Thu 02 Mar 2023 17:26",
          "username": "\t\t\t\tvherman\t\t\t",
          "content": "SQS has a limit 12h for visibility time out",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 784400,
          "date": "Sun 22 Jan 2023 15:42",
          "username": "\t\t\t\tbullrem\t\t\t",
          "content": "Option C, using Amazon SQS, is a valid solution that meets the requirements of the company. However, it may not be the most operationally efficient solution because SQS is a managed message queue service that requires additional operational overhead to handle the retention of messages that failed to process. Option B, using Amazon Kinesis Data Streams, is more operationally efficient for this use case because it can handle the retention of messages that failed to process automatically and provides the ability to process and analyze streaming data in real-time.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Kinesis stream save data for up to 24 hours, doesn't meet the 2 day requirement.<br>Kinesis streams don't have fail-safe for failed processing, unlike SQS.<br>The correct answer is C - SQS.</li><li>There's no way for kinesis to know whether the message processing failed.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 824043,
          "date": "Mon 27 Feb 2023 19:53",
          "username": "\t\t\t\tUnluckyDucky\t\t\t",
          "content": "Kinesis stream save data for up to 24 hours, doesn't meet the 2 day requirement.<br>Kinesis streams don't have fail-safe for failed processing, unlike SQS.<br>The correct answer is C - SQS.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 795876,
          "date": "Thu 02 Feb 2023 09:16",
          "username": "\t\t\t\tLuckyAro\t\t\t",
          "content": "There's no way for kinesis to know whether the message processing failed.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 765743,
          "date": "Wed 04 Jan 2023 15:20",
          "username": "\t\t\t\taba2s\t\t\t",
          "content": "Amazon SQS supports dead-letter queues (DLQ), which other queues (source queues) can target for messages that can't be processed (consumed) successfully.<br><br>https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 748363,
          "date": "Sat 17 Dec 2022 20:59",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option C. ",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 734903,
          "date": "Sun 04 Dec 2022 08:17",
          "username": "\t\t\t\tocbn3wby\t\t\t",
          "content": "This matches mostly the job of Dead Letter Q: <br><br>https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html<br>vs<br>https://docs.aws.amazon.com/streams/latest/dev/shared-throughput-kcl-consumers.html",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 729503,
          "date": "Mon 28 Nov 2022 19:15",
          "username": "\t\t\t\tKapello10\t\t\t",
          "content": "Option C is the correct ans",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 729216,
          "date": "Mon 28 Nov 2022 15:26",
          "username": "\t\t\t\tGabs90\t\t\t",
          "content": "C is correct. The B is wrong because the question ask for a way to let the two application to comunicate, so che process is already done",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 727713,
          "date": "Sat 26 Nov 2022 18:31",
          "username": "\t\t\t\tTelaO\t\t\t",
          "content": "Please explain by \\\"B\\\" is incorrect?How does SQS process data?<br><br>\\\"KCL helps you consume and process data from a Kinesis data stream by taking care of many of the complex tasks associated with distributed computing.\\\"<br><br>https://docs.aws.amazon.com/streams/latest/dev/shared-throughput-kcl-consumers.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>As per question, the processing application will take messages.<br>\\\"The company wants to implement an AWS service to handle messages between the two applications.\\\"</li><li>The processing is done at the 2nd application level. <br><br>This seems to be the job of Dead Letter Q</li><li>Kinesis may not be having message retry - there is no way for kinesis to know whether the message processing failed. message can be there till their retention period.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 764945,
          "date": "Tue 03 Jan 2023 20:02",
          "username": "\t\t\t\tHayLLlHuK\t\t\t",
          "content": "As per question, the processing application will take messages.<br>\\\"The company wants to implement an AWS service to handle messages between the two applications.\\\"",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 734902,
          "date": "Sun 04 Dec 2022 08:16",
          "username": "\t\t\t\tocbn3wby\t\t\t",
          "content": "The processing is done at the 2nd application level. <br><br>This seems to be the job of Dead Letter Q",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 729362,
          "date": "Mon 28 Nov 2022 17:11",
          "username": "\t\t\t\tKADSM\t\t\t",
          "content": "Kinesis may not be having message retry - there is no way for kinesis to know whether the message processing failed. message can be there till their retention period.",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 724463,
          "date": "Tue 22 Nov 2022 17:26",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 719890,
          "date": "Wed 16 Nov 2022 19:13",
          "username": "\t\t\t\tmabotega\t\t\t",
          "content": "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 719727,
          "date": "Wed 16 Nov 2022 15:17",
          "username": "\t\t\t\tBENICE\t\t\t",
          "content": "Option: C<br>\\\"Amazon FSx for Lustre\\\" ---> Dead Letter Queue",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 718852,
          "date": "Tue 15 Nov 2022 15:30",
          "username": "\t\t\t\tNigma\t\t\t",
          "content": "Ans: C<br>https://aws.amazon.com/blogs/compute/building-loosely-coupled-scalable-c-applications-with-amazon-sqs-and-amazon-sns/<br>https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#165",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A solutions architect must design a solution that uses Amazon CloudFront with an Amazon S3 origin to store a static website. The company's security policy requires that all website traffic be inspected by AWS WAF. <br><br>How should the solutions architect comply with these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#165",
          "answers": [
            {
              "choice": "<p>A. Configure an S3 bucket policy to accept requests coming from the AWS WAF Amazon Resource Name (ARN) only.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Configure Amazon CloudFront to forward all incoming requests to AWS WAF before requesting content from the S3 origin.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Configure a security group that allows Amazon CloudFront IP addresses to access Amazon S3 only. Associate AWS WAF to CloudFront.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure Amazon CloudFront and Amazon S3 to use an origin access identity (OAI) to restrict access to the S3 bucket. Enable AWS WAF on the distribution.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 718854,
          "date": "Tue 15 Nov 2022 15:32",
          "username": "\t\t\t\tNigma\t\t\t",
          "content": "Answer D.  Use an OAI to lockdown CloudFront to S3 origin & enable WAF on CF distribution<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-access-to-amazon-s3/ confirms use of OAI (and option D).</li></ul>",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 761481,
          "date": "Thu 29 Dec 2022 21:53",
          "username": "\t\t\t\tFNJ1111\t\t\t",
          "content": "https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-access-to-amazon-s3/ confirms use of OAI (and option D).",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 850791,
          "date": "Sun 26 Mar 2023 09:11",
          "username": "\t\t\t\tMaxMa\t\t\t",
          "content": "To option B, If OAI is not used, how about the direct traffic to S3 be inspect by WAF?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 847582,
          "date": "Wed 22 Mar 2023 23:51",
          "username": "\t\t\t\tnbisdfsd\t\t\t",
          "content": "D.  Is wrong because \\\"..specifically, OAI doesn't support:<br><br>Amazon S3 buckets in all AWS Regions, including opt-in Regions\\\"<br><br>https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 834196,
          "date": "Thu 09 Mar 2023 17:56",
          "username": "\t\t\t\tBogeyman1984\t\t\t",
          "content": "According to chat gpt <br><br>To comply with the security policy that requires all website traffic to be inspected by AWS WAF, the solutions architect should configure Amazon CloudFront to forward all incoming requests to AWS WAF before requesting content from the S3 origin. Therefore, option B is the correct answer.<br><br>Option A is not sufficient because it only restricts access to the S3 bucket, but it does not ensure that all website traffic is inspected by AWS WAF. <br><br>Option C is also not sufficient because it only allows Amazon CloudFront IP addresses to access Amazon S3, but it does not ensure that all website traffic is inspected by AWS WAF. <br><br>Option D is partially correct because it uses an origin access identity (OAI) to restrict access to the S3 bucket, but it does not mention configuring Amazon CloudFront to forward all incoming requests to AWS WAF before requesting content from the S3 origin. Therefore, it is not the best answer.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 833473,
          "date": "Thu 09 Mar 2023 01:26",
          "username": "\t\t\t\tMaxMa\t\t\t",
          "content": "With option B, the question is if the WAF can be intergrated with the S3?",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 832820,
          "date": "Wed 08 Mar 2023 11:35",
          "username": "\t\t\t\tAkademik6\t\t\t",
          "content": "The Answer is D. ",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 816321,
          "date": "Tue 21 Feb 2023 08:27",
          "username": "\t\t\t\tTtomm\t\t\t",
          "content": "it should be D.  refer at section \\\"Securing Your Content\\\"<br>https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 799955,
          "date": "Mon 06 Feb 2023 17:28",
          "username": "\t\t\t\tCaoMengde09\t\t\t",
          "content": "For people who chose B as the right Answer, look at this link : https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html<br><br>\\\"When you create a web ACL, you can specify one or more CloudFront distributions that you want AWS WAF to inspect. AWS WAF starts to inspect and manage web requests for those distributions based on the criteria that you identify in the web ACL\\\"<br><br>You don't configure Cloudfront to redirect traffic to WAF.  You just create an ACL and points to the Cloudfront distribution.<br><br>So D is the best solution to secure and integrate Cloudfront with S3 and WAF. <br><br>From one side it protects your S3 Content by allowing user requests to access only the OAI.<br>And from other side it enable WAF to control traffic before reaching Cloudfront by creating a WAF Rule or ACL (Not redirecting Cloudfront traffic to WAF which as a solution architect you cannot do)",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 799294,
          "date": "Mon 06 Feb 2023 01:53",
          "username": "\t\t\t\ttinkeringpuncturing\t\t\t",
          "content": "explicitly explains the rationale for war forwarding-- new feature<br>https://aws.amazon.com/blogs/security/how-to-enhance-amazon-cloudfront-origin-security-with-aws-waf-and-aws-secrets-manager/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 787222,
          "date": "Wed 25 Jan 2023 03:49",
          "username": "\t\t\t\tTraining4aBetterLife\t\t\t",
          "content": "This can be done by selecting \\\"Yes\\\" for \\\"Viewer Protocol Policy\\\" when creating or updating the CloudFront distribution and selecting \\\"AWS WAF\\\" for \\\"Origin Protocol Policy.\\\" This will ensure that all traffic to the website is inspected by AWS WAF before being served by CloudFront.<br><br>Option D is incorrect because configuring Amazon CloudFront and Amazon S3 to use an origin access identity (OAI) to restrict access to the S3 bucket and enabling AWS WAF on the distribution will not allow AWS WAF to inspect website traffic BEFORE it is served by CloudFront and S3.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 783736,
          "date": "Sat 21 Jan 2023 22:20",
          "username": "\t\t\t\tbullrem\t\t\t",
          "content": "This allows the website traffic to be inspected by AWS WAF before being served by CloudFront and S3.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 775913,
          "date": "Sat 14 Jan 2023 21:45",
          "username": "\t\t\t\tmj61\t\t\t",
          "content": "Option B is the best solution as it specifies that Cloudfront should forward ALL incoming requests to AWS WAF before requesting content from S3 origin. This way, all the incoming traffic to the website will be inspected by AWS WAF and only the traffic that meets the security rules will be allowed to access the content stored in the S3 bucket.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 764791,
          "date": "Tue 03 Jan 2023 16:14",
          "username": "\t\t\t\tSilentMilli\t\t\t",
          "content": "Key word: origin access identity (OAI)",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 763430,
          "date": "Mon 02 Jan 2023 02:12",
          "username": "\t\t\t\tMindvision\t\t\t",
          "content": "D = correct answer<br>Not sure why people are picking B.  Traffic is inspected first by the WAF if conditions are met the Cloudfront responds to requests either to request content or deny from the S3 this would then be based on OAI.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 763343,
          "date": "Sun 01 Jan 2023 22:17",
          "username": "\t\t\t\tdan80\t\t\t",
          "content": "B - https://aws.amazon.com/blogs/security/how-to-enhance-amazon-cloudfront-origin-security-with-aws-waf-and-aws-secrets-manager/<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>The URL shows that B is wrong! You do not 'Configure Amazon CloudFront to forward all incoming requests to AWS WAF' but instead 'When you create a web ACL, you can specify one or more CloudFront distributions that you want AWS WAF to inspect' - see https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html</li><li>Ignore that, I'm thinking B too now</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 766429,
          "date": "Thu 05 Jan 2023 10:12",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "The URL shows that B is wrong! You do not 'Configure Amazon CloudFront to forward all incoming requests to AWS WAF' but instead 'When you create a web ACL, you can specify one or more CloudFront distributions that you want AWS WAF to inspect' - see https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Ignore that, I'm thinking B too now</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 766432,
          "date": "Thu 05 Jan 2023 10:13",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "Ignore that, I'm thinking B too now",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 755866,
          "date": "Sun 25 Dec 2022 18:33",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "Answer is D only<br>https://blog.shikisoft.com/restrict-amazon-s3-bucket-access-on-cloudfront/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 753544,
          "date": "Thu 22 Dec 2022 18:56",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The correct answer is B.  Configure Amazon CloudFront to forward all incoming requests to AWS WAF before requesting content from the S3 origin.<br><br>To comply with the security policy, the solutions architect should configure Amazon CloudFront to forward all incoming requests to AWS WAF before requesting content from the S3 origin. This will allow AWS WAF to inspect all website traffic before it is served by CloudFront and S3.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A is incorrect because configuring an S3 bucket policy to accept requests coming from the AWS WAF Amazon Resource Name (ARN) only will not allow CloudFront to forward incoming requests to AWS WAF. <br><br>Option C is incorrect because configuring a security group that allows Amazon CloudFront IP addresses to access Amazon S3 only and associating AWS WAF to CloudFront will not allow AWS WAF to inspect website traffic before it is served by CloudFront and S3.<br><br>Option D is incorrect because configuring Amazon CloudFront and Amazon S3 to use an origin access identity (OAI) to restrict access to the S3 bucket and enabling AWS WAF on the distribution will not allow AWS WAF to inspect website traffic before it is served by CloudFront and S3.</li><li>This can be done by selecting \\\"Yes\\\" for \\\"Viewer Protocol Policy\\\" when creating or updating the CloudFront distribution and selecting \\\"AWS WAF\\\" for \\\"Origin Protocol Policy.\\\" This will ensure that all traffic to the website is inspected by AWS WAF before being served by CloudFront.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 753546,
          "date": "Thu 22 Dec 2022 18:56",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A is incorrect because configuring an S3 bucket policy to accept requests coming from the AWS WAF Amazon Resource Name (ARN) only will not allow CloudFront to forward incoming requests to AWS WAF. <br><br>Option C is incorrect because configuring a security group that allows Amazon CloudFront IP addresses to access Amazon S3 only and associating AWS WAF to CloudFront will not allow AWS WAF to inspect website traffic before it is served by CloudFront and S3.<br><br>Option D is incorrect because configuring Amazon CloudFront and Amazon S3 to use an origin access identity (OAI) to restrict access to the S3 bucket and enabling AWS WAF on the distribution will not allow AWS WAF to inspect website traffic before it is served by CloudFront and S3.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 753547,
          "date": "Thu 22 Dec 2022 18:59",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "This can be done by selecting \\\"Yes\\\" for \\\"Viewer Protocol Policy\\\" when creating or updating the CloudFront distribution and selecting \\\"AWS WAF\\\" for \\\"Origin Protocol Policy.\\\" This will ensure that all traffic to the website is inspected by AWS WAF before being served by CloudFront.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#166",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>Organizers for a global event want to put daily reports online as static HTML pages. The pages are expected to generate millions of views from users around the world. The files are stored in an Amazon S3 bucket. A solutions architect has been asked to design an efficient and effective solution.<br><br>Which action should the solutions architect take to accomplish this?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#166",
          "answers": [
            {
              "choice": "<p>A. Generate presigned URLs for the files.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use cross-Region replication to all Regions.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use the geoproximity feature of Amazon Route 53.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use Amazon CloudFront with the S3 bucket as its origin.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 753553,
          "date": "Thu 22 Dec 2022 19:02",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The most effective and efficient solution would be Option D (Use Amazon CloudFront with the S3 bucket as its origin.) <br><br>Amazon CloudFront is a content delivery network (CDN) that speeds up the delivery of static and dynamic web content, such as HTML pages, images, and videos. By using CloudFront, the HTML pages will be served to users from the edge location that is closest to them, resulting in faster delivery and a better user experience. CloudFront can also handle the high traffic and large number of requests expected for the global event, ensuring that the HTML pages are available and accessible to users around the world.",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 748367,
          "date": "Sat 17 Dec 2022 21:03",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 746065,
          "date": "Thu 15 Dec 2022 13:32",
          "username": "\t\t\t\tk1kavi1\t\t\t",
          "content": "Agreed",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 742411,
          "date": "Mon 12 Dec 2022 05:52",
          "username": "\t\t\t\tSahilbhai\t\t\t",
          "content": "answer is D agree with Shasha1",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 741686,
          "date": "Sun 11 Dec 2022 13:35",
          "username": "\t\t\t\tShasha1\t\t\t",
          "content": "D<br>CloudFront is a content delivery network (CDN) offered by Amazon Web Services (AWS). It functions as a reverse proxy service that caches web content across AWS's global data centers, improving loading speeds and reducing the strain on origin servers. CloudFront can be used to efficiently deliver large amounts of static or dynamic content anywhere in the world.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 724467,
          "date": "Tue 22 Nov 2022 17:28",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "D is correct",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 718862,
          "date": "Tue 15 Nov 2022 15:35",
          "username": "\t\t\t\tNigma\t\t\t",
          "content": "D<br><br>Static content on S3 and hence Cloudfront is the best way",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 718848,
          "date": "Tue 15 Nov 2022 15:27",
          "username": "\t\t\t\tPamban\t\t\t",
          "content": "D is the correct answer",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        }
      ]
    },
    {
      "question_id": "#167",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company runs a production application on a fleet of Amazon EC2 instances. The application reads the data from an Amazon SQS queue and processes the messages in parallel. The message volume is unpredictable and often has intermittent traffic. This application should continually process messages without any downtime.<br><br>Which solution meets these requirements MOST cost-effectively?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#167",
          "answers": [
            {
              "choice": "<p>A. Use Spot Instances exclusively to handle the maximum capacity required.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use Reserved Instances exclusively to handle the maximum capacity required.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use Reserved Instances for the baseline capacity and use Spot Instances to handle additional capacity.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use Reserved Instances for the baseline capacity and use On-Demand Instances to handle additional capacity.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 720240,
          "date": "Thu 17 Nov 2022 06:41",
          "username": "\t\t\t\ttaer\t\t\t",
          "content": "D is the correct answer<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>C is correct, read for cost effectiveness</li></ul>",
          "upvote_count": "11",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 804640,
          "date": "Fri 10 Feb 2023 19:22",
          "username": "\t\t\t\tDrayen25\t\t\t",
          "content": "C is correct, read for cost effectiveness",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 847904,
          "date": "Thu 23 Mar 2023 08:07",
          "username": "\t\t\t\tMssP\t\t\t",
          "content": "\\\"continually process messages without any downtime\\\" is just for making noise, Sopt instances do it when there is a SQS.C is the most cost-effective.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 817579,
          "date": "Wed 22 Feb 2023 08:37",
          "username": "\t\t\t\tTofu13\t\t\t",
          "content": "Key to answering this question is how you think AWS interprets \\\"continually process messages without any downtime\\\".<br>As suggested by the info provided by Alhaz and others, applications can minimize the impact of a Spot Instance interruption.<br>Data will not be lost because another instance will poll the message again. <br>As Reserved Instances are being used for the baseline capacity continuously processing should be ensured (even if slowed down due to Spot Instance interruption).<br>As they want the most cost-effectively solution, C looks right to me.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 816487,
          "date": "Tue 21 Feb 2023 12:28",
          "username": "\t\t\t\tAlhaz\t\t\t",
          "content": "https://aws.amazon.com/blogs/compute/running-cost-effective-queue-workers-with-amazon-sqs-and-amazon-ec2-spot-instances/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 810999,
          "date": "Thu 16 Feb 2023 19:31",
          "username": "\t\t\t\tbdp123\t\t\t",
          "content": "I change my answer to 'C' because of cost and explanation below:<br>https://aws.amazon.com/blogs/compute/running-cost-effective-queue<br>-workers-with-amazon-sqs-and-amazon-ec2-spot-instances/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 808471,
          "date": "Tue 14 Feb 2023 15:41",
          "username": "\t\t\t\tbdp123\t\t\t",
          "content": "We recommend that you use On-Demand Instances for applications with short-term, irregular workloads that cannot be interrupted.<br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-on-demand-instances.html",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 806253,
          "date": "Sun 12 Feb 2023 12:23",
          "username": "\t\t\t\tOuk\t\t\t",
          "content": "Without downtime so On-demand",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 803754,
          "date": "Fri 10 Feb 2023 00:02",
          "username": "\t\t\t\tDeepak_k\t\t\t",
          "content": "Answer : C (Explained clearly when spot instance terminated and what happens to the message in queue)<br>https://aws.amazon.com/blogs/compute/running-cost-effective-queue<br>-workers-with-amazon-sqs-and-amazon-ec2-spot-instances/<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Handling Spot Instance interruptions<br><br>Applications can minimize the impact of a Spot Instance interruption. To do so, an application catches the two-minute interruption notification (available in the instance's metadata), and instructs itself to stop fetching jobs from the queue. If there's an image still being processed when the two minutes expire and the instance is terminated, the application does not delete the message from the queue after finishing the process. Instead, the message simply becomes visible again for another instance to pick up and process after the Amazon SQS visibility timeout expires.<br><br>Alternatively, you can release any ongoing job back to the queue upon receiving a Spot Instance interruption notification by setting the visibility timeout of the specific message to 0. This timeout potentially decreases the total time it takes to process the message.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 803756,
          "date": "Fri 10 Feb 2023 00:04",
          "username": "\t\t\t\tDeepak_k\t\t\t",
          "content": "Handling Spot Instance interruptions<br><br>Applications can minimize the impact of a Spot Instance interruption. To do so, an application catches the two-minute interruption notification (available in the instance's metadata), and instructs itself to stop fetching jobs from the queue. If there's an image still being processed when the two minutes expire and the instance is terminated, the application does not delete the message from the queue after finishing the process. Instead, the message simply becomes visible again for another instance to pick up and process after the Amazon SQS visibility timeout expires.<br><br>Alternatively, you can release any ongoing job back to the queue upon receiving a Spot Instance interruption notification by setting the visibility timeout of the specific message to 0. This timeout potentially decreases the total time it takes to process the message.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 801754,
          "date": "Wed 08 Feb 2023 08:53",
          "username": "\t\t\t\tomarinux\t\t\t",
          "content": "Explanation/Reference: We recommend that you use On-Demand Instances for applications with short-term, irregular workloads thatcannot be interrupted.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 800891,
          "date": "Tue 07 Feb 2023 13:55",
          "username": "\t\t\t\tAndyMartinez\t\t\t",
          "content": "I think the right option is C based on the cost-effectively request.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 796639,
          "date": "Fri 03 Feb 2023 03:21",
          "username": "\t\t\t\tremand\t\t\t",
          "content": "This is the sneaky way of saying processing can be terminated anytime. Because messages can go back to SQS if Spot instance is pulled back, C is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 795904,
          "date": "Thu 02 Feb 2023 10:07",
          "username": "\t\t\t\tLuckyAro\t\t\t",
          "content": "The message volume is unpredictable and often has intermittent traffic = No Baseline period = C % D are incorrect. <br>This application should continually process messages without any downtime = No Spot Instances = A is Incorrect.<br><br>B is the answer = On demand instances due to unpredictable pattern.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 791359,
          "date": "Sun 29 Jan 2023 04:26",
          "username": "\t\t\t\tjoric\t\t\t",
          "content": "c - most cost effective: spot instanes for traffic peaks",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 778356,
          "date": "Mon 16 Jan 2023 23:54",
          "username": "\t\t\t\tLuckyAro\t\t\t",
          "content": "Because cost was not a consideration in the question, I would reluctantly vote D.  Autoscaling group filled with spot instances would have made better architecture due to cost consideration.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>It is the main question. Which solution meets these requirements MOST cost-effectively?</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 781583,
          "date": "Thu 19 Jan 2023 21:39",
          "username": "\t\t\t\tmj61\t\t\t",
          "content": "It is the main question. Which solution meets these requirements MOST cost-effectively?",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 767508,
          "date": "Fri 06 Jan 2023 11:18",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "There is no sensible logical explanation given for D.  The explanations below are all flawed. C is explained accurately",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 765774,
          "date": "Wed 04 Jan 2023 15:53",
          "username": "\t\t\t\taba2s\t\t\t",
          "content": "Arguments from SAA-CO2<br>A.  Nop, we need to process messages without any downtime.B.  It would be a waste to have instances running when there is intermittent traffic.C.  Could be, but we can't use Spot InstancesD.  Sounds about right, even though on-demand is expensive, there can't be any downtime.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 765413,
          "date": "Wed 04 Jan 2023 09:59",
          "username": "\t\t\t\tHayLLlHuK\t\t\t",
          "content": "\\\"without any downtime\\\" - Reserved Instances for the baseline capacity<br>\\\"MOST cost-effectively\\\" - Spot Instances to handle additional capacity<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Dude, read the question, cost consideration was not mentioned in the question.</li><li>Dude, read the question, \\\"Which solution meets these requirements MOST cost-effectively?\\\"</li></ul>",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 778353,
          "date": "Mon 16 Jan 2023 23:52",
          "username": "\t\t\t\tLuckyAro\t\t\t",
          "content": "Dude, read the question, cost consideration was not mentioned in the question.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Dude, read the question, \\\"Which solution meets these requirements MOST cost-effectively?\\\"</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 782531,
          "date": "Fri 20 Jan 2023 17:57",
          "username": "\t\t\t\tShinobiGrappler\t\t\t",
          "content": "Dude, read the question, \\\"Which solution meets these requirements MOST cost-effectively?\\\"",
          "upvote_count": "11",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#168",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A security team wants to limit access to specific services or actions in all of the team's AWS accounts. All accounts belong to a large organization in AWS Organizations. The solution must be scalable and there must be a single point where permissions can be maintained.<br><br>What should a solutions architect do to accomplish this?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#168",
          "answers": [
            {
              "choice": "<p>A. Create an ACL to provide access to the services or actions.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a security group to allow accounts and attach it to user groups.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create cross-account roles in each account to deny access to the services or actions.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create a service control policy in the root organizational unit to deny access to the services or actions.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 718869,
          "date": "Tue 15 Nov 2022 15:41",
          "username": "\t\t\t\tNigma\t\t\t",
          "content": "D.  Service control policies (SCPs) are one type of policy that you can use to manage your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization, allowing you to ensure your accounts stay within your organization's access control guidelines. See https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html.",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 753561,
          "date": "Thu 22 Dec 2022 19:15",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "To limit access to specific services or actions in all of the team's AWS accounts and maintain a single point where permissions can be managed, the solutions architect should create a service control policy (SCP) in the root organizational unit to deny access to the services or actions (Option D).<br><br>Service control policies (SCPs) are policies that you can use to set fine-grained permissions for your AWS accounts within your organization. SCPs are attached to the root of the organizational unit (OU) or to individual accounts, and they specify the permissions that are allowed or denied for the accounts within the scope of the policy. By creating an SCP in the root organizational unit, the security team can set permissions for all of the accounts in the organization from a single location, ensuring that the permissions are consistently applied across all accounts.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 748371,
          "date": "Sat 17 Dec 2022 21:08",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 724469,
          "date": "Tue 22 Nov 2022 17:32",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "D iscorrect",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 719595,
          "date": "Wed 16 Nov 2022 13:01",
          "username": "\t\t\t\tbabaxoxo\t\t\t",
          "content": "an organization and requires single point place to manage permissions",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 718757,
          "date": "Tue 15 Nov 2022 14:40",
          "username": "\t\t\t\tgoatbernard\t\t\t",
          "content": "SCP for organization",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        }
      ]
    },
    {
      "question_id": "#169",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is concerned about the security of its public web application due to recent web attacks. The application uses an Application Load Balancer (ALB). A solutions architect must reduce the risk of DDoS attacks against the application.<br><br>What should the solutions architect do to meet this requirement?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#169",
          "answers": [
            {
              "choice": "<p>A. Add an Amazon Inspector agent to the ALB. <br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Configure Amazon Macie to prevent attacks.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Enable AWS Shield Advanced to prevent attacks.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure Amazon GuardDuty to monitor the ALB. <br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 760158,
          "date": "Wed 28 Dec 2022 18:34",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "Explained in details here https://medium.com/@tshemku/aws-waf-vs-firewall-manager-vs-shield-vs-shield-advanced-4c86911e94c6",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 753562,
          "date": "Thu 22 Dec 2022 19:18",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "To reduce the risk of DDoS attacks against the application, the solutions architect should enable AWS Shield Advanced (Option C).<br><br>AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that helps protect web applications running on AWS from DDoS attacks. AWS Shield Advanced is an additional layer of protection that provides enhanced DDoS protection capabilities, including proactive monitoring and automatic inline mitigations, to help protect against even the largest and most sophisticated DDoS attacks. By enabling AWS Shield Advanced, the solutions architect can help protect the application from DDoS attacks and reduce the risk of disruption to the application.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 748374,
          "date": "Sat 17 Dec 2022 21:09",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "C is right answer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 724470,
          "date": "Tue 22 Nov 2022 17:32",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 719189,
          "date": "Wed 16 Nov 2022 00:51",
          "username": "\t\t\t\tgoatbernard\t\t\t",
          "content": "AWS Shield Advanced",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 718871,
          "date": "Tue 15 Nov 2022 15:48",
          "username": "\t\t\t\tNigma\t\t\t",
          "content": "DDOS = AWS Shield",
          "upvote_count": "4",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#170",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company's web application is running on Amazon EC2 instances behind an Application Load Balancer. The company recently changed its policy, which now requires the application to be accessed from one specific country only.<br><br>Which configuration will meet this requirement?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#170",
          "answers": [
            {
              "choice": "<p>A. Configure the security group for the EC2 instances.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Configure the security group on the Application Load Balancer.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Configure AWS WAF on the Application Load Balancer in a VPC. <br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure the network ACL for the subnet that contains the EC2 instances.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 723531,
          "date": "Mon 21 Nov 2022 14:17",
          "username": "\t\t\t\thandyplazt\t\t\t",
          "content": "Geographic (Geo) Match Conditions in AWS WAF.  This new condition type allows you to use AWS WAF to restrict application access based on the geographic location of your viewers. With geo match conditions you can choose the countries from which AWS WAF should allow access. <br>https://aws.amazon.com/about-aws/whats-new/2017/10/aws-waf-now-supports-geographic-match/",
          "upvote_count": "11",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 769629,
          "date": "Sun 08 Jan 2023 17:37",
          "username": "\t\t\t\taba2s\t\t\t",
          "content": "Source from an AWS link<br>Geographic (Geo) Match Conditions in AWS WAF.  This condition type allows you to use AWS WAF to restrict application access based on the geographic location of your viewers.<br>With geo match conditions you can choose the countries from which AWS WAF should allow access.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 755875,
          "date": "Sun 25 Dec 2022 18:49",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "WAF Shield Advanced for DDOS,<br>GuardDuty is a continuous monitoring service that alerts you of potential threats, while Inspector is a one-time assessment service that provides a report of vulnerabilities and deviations from best practices.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 753564,
          "date": "Thu 22 Dec 2022 19:21",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "To meet the requirement of allowing the web application to be accessed from one specific country only, the company should configure AWS WAF (Web Application Firewall) on the Application Load Balancer in a VPC (Option C).<br><br>AWS WAF is a web application firewall service that helps protect web applications from common web exploits that could affect application availability, compromise security, or consume excessive resources. AWS WAF allows you to create rules that block or allow traffic based on the values of specific request parameters, such as IP address, HTTP header, or query string value. By configuring AWS WAF on the Application Load Balancer and creating rules that allow traffic from a specific country, the company can ensure that the web application is only accessible from that country.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 748324,
          "date": "Sat 17 Dec 2022 20:16",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "OptionC.  Configure WAF for Geo Match Policy",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 724471,
          "date": "Tue 22 Nov 2022 17:33",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 719605,
          "date": "Wed 16 Nov 2022 13:22",
          "username": "\t\t\t\tmricee9\t\t\t",
          "content": "C<br>https://aws.amazon.com/about-aws/whats-new/2017/10/aws-waf-now-supports-geographic-match/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 718880,
          "date": "Tue 15 Nov 2022 15:53",
          "username": "\t\t\t\tNigma\t\t\t",
          "content": "C.  WAF with ALB is the right option",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#171",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company provides an API to its users that automates inquiries for tax computations based on item prices. The company experiences a larger number of inquiries during the holiday season only that cause slower response times. A solutions architect needs to design a solution that is scalable and elastic.<br><br>What should the solutions architect do to accomplish this?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#171",
          "answers": [
            {
              "choice": "<p>A. Provide an API hosted on an Amazon EC2 instance. The EC2 instance performs the required computations when the API request is made.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Design a REST API using Amazon API Gateway that accepts the item names. API Gateway passes item names to AWS Lambda for tax computations.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an Application Load Balancer that has two Amazon EC2 instances behind it. The EC2 instances will compute the tax on the received item names.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Design a REST API using Amazon API Gateway that connects with an API hosted on an Amazon EC2 instance. API Gateway accepts and passes the item names to the EC2 instance for tax computations.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 784413,
          "date": "Sun 22 Jan 2023 16:02",
          "username": "\t\t\t\tbullrem\t\t\t",
          "content": "Option D is similar to option B in that it uses Amazon API Gateway to handle the API requests, but it also includes an EC2 instance to perform the tax computations. However, using an EC2 instance in this way is less scalable and less elastic than using AWS Lambda to perform the computations. An EC2 instance is a fixed resource and requires manual scaling and management, while Lambda is an event-driven, serverless compute service that automatically scales with the number of requests, making it more suitable for handling variable workloads and reducing response times during high traffic periods. Additionally, Lambda is more cost-efficient than EC2 instances, as you only pay for the compute time consumed by your functions, making it a more cost-effective solution.",
          "upvote_count": "6",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 791490,
          "date": "Sun 29 Jan 2023 10:16",
          "username": "\t\t\t\tProfXsamson\t\t\t",
          "content": "B.  Serverless option wins over EC2",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 754749,
          "date": "Sat 24 Dec 2022 08:18",
          "username": "\t\t\t\tsona21\t\t\t",
          "content": "Lambda is serverless is scalable so answer should be B. ",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 753568,
          "date": "Thu 22 Dec 2022 19:26",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "To design a scalable and elastic solution for providing an API for tax computations, the solutions architect should design a REST API using Amazon API Gateway that connects with an API hosted on an Amazon EC2 instance (Option D).<br><br>API Gateway is a fully managed service that makes it easy to create, publish, maintain, monitor, and secure APIs at any scale. By designing a REST API using API Gateway, the solutions architect can create an API that is scalable, flexible, and easy to use. The API Gateway can accept and pass the item names to the EC2 instance for tax computations, and the EC2 instance can perform the required computations when the API request is made.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A (providing an API hosted on an EC2 instance) would not be a suitable solution as it may not be scalable or elastic enough to handle the increased demand during the holiday season.<br><br>Option B (designing a REST API using API Gateway that passes item names to Lambda for tax computations) would not be a suitable solution as it may not be suitable for computations that require a larger amount of resources or longer execution times.<br><br>Option C (creating an Application Load Balancer with two EC2 instances behind it) would not be a suitable solution as it may not provide the necessary scalability and elasticity. Additionally, it would not provide the benefits of using API Gateway, such as API management and monitoring capabilities.</li><li>But Option D is not scalable. The requirements state \\\"A solutions architect needs to design a solution that is scalable and elastic\\\". D fails to meet these requirements. C on the other hand is scalable. There is nothing in the question to suggest that a longer execution than lambda can handle happens. Therefore D is wrong, and C is possible.</li><li>Sorry, it should say \\\"Therefore D is wrong, and B is possible.\\\"</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 753569,
          "date": "Thu 22 Dec 2022 19:26",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A (providing an API hosted on an EC2 instance) would not be a suitable solution as it may not be scalable or elastic enough to handle the increased demand during the holiday season.<br><br>Option B (designing a REST API using API Gateway that passes item names to Lambda for tax computations) would not be a suitable solution as it may not be suitable for computations that require a larger amount of resources or longer execution times.<br><br>Option C (creating an Application Load Balancer with two EC2 instances behind it) would not be a suitable solution as it may not provide the necessary scalability and elasticity. Additionally, it would not provide the benefits of using API Gateway, such as API management and monitoring capabilities.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>But Option D is not scalable. The requirements state \\\"A solutions architect needs to design a solution that is scalable and elastic\\\". D fails to meet these requirements. C on the other hand is scalable. There is nothing in the question to suggest that a longer execution than lambda can handle happens. Therefore D is wrong, and C is possible.</li><li>Sorry, it should say \\\"Therefore D is wrong, and B is possible.\\\"</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 767668,
          "date": "Fri 06 Jan 2023 13:42",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "But Option D is not scalable. The requirements state \\\"A solutions architect needs to design a solution that is scalable and elastic\\\". D fails to meet these requirements. C on the other hand is scalable. There is nothing in the question to suggest that a longer execution than lambda can handle happens. Therefore D is wrong, and C is possible.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Sorry, it should say \\\"Therefore D is wrong, and B is possible.\\\"</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 767670,
          "date": "Fri 06 Jan 2023 13:42",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "Sorry, it should say \\\"Therefore D is wrong, and B is possible.\\\"",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 750483,
          "date": "Tue 20 Dec 2022 05:08",
          "username": "\t\t\t\tBENICE\t\t\t",
          "content": "B is the option",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 748326,
          "date": "Sat 17 Dec 2022 20:20",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option B.  Though D is also possible B is more scalable as Lambda will autoscale to meet the dynamic load.",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 735045,
          "date": "Sun 04 Dec 2022 12:47",
          "username": "\t\t\t\tGil80\t\t\t",
          "content": "B.  Lambda scales much better",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 729516,
          "date": "Mon 28 Nov 2022 19:22",
          "username": "\t\t\t\tKapello10\t\t\t",
          "content": "B is the correct ans",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 729225,
          "date": "Mon 28 Nov 2022 15:32",
          "username": "\t\t\t\tGabs90\t\t\t",
          "content": "B is correct, lamba is a better choice",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 726819,
          "date": "Fri 25 Nov 2022 14:56",
          "username": "\t\t\t\tVISHNUKANDH\t\t\t",
          "content": "B is the right answer",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 724478,
          "date": "Tue 22 Nov 2022 17:38",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "B is correct",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 720371,
          "date": "Thu 17 Nov 2022 10:41",
          "username": "\t\t\t\tBENICE\t\t\t",
          "content": "Seems like B is the correct option",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 719190,
          "date": "Wed 16 Nov 2022 00:53",
          "username": "\t\t\t\tgoatbernard\t\t\t",
          "content": "Lambda",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 718904,
          "date": "Tue 15 Nov 2022 16:22",
          "username": "\t\t\t\tsdasdawa\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/35849-exam-aws-certified-solutions-architect-associate-saa-c02/",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 718892,
          "date": "Tue 15 Nov 2022 16:08",
          "username": "\t\t\t\tOhnet\t\t\t",
          "content": "It should be B,Lambda server-less is scalable and elastic than EC2 api gateway solution",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 718885,
          "date": "Tue 15 Nov 2022 15:56",
          "username": "\t\t\t\tNigma\t\t\t",
          "content": "B.  Lambda serverless is scalable and elastic than EC2 api gateway solution",
          "upvote_count": "4",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#172",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A solutions architect is creating a new Amazon CloudFront distribution for an application. Some of the information submitted by users is sensitive. The application uses HTTPS but needs another layer of security. The sensitive information should.be protected throughout the entire application stack, and access to the information should be restricted to certain applications.<br><br>Which action should the solutions architect take?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#172",
          "answers": [
            {
              "choice": "<p>A. Configure a CloudFront signed URL.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Configure a CloudFront signed cookie.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Configure a CloudFront field-level encryption profile.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure CloudFront and set the Origin Protocol Policy setting to HTTPS Only for the Viewer Protocol Policy.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 724130,
          "date": "Tue 22 Nov 2022 07:52",
          "username": "\t\t\t\tBobbybash\t\t\t",
          "content": "CCCCCCCCC<br>Field-level encryption allows you to enable your users to securely upload sensitive information to your web servers. The sensitive information provided by your users is encrypted at the edge, close to the user, and remains encrypted throughout your entire application stack. This encryption ensures that only applications that need the dataand have the credentials to decrypt itare able to do so.",
          "upvote_count": "22",
          "selected_answers": ""
        },
        {
          "id": 831574,
          "date": "Tue 07 Mar 2023 06:11",
          "username": "\t\t\t\tWherecanIstart\t\t\t",
          "content": "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-encryption.html<br><br>\\\"Field-level encryption allows you to enable your users to securely upload sensitive information to your web servers. The sensitive information provided by your users is encrypted at the edge, close to the user, and remains encrypted throughout your entire application stack\\\".",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 795237,
          "date": "Wed 01 Feb 2023 16:01",
          "username": "\t\t\t\tbdp123\t\t\t",
          "content": "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-levelencryption.<br>html<br>\\\"With Amazon CloudFront, you can enforce secure end-to-end connections to origin servers by using<br>HTTPS. Field-level encryption adds an additional layer of security that lets you protect specific data<br>throughout system processing so that only certain applications can see it.\\\"",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 791506,
          "date": "Sun 29 Jan 2023 10:26",
          "username": "\t\t\t\tProfXsamson\t\t\t",
          "content": "C, field-level encryption should be used when necessary to protect sensitive data.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 772176,
          "date": "Wed 11 Jan 2023 08:28",
          "username": "\t\t\t\tayanshbhaiji\t\t\t",
          "content": "It should be C",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 765701,
          "date": "Wed 04 Jan 2023 14:38",
          "username": "\t\t\t\tHayLLlHuK\t\t\t",
          "content": "C!<br>CloudFront's field-level encryption further encrypts sensitive data in an HTTPS form using field-specific encryption keys (which you supply) before a POST request is forwarded to your origin. This ensures that sensitive data can only be decrypted and viewed by certain components or services in your application stack.<br>https://aws.amazon.com/about-aws/whats-new/2017/12/introducing-field-level-encryption-on-amazon-cloudfront/",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 765304,
          "date": "Wed 04 Jan 2023 07:06",
          "username": "\t\t\t\tkbaruu\t\t\t",
          "content": "Field-Level Encryption allows you to securely upload user-submitted sensitive information to your web servers.x Signed cookie - provides access to download multiple private files (from Tutorial Dojo)",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 763783,
          "date": "Mon 02 Jan 2023 15:28",
          "username": "\t\t\t\tMindvision\t\t\t",
          "content": "C = Answer<br><br>I concur. why? CloudFront's field-level encryption further encrypts sensitive data in an HTTPS form using field-specific encryption keys (which you supply) before a POST request is forwarded to your origin. This ensures that sensitive data can only be decrypted and viewed by certain components or services in your application stack.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 763466,
          "date": "Mon 02 Jan 2023 03:35",
          "username": "\t\t\t\tZerotn3\t\t\t",
          "content": "he correct answer is B.  Configure a CloudFront signed cookie.<br><br>CloudFront signed cookies can be used to protect sensitive information by requiring users to authenticate with a signed cookie before they can access content that is served through CloudFront. This can be used to restrict access to certain applications and ensure that the sensitive information is protected throughout the entire application stack.<br><br>Option A, Configure a CloudFront signed URL, would also provide an additional layer of security by requiring users to authenticate with a signed URL before they can access content served through CloudFront. However, this option would not protect the sensitive information throughout the entire application stack.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option C, Configure a CloudFront field-level encryption profile, can be used to protect sensitive information that is stored in Amazon S3 and served through CloudFront. However, this option would not provide an additional layer of security for the entire application stack.</li><li>CloudFront signed cookie are used to control user access to sensitive documents but that is not what is required. \\\"Some of the information submitted by users is sensitive\\\" This is what you are looking to protect, when it's in the system, (not when users are trying to access it and this is not mentioned in the Q).<br>Field-level encryption encrypts sensitive data ... This ensures sensitive data can only be decrypted and viewed by certain components or services. (q states \\\"access to the information should be restricted to certain applications.\\\"), so C is a perfect match</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 763467,
          "date": "Mon 02 Jan 2023 03:36",
          "username": "\t\t\t\tZerotn3\t\t\t",
          "content": "Option C, Configure a CloudFront field-level encryption profile, can be used to protect sensitive information that is stored in Amazon S3 and served through CloudFront. However, this option would not provide an additional layer of security for the entire application stack.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>CloudFront signed cookie are used to control user access to sensitive documents but that is not what is required. \\\"Some of the information submitted by users is sensitive\\\" This is what you are looking to protect, when it's in the system, (not when users are trying to access it and this is not mentioned in the Q).<br>Field-level encryption encrypts sensitive data ... This ensures sensitive data can only be decrypted and viewed by certain components or services. (q states \\\"access to the information should be restricted to certain applications.\\\"), so C is a perfect match</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 767680,
          "date": "Fri 06 Jan 2023 13:50",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "CloudFront signed cookie are used to control user access to sensitive documents but that is not what is required. \\\"Some of the information submitted by users is sensitive\\\" This is what you are looking to protect, when it's in the system, (not when users are trying to access it and this is not mentioned in the Q).<br>Field-level encryption encrypts sensitive data ... This ensures sensitive data can only be decrypted and viewed by certain components or services. (q states \\\"access to the information should be restricted to certain applications.\\\"), so C is a perfect match",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 757818,
          "date": "Mon 26 Dec 2022 21:26",
          "username": "\t\t\t\tmuhtoy\t\t\t",
          "content": "configuring a CloudFront signed cookie is a better solution for protecting sensitive information and restricting access to certain applications throughout the entire application stack, This will allow them to restrict access to content based on the viewer's identity and ensure that the sensitive information is protected throughout the entire application stack",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 755887,
          "date": "Sun 25 Dec 2022 19:04",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "Option B, \\\"Configure a CloudFront signed cookie,\\\" is not a suitable solution for this scenario because signed cookies are used to grant temporary access to specific content in your CloudFront distribution. They do not provide an additional layer of security for the sensitive information submitted by users, nor do they allow you to restrict access to certain applications.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 754677,
          "date": "Sat 24 Dec 2022 04:16",
          "username": "\t\t\t\tNV305\t\t\t",
          "content": "Field-level encryption profiles, which you create in CloudFront, define the fields that you want to be encrypted.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 754674,
          "date": "Sat 24 Dec 2022 04:11",
          "username": "\t\t\t\tNV305\t\t\t",
          "content": "Use signed URLs in the following cases:<br><br>You want to restrict access to individual files, for example, an installation download for your application.<br><br>Your users are using a client (for example, a custom HTTP client) that doesn't support cookies.<br><br>Use signed cookies in the following cases:<br><br>You want to provide access to multiple restricted files, for example, all of the files for a video in HLS format or all of the files in the subscribers' area of website.<br><br>You don't want to change your current URLs.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 753576,
          "date": "Thu 22 Dec 2022 19:31",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "To protect sensitive information throughout the entire application stack and restrict access to certain applications, the solutions architect should configure a CloudFront signed cookie (Option B).<br><br>CloudFront signed cookies are a feature of CloudFront that allows you to limit access to content in your distribution by requiring users to present a valid cookie with a signed value. By creating a signed cookie and requiring users to present the cookie in order to access the content, you can restrict access to the content to only those users who have a valid cookie. This can help protect sensitive information throughout the entire application stack and ensure that only authorized applications have access to the information.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Field-level encryption profiles, which you create in CloudFront, define the fields that you want to be encrypted.</li><li>Option A (configuring a CloudFront signed URL) would not be a suitable solution as signed URLs are temporary URLs that allow users to access specific objects in an S3 bucket or a custom origin without requiring AWS credentials. While signed URLs can be useful for providing limited and secure access to specific objects, they are not designed for protecting content throughout the entire application stack or for restricting access to certain applications.<br><br>Option C (configuring a CloudFront field-level encryption profile) would not be a suitable solution as field-level encryption is a feature of CloudFront that allows you to encrypt specific fields in an HTTP request or response, rather than the entire content. While field-level encryption can be useful for protecting specific fields of sensitive information, it is not designed for protecting the entire content or for restricting access to certain applications.</li><li>You are not told that the entire content requires protection, just some sensitive information. <br>And yes \\\"Field-level encryption ensures ... sensitive data can only be decrypted and viewed by certain components or services\\\" so does achieve the requirements.</li><li>Option D (configuring CloudFront and setting the Origin Protocol Policy setting to HTTPS Only for the Viewer Protocol Policy) would not be a suitable solution as the Origin Protocol Policy setting determines whether CloudFront sends HTTP or HTTPS requests to the origin, rather than protecting the content or restricting access to certain applications.</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 755893,
          "date": "Sun 25 Dec 2022 19:09",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "Field-level encryption profiles, which you create in CloudFront, define the fields that you want to be encrypted.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 753578,
          "date": "Thu 22 Dec 2022 19:33",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A (configuring a CloudFront signed URL) would not be a suitable solution as signed URLs are temporary URLs that allow users to access specific objects in an S3 bucket or a custom origin without requiring AWS credentials. While signed URLs can be useful for providing limited and secure access to specific objects, they are not designed for protecting content throughout the entire application stack or for restricting access to certain applications.<br><br>Option C (configuring a CloudFront field-level encryption profile) would not be a suitable solution as field-level encryption is a feature of CloudFront that allows you to encrypt specific fields in an HTTP request or response, rather than the entire content. While field-level encryption can be useful for protecting specific fields of sensitive information, it is not designed for protecting the entire content or for restricting access to certain applications.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>You are not told that the entire content requires protection, just some sensitive information. <br>And yes \\\"Field-level encryption ensures ... sensitive data can only be decrypted and viewed by certain components or services\\\" so does achieve the requirements.</li><li>Option D (configuring CloudFront and setting the Origin Protocol Policy setting to HTTPS Only for the Viewer Protocol Policy) would not be a suitable solution as the Origin Protocol Policy setting determines whether CloudFront sends HTTP or HTTPS requests to the origin, rather than protecting the content or restricting access to certain applications.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 767685,
          "date": "Fri 06 Jan 2023 13:52",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "You are not told that the entire content requires protection, just some sensitive information. <br>And yes \\\"Field-level encryption ensures ... sensitive data can only be decrypted and viewed by certain components or services\\\" so does achieve the requirements.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 753579,
          "date": "Thu 22 Dec 2022 19:33",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option D (configuring CloudFront and setting the Origin Protocol Policy setting to HTTPS Only for the Viewer Protocol Policy) would not be a suitable solution as the Origin Protocol Policy setting determines whether CloudFront sends HTTP or HTTPS requests to the origin, rather than protecting the content or restricting access to certain applications.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 750482,
          "date": "Tue 20 Dec 2022 05:07",
          "username": "\t\t\t\tBENICE\t\t\t",
          "content": "Cis the option",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 748333,
          "date": "Sat 17 Dec 2022 20:26",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 743572,
          "date": "Tue 13 Dec 2022 04:48",
          "username": "\t\t\t\tQjb8m9h\t\t\t",
          "content": "Answer is : C<br>Field-level encryption allows you to enable your users to securely upload sensitive information to your web servers. The sensitive information provided by your users is encrypted at the edge, close to the user, and remains encrypted throughout your entire application stack. This encryption ensures that only applications that need the dataand have the credentials to decrypt itare able to do so.<br><br>https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-encryption.html",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#173",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A gaming company hosts a browser-based application on AWS. The users of the application consume a large number of videos and images that are stored in Amazon S3. This content is the same for all users.<br><br>The application has increased in popularity, and millions of users worldwide accessing these media files. The company wants to provide the files to the users while reducing the load on the origin.<br><br>Which solution meets these requirements MOST cost-effectively?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#173",
          "answers": [
            {
              "choice": "<p>A. Deploy an AWS Global Accelerator accelerator in front of the web servers.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Deploy an Amazon CloudFront web distribution in front of the S3 bucket.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Deploy an Amazon ElastiCache for Redis instance in front of the web servers.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Deploy an Amazon ElastiCache for Memcached instance in front of the web servers.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 718899,
          "date": "Tue 15 Nov 2022 16:16",
          "username": "\t\t\t\tNigma\t\t\t",
          "content": "B.  Cloud front is best for content delivery. Global Accelerator is best for non-HTTP (TCP/UDP) cases and supports HTTP cases as well but with static IP (elastic IP) or anycast IP address only.",
          "upvote_count": "15",
          "selected_answers": ""
        },
        {
          "id": 778392,
          "date": "Tue 17 Jan 2023 00:52",
          "username": "\t\t\t\tLuckyAro\t\t\t",
          "content": "The company wants to provide the files to the users while reducing the load on the origin.<br>Cloudfront speeds-up content delivery but I'm not sure it reduces the load on the origin.<br>Some form of caching would cache content and deliver to users without going to the origin for each request.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 753584,
          "date": "Thu 22 Dec 2022 19:53",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "To provide media files to users while reducing the load on the origin and meeting the requirements cost-effectively, the gaming company should deploy an Amazon CloudFront web distribution in front of the S3 bucket (Option B).<br><br>CloudFront is a content delivery network (CDN) that speeds up the delivery of static and dynamic web content, such as images and videos, to users. By using CloudFront, the media files will be served to users from the edge location that is closest to them, resulting in faster delivery and a better user experience. CloudFront can also handle the high traffic and large number of requests expected from the millions of users, ensuring that the media files are available and accessible to users around the world.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Please dont post ChatGPT answers here,chatgpt keeps on changing its answers,its not the right way to copy paste,thanks.</li><li>why not? if the answers are correct and offer best possible explanation for the wrong options, I see no reason why it shouldn't be posted here. Also, most of his answers were right, although reasons for the wrong options were sometimes lacking, but all in all, his responses were very good.</li><li>Woaaaa! I always wondered where this kind of logic and explanation came from in this guy's answers. Nice catch TECHHB!</li><li>Answers are mostly correct. Only a small percentage were wrong</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 755890,
          "date": "Sun 25 Dec 2022 19:07",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "Please dont post ChatGPT answers here,chatgpt keeps on changing its answers,its not the right way to copy paste,thanks.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>why not? if the answers are correct and offer best possible explanation for the wrong options, I see no reason why it shouldn't be posted here. Also, most of his answers were right, although reasons for the wrong options were sometimes lacking, but all in all, his responses were very good.</li><li>Woaaaa! I always wondered where this kind of logic and explanation came from in this guy's answers. Nice catch TECHHB!</li><li>Answers are mostly correct. Only a small percentage were wrong</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 825558,
          "date": "Wed 01 Mar 2023 07:22",
          "username": "\t\t\t\tBofi\t\t\t",
          "content": "why not? if the answers are correct and offer best possible explanation for the wrong options, I see no reason why it shouldn't be posted here. Also, most of his answers were right, although reasons for the wrong options were sometimes lacking, but all in all, his responses were very good.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 793729,
          "date": "Tue 31 Jan 2023 07:36",
          "username": "\t\t\t\tocbn3wby\t\t\t",
          "content": "Woaaaa! I always wondered where this kind of logic and explanation came from in this guy's answers. Nice catch TECHHB!",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 793730,
          "date": "Tue 31 Jan 2023 07:37",
          "username": "\t\t\t\tocbn3wby\t\t\t",
          "content": "Answers are mostly correct. Only a small percentage were wrong",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 748334,
          "date": "Sat 17 Dec 2022 20:29",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 746073,
          "date": "Thu 15 Dec 2022 13:47",
          "username": "\t\t\t\tk1kavi1\t\t\t",
          "content": "Agreed",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 727252,
          "date": "Sat 26 Nov 2022 05:14",
          "username": "\t\t\t\trewdboy\t\t\t",
          "content": "B is the correct answer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 724481,
          "date": "Tue 22 Nov 2022 17:39",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#174",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has a multi-tier application that runs six front-end web servers in an Amazon EC2 Auto Scaling group in a single Availability Zone behind an Application Load Balancer (ALB). A solutions architect needs to modify the infrastructure to be highly available without modifying the application.<br><br>Which architecture should the solutions architect choose that provides high availability?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#174",
          "answers": [
            {
              "choice": "<p>A. Create an Auto Scaling group that uses three instances across each of two Regions.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Modify the Auto Scaling group to use three instances across each of two Availability Zones.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an Auto Scaling template that can be used to quickly create more instances in another Region.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Change the ALB in front of the Amazon EC2 instances in a round-robin configuration to balance traffic to the web tier.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 718902,
          "date": "Tue 15 Nov 2022 16:19",
          "username": "\t\t\t\tNigma\t\t\t",
          "content": "B.  auto scaling groups can not span multi region",
          "upvote_count": "15",
          "selected_answers": ""
        },
        {
          "id": 756091,
          "date": "Mon 26 Dec 2022 00:40",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "B.  auto scaling groups cannot span multi region",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 753590,
          "date": "Thu 22 Dec 2022 20:07",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option B.  Modify the Auto Scaling group to use three instances across each of the two Availability Zones.<br><br>This option would provide high availability by distributing the front-end web servers across multiple Availability Zones. If there is an issue with one Availability Zone, the other Availability Zone would still be available to serve traffic. This would ensure that the application remains available and highly available even if there is a failure in one of the Availability Zones.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 748338,
          "date": "Sat 17 Dec 2022 20:31",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 746076,
          "date": "Thu 15 Dec 2022 13:49",
          "username": "\t\t\t\tk1kavi1\t\t\t",
          "content": "Agreed",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 741779,
          "date": "Sun 11 Dec 2022 15:23",
          "username": "\t\t\t\tShasha1\t\t\t",
          "content": "B<br>option B This architecture provides high availability by having multiple Availability Zones hosting the same application. This allows for redundancy in case one Availability Zone experiences downtime, as traffic can be served by the other Availability Zone. This solution also increases scalability and performance by allowing traffic to be spread across two Availability Zones.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 726978,
          "date": "Fri 25 Nov 2022 18:20",
          "username": "\t\t\t\tmricee9\t\t\t",
          "content": "B is rightt",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 724484,
          "date": "Tue 22 Nov 2022 17:41",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 721976,
          "date": "Sat 19 Nov 2022 13:08",
          "username": "\t\t\t\txua81376\t\t\t",
          "content": "Bauto scaling i multiple AZ",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#175",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>An ecommerce company has an order-processing application that uses Amazon API Gateway and an AWS Lambda function. The application stores data in an Amazon Aurora PostgreSQL database. During a recent sales event, a sudden surge in customer orders occurred. Some customers experienced timeouts, and the application did not process the orders of those customers.<br><br>A solutions architect determined that the CPU utilization and memory utilization were high on the database because of a large number of open connections. The solutions architect needs to prevent the timeout errors while making the least possible changes to the application.<br><br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#175",
          "answers": [
            {
              "choice": "<p>A. Configure provisioned concurrency for the Lambda function. Modify the database to be a global database in multiple AWS Regions.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use Amazon RDS Proxy to create a proxy for the database. Modify the Lambda function to use the RDS Proxy endpoint instead of the database endpoint.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a read replica for the database in a different AWS Region. Use query string parameters in API Gateway to route traffic to the read replica.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Migrate the data from Aurora PostgreSQL to Amazon DynamoDB by using AWS Database Migration Service (AWS DMS). Modify the Lambda function to use the DynamoDB table.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 723450,
          "date": "Mon 21 Nov 2022 13:10",
          "username": "\t\t\t\thandyplazt\t\t\t",
          "content": "Many applications, including those built on modern serverless architectures, can have a large number of open connections to the database server and may open and close database connections at a high rate, exhausting database memory and compute resources. Amazon RDS Proxy allows applications to pool and share connections established with the database, improving database efficiency and application scalability.<br>https://aws.amazon.com/id/rds/proxy/",
          "upvote_count": "16",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 719598,
          "date": "Wed 16 Nov 2022 13:08",
          "username": "\t\t\t\tbabaxoxo\t\t\t",
          "content": "Issue related to opening many connections and the solution requires least code changes so B satisfies the conditions",
          "upvote_count": "5",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 778362,
          "date": "Tue 17 Jan 2023 00:08",
          "username": "\t\t\t\tsairam\t\t\t",
          "content": "I also think the answer is B.  However can RDS Proxy be used with Amazon Aurora PostgreSQL database?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>RDS Proxy can be used with Aurora<br>https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-proxy.html</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 824548,
          "date": "Tue 28 Feb 2023 09:21",
          "username": "\t\t\t\teverfly\t\t\t",
          "content": "RDS Proxy can be used with Aurora<br>https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-proxy.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 763934,
          "date": "Mon 02 Jan 2023 20:27",
          "username": "\t\t\t\tgustavtd\t\t\t",
          "content": "I expect a answer with database replica but there is not, so B is most suitable",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 753645,
          "date": "Thu 22 Dec 2022 22:25",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option B.  Use Amazon RDS Proxy to create a proxy for the database. Modify the Lambda function to use the RDS Proxy endpoint instead of the database endpoint.<br><br>Using Amazon RDS Proxy can help reduce the number of connections to the database and improve the performance of the application. RDS Proxy establishes a connection pool to the database and routes connections to the available connections in the pool. This can help reduce the number of open connections to the database and improve the performance of the application. The Lambda function can be modified to use the RDS Proxy endpoint instead of the database endpoint to take advantage of this improvement.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A is not a valid solution because configuring provisioned concurrency for the Lambda function does not address the issue of high CPU utilization and memory utilization on the database. <br><br>Option C is not a valid solution because creating a read replica in a different Region does not address the issue of high CPU utilization and memory utilization on the database. <br><br>Option D is not a valid solution because migrating the data from Aurora PostgreSQL to DynamoDB would require significant changes to the application and may not be the best solution for this particular problem.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 753648,
          "date": "Thu 22 Dec 2022 22:26",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A is not a valid solution because configuring provisioned concurrency for the Lambda function does not address the issue of high CPU utilization and memory utilization on the database. <br><br>Option C is not a valid solution because creating a read replica in a different Region does not address the issue of high CPU utilization and memory utilization on the database. <br><br>Option D is not a valid solution because migrating the data from Aurora PostgreSQL to DynamoDB would require significant changes to the application and may not be the best solution for this particular problem.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 750481,
          "date": "Tue 20 Dec 2022 05:07",
          "username": "\t\t\t\tBENICE\t\t\t",
          "content": "Option --- B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 748340,
          "date": "Sat 17 Dec 2022 20:34",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "As it is mentioned that issue was due to high CPU and Memory due to many open corrections to DB, B is the right answer.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 741797,
          "date": "Sun 11 Dec 2022 15:50",
          "username": "\t\t\t\tShasha1\t\t\t",
          "content": "B<br>Using Amazon RDS Proxy will allow the application to handle more connections and higher loads without timeouts, while making the least possible changes to the application. The RDS Proxy will enable connection pooling, allowing multiple connections from the Lambda function to be served from a single proxy connection. This will reduce the number of open connections on the database, which is causing high CPU and memory utilization",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 724487,
          "date": "Tue 22 Nov 2022 17:43",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 721974,
          "date": "Sat 19 Nov 2022 13:07",
          "username": "\t\t\t\txua81376\t\t\t",
          "content": "B - Proxy to manage connections",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 718905,
          "date": "Tue 15 Nov 2022 16:24",
          "username": "\t\t\t\tNigma\t\t\t",
          "content": "Correct B",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#176",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>An application runs on Amazon EC2 instances in private subnets. The application needs to access an Amazon DynamoDB table.<br><br>What is the MOST secure way to access the table while ensuring that the traffic does not leave the AWS network?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#176",
          "answers": [
            {
              "choice": "<p>A. Use a VPC endpoint for DynamoDB. <br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use a NAT gateway in a public subnet.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use a NAT instance in a private subnet.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use the internet gateway attached to the VPC. <br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 719145,
          "date": "Tue 15 Nov 2022 22:26",
          "username": "\t\t\t\tmabotega\t\t\t",
          "content": "VPC endpoints for service in private subnets<br>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-dynamodb.html",
          "upvote_count": "7",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 835234,
          "date": "Fri 10 Mar 2023 17:40",
          "username": "\t\t\t\tGalileoEC2\t\t\t",
          "content": "A<br>The most secure way to access an Amazon DynamoDB table from Amazon EC2 instances in private subnets while ensuring that the traffic does not leave the AWS network is to use Amazon VPC Endpoints for DynamoDB. <br><br>Amazon VPC Endpoints enable private communication between Amazon EC2 instances in a VPC and Amazon services such as DynamoDB, without the need for an internet gateway, NAT device, or VPN connection. When you create a VPC endpoint for DynamoDB, traffic from the EC2 instances to the DynamoDB table remains within the AWS network and does not traverse the public internet.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 808017,
          "date": "Tue 14 Feb 2023 04:03",
          "username": "\t\t\t\tAllGOD\t\t\t",
          "content": "private...backend Answer A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 795241,
          "date": "Wed 01 Feb 2023 16:08",
          "username": "\t\t\t\tbdp123\t\t\t",
          "content": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpointsdynamodb.<br>html A VPC endpoint for DynamoDB enables Amazon EC2 instances in your VPC to use<br>their private IP addresses to access DynamoDB with no exposure to the public internet. Your EC2<br>instances do not require public IP addresses, and you don't need an internet gateway, a NAT device,<br>or a virtual private gateway in your VPC.  You use endpoint policies to control access to DynamoDB. <br>Traffic between your VPC and the AWS service does not leave the Amazon network.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 791525,
          "date": "Sun 29 Jan 2023 10:46",
          "username": "\t\t\t\tProfXsamson\t\t\t",
          "content": "ExamTopics.com should be sued for this answer tagged as Correct answer.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 761979,
          "date": "Fri 30 Dec 2022 14:19",
          "username": "\t\t\t\tmp165\t\t\t",
          "content": "A is correct. VPC end point.D exposed to the internet",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 753650,
          "date": "Thu 22 Dec 2022 22:29",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The most secure way to access the DynamoDB table while ensuring that the traffic does not leave the AWS network is Option A (Use a VPC endpoint for DynamoDB. )<br><br>A VPC endpoint for DynamoDB allows you to privately connect your VPC to the DynamoDB service without requiring an Internet Gateway, VPN connection, or AWS Direct Connect connection. This ensures that the traffic between the application and the DynamoDB table stays within the AWS network and is not exposed to the public Internet.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option B, using a NAT gateway in a public subnet, would allow the traffic to leave the AWS network and traverse the public Internet, which is less secure. <br><br>Option C, using a NAT instance in a private subnet, would also allow the traffic to leave the AWS network but would require you to manage the NAT instance yourself. <br><br>Option D, using the internet gateway attached to the VPC, would also expose the traffic to the public Internet.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 753651,
          "date": "Thu 22 Dec 2022 22:30",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option B, using a NAT gateway in a public subnet, would allow the traffic to leave the AWS network and traverse the public Internet, which is less secure. <br><br>Option C, using a NAT instance in a private subnet, would also allow the traffic to leave the AWS network but would require you to manage the NAT instance yourself. <br><br>Option D, using the internet gateway attached to the VPC, would also expose the traffic to the public Internet.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 750480,
          "date": "Tue 20 Dec 2022 05:06",
          "username": "\t\t\t\tBENICE\t\t\t",
          "content": "A ---- is correct answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 748342,
          "date": "Sat 17 Dec 2022 20:36",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option A. ",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 724490,
          "date": "Tue 22 Nov 2022 17:44",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 721973,
          "date": "Sat 19 Nov 2022 13:06",
          "username": "\t\t\t\txua81376\t\t\t",
          "content": "Sure A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 720383,
          "date": "Thu 17 Nov 2022 11:09",
          "username": "\t\t\t\tds0321\t\t\t",
          "content": "A - VPC endpoint",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 719276,
          "date": "Wed 16 Nov 2022 03:15",
          "username": "\t\t\t\tgoatbernard\t\t\t",
          "content": "A - VPC endpoint",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 718912,
          "date": "Tue 15 Nov 2022 16:28",
          "username": "\t\t\t\tsdasdawa\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/27700-exam-aws-certified-solutions-architect-associate-saa-c02/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 718908,
          "date": "Tue 15 Nov 2022 16:25",
          "username": "\t\t\t\tNigma\t\t\t",
          "content": "A for sure. https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-dynamodb.html",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 718903,
          "date": "Tue 15 Nov 2022 16:19",
          "username": "\t\t\t\tOhnet\t\t\t",
          "content": "Its A. ",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#177",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>An entertainment company is using Amazon DynamoDB to store media metadata. The application is read intensive and experiencing delays. The company does not have staff to handle additional operational overhead and needs to improve the performance efficiency of DynamoDB without reconfiguring the application.<br><br>What should a solutions architect recommend to meet this requirement?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#177",
          "answers": [
            {
              "choice": "<p>A. Use Amazon ElastiCache for Redis.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use Amazon DynamoDB Accelerator (DAX).<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Replicate data by using DynamoDB global tables.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use Amazon ElastiCache for Memcached with Auto Discovery enabled.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 756107,
          "date": "Mon 26 Dec 2022 01:34",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "DAX stands for DynamoDB Accelerator, and it's like a turbo boost for your DynamoDB tables. It's a fully managed, in-memory cache that speeds up the read and write performance of your DynamoDB tables, so you can get your data faster than ever before.",
          "upvote_count": "7",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 850921,
          "date": "Sun 26 Mar 2023 13:07",
          "username": "\t\t\t\tosmk\t\t\t",
          "content": "B-->Applications that are read-intensive===>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.html#DAX.use-cases",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 778437,
          "date": "Tue 17 Jan 2023 02:13",
          "username": "\t\t\t\tLuckyAro\t\t\t",
          "content": "DynamoDB Accelerator, less over head.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 777526,
          "date": "Mon 16 Jan 2023 11:04",
          "username": "\t\t\t\twmp7039\t\t\t",
          "content": "Option B is incorrect as the constraint in the question is not to recode the application. DAX requires application to be reconfigured and point to DAX instead of DynamoDB<br>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.client.modify-your-app.html<br>Answer should be A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 753653,
          "date": "Thu 22 Dec 2022 22:33",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "To improve the performance efficiency of DynamoDB without reconfiguring the application, a solutions architect should recommend using Amazon DynamoDB Accelerator (DAX) which is Option B as the correct answer.<br><br>DAX is a fully managed, in-memory cache that can be used to improve the performance of read-intensive workloads on DynamoDB.  DAX stores frequently accessed data in memory, allowing the application to retrieve data from the cache rather than making a request to DynamoDB.  This can significantly reduce the number of read requests made to DynamoDB, improving the performance and reducing the latency of the application.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A, using Amazon ElastiCache for Redis, would not be a good fit because it is not specifically designed for use with DynamoDB and would require reconfiguring the application to use it. <br><br>Option C, replicating data using DynamoDB global tables, would not directly improve the performance of reading requests and would require additional operational overhead to maintain the replication. <br><br>Option D, using Amazon ElastiCache for Memcached with Auto Discovery enabled, would also not be a good fit because it is not specifically designed for use with DynamoDB and would require reconfiguring the application to use it.</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 753654,
          "date": "Thu 22 Dec 2022 22:33",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A, using Amazon ElastiCache for Redis, would not be a good fit because it is not specifically designed for use with DynamoDB and would require reconfiguring the application to use it. <br><br>Option C, replicating data using DynamoDB global tables, would not directly improve the performance of reading requests and would require additional operational overhead to maintain the replication. <br><br>Option D, using Amazon ElastiCache for Memcached with Auto Discovery enabled, would also not be a good fit because it is not specifically designed for use with DynamoDB and would require reconfiguring the application to use it.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 748344,
          "date": "Sat 17 Dec 2022 20:37",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option B",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 746082,
          "date": "Thu 15 Dec 2022 13:53",
          "username": "\t\t\t\tk1kavi1\t\t\t",
          "content": "Agreed",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 741820,
          "date": "Sun 11 Dec 2022 16:12",
          "username": "\t\t\t\tShasha1\t\t\t",
          "content": "B<br>DAX is a fully managed, highly available, in-memory cache for DynamoDB that delivers lightning-fast performance and consistent low-latency responses. It provides fast performance without requiring any application reconfiguration",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 724491,
          "date": "Tue 22 Nov 2022 17:45",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 719277,
          "date": "Wed 16 Nov 2022 03:17",
          "username": "\t\t\t\tgoatbernard\t\t\t",
          "content": "DAX is the cache for this",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 719220,
          "date": "Wed 16 Nov 2022 01:52",
          "username": "\t\t\t\tnhlegend\t\t\t",
          "content": "B is correct, DAX provides caching + no changes",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#178",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company's infrastructure consists of Amazon EC2 instances and an Amazon RDS DB instance in a single AWS Region. The company wants to back up its data in a separate Region.<br><br>Which solution will meet these requirements with the LEAST operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#178",
          "answers": [
            {
              "choice": "<p>A. Use AWS Backup to copy EC2 backups and RDS backups to the separate Region.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use Amazon Data Lifecycle Manager (Amazon DLM) to copy EC2 backups and RDS backups to the separate Region.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create Amazon Machine Images (AMIs) of the EC2 instances. Copy the AMIs to the separate Region. Create a read replica for the RDS DB instance in the separate Region.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create Amazon Elastic Block Store (Amazon EBS) snapshots. Copy the EBS snapshots to the separate Region. Create RDS snapshots. Export the RDS snapshots to Amazon S3. Configure S3 Cross-Region Replication (CRR) to the separate Region.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 763906,
          "date": "Mon 02 Jan 2023 19:29",
          "username": "\t\t\t\tvtbk\t\t\t",
          "content": "Cross-Region backup<br>Using AWS Backup, you can copy backups to multiple different AWS Regions on demand or automatically as part of a scheduled backup plan. Cross-Region backup is particularly valuable if you have business continuity or compliance requirements to store backups a minimum distance away from your production data.<br>https://docs.aws.amazon.com/aws-backup/latest/devguide/whatisbackup.html",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 763347,
          "date": "Sun 01 Jan 2023 22:30",
          "username": "\t\t\t\tdan80\t\t\t",
          "content": "A is correct - you need to find a backup solution for EC2 and RDS. DLM doent work with RDS , only with snapshots.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 756109,
          "date": "Mon 26 Dec 2022 01:42",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "using Amazon DLM to copy EC2 backups and RDS backups to the separate region, is not a valid solution because Amazon DLM does not support backing up data across regions.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 753663,
          "date": "Thu 22 Dec 2022 22:47",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option B.  Use Amazon Data Lifecycle Manager (Amazon DLM) to copy EC2 backups and RDS backups to the separate Region.<br><br>Amazon DLM is a fully managed service that helps automate the creation and retention of Amazon EBS snapshots and RDS DB snapshots. It can be used to create and manage backup policies that specify when and how often snapshots should be created, as well as how long they should be retained. With Amazon DLM, you can easily and automatically create and manage backups of your EC2 instances and RDS DB instances in a separate Region, with minimal operational overhead.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Buruguduystunstugudunstuy, sorry, but I haven't found any info about copying RDS backups by DLM. The DLM works only with EBS.<br>So the only answer is A - AWS Backup</li><li>Option A, using AWS Backup to copy EC2 backups and RDS backups to the separate Region, would also work, but it may require more manual configuration and management.<br><br>Option C, creating AMIs of the EC2 instances and copying them to the separate Region, and creating a read replica for the RDS DB instance in the separate Region, would work, but it may require more manual effort to set up and maintain.<br><br>Option D, creating EBS snapshots and copying them to the separate Region, creating RDS snapshots, and exporting them to Amazon S3, and configuring S3 CRR to the separate Region, would also work, but it would involve multiple steps and may require more manual effort to set up and maintain. Overall, using Amazon DLM is likely to be the easiest and most efficient option for meeting the requirements with the least operational overhead.</li><li>This guy is giving wrong answers in detail...lol</li><li>Some of your answers are very detailed. Can you back them up with a reference?</li><li>All of their answers are from ChatGPT</li><li>using Amazon DLM to copy EC2 backups and RDS backups to the separate region, is not a valid solution because Amazon DLM does not support backing up data across regions.</li><li>I choose A, but DLM support cross regions. DLM doesn't support RDS. Cross region copy rules it's a feature of DLM (\\\"For each schedule, you can define the frequency, fast snapshot restore settings (snapshot lifecycle policies only), cross-Region copy rules, and tags\\\")<br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-lifecycle.html</li><li>Thanks techhb</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 765992,
          "date": "Wed 04 Jan 2023 19:37",
          "username": "\t\t\t\tHayLLlHuK\t\t\t",
          "content": "Buruguduystunstugudunstuy, sorry, but I haven't found any info about copying RDS backups by DLM. The DLM works only with EBS.<br>So the only answer is A - AWS Backup",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 753666,
          "date": "Thu 22 Dec 2022 22:48",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A, using AWS Backup to copy EC2 backups and RDS backups to the separate Region, would also work, but it may require more manual configuration and management.<br><br>Option C, creating AMIs of the EC2 instances and copying them to the separate Region, and creating a read replica for the RDS DB instance in the separate Region, would work, but it may require more manual effort to set up and maintain.<br><br>Option D, creating EBS snapshots and copying them to the separate Region, creating RDS snapshots, and exporting them to Amazon S3, and configuring S3 CRR to the separate Region, would also work, but it would involve multiple steps and may require more manual effort to set up and maintain. Overall, using Amazon DLM is likely to be the easiest and most efficient option for meeting the requirements with the least operational overhead.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>This guy is giving wrong answers in detail...lol</li><li>Some of your answers are very detailed. Can you back them up with a reference?</li><li>All of their answers are from ChatGPT</li><li>using Amazon DLM to copy EC2 backups and RDS backups to the separate region, is not a valid solution because Amazon DLM does not support backing up data across regions.</li><li>I choose A, but DLM support cross regions. DLM doesn't support RDS. Cross region copy rules it's a feature of DLM (\\\"For each schedule, you can define the frequency, fast snapshot restore settings (snapshot lifecycle policies only), cross-Region copy rules, and tags\\\")<br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-lifecycle.html</li><li>Thanks techhb</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 775908,
          "date": "Sat 14 Jan 2023 21:41",
          "username": "\t\t\t\tKruiz29\t\t\t",
          "content": "This guy is giving wrong answers in detail...lol",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 757829,
          "date": "Mon 26 Dec 2022 21:35",
          "username": "\t\t\t\tPassNow1234\t\t\t",
          "content": "Some of your answers are very detailed. Can you back them up with a reference?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>All of their answers are from ChatGPT</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 789714,
          "date": "Fri 27 Jan 2023 16:03",
          "username": "\t\t\t\tjwu413\t\t\t",
          "content": "All of their answers are from ChatGPT",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 756108,
          "date": "Mon 26 Dec 2022 01:42",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "using Amazon DLM to copy EC2 backups and RDS backups to the separate region, is not a valid solution because Amazon DLM does not support backing up data across regions.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>I choose A, but DLM support cross regions. DLM doesn't support RDS. Cross region copy rules it's a feature of DLM (\\\"For each schedule, you can define the frequency, fast snapshot restore settings (snapshot lifecycle policies only), cross-Region copy rules, and tags\\\")<br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-lifecycle.html</li><li>Thanks techhb</li></ul>",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 781486,
          "date": "Thu 19 Jan 2023 19:44",
          "username": "\t\t\t\tegmiranda\t\t\t",
          "content": "I choose A, but DLM support cross regions. DLM doesn't support RDS. Cross region copy rules it's a feature of DLM (\\\"For each schedule, you can define the frequency, fast snapshot restore settings (snapshot lifecycle policies only), cross-Region copy rules, and tags\\\")<br>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-lifecycle.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 760416,
          "date": "Thu 29 Dec 2022 00:15",
          "username": "\t\t\t\tPassNow1234\t\t\t",
          "content": "Thanks techhb",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 748347,
          "date": "Sat 17 Dec 2022 20:41",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option A as it is fully managed service with least operational overhead",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 741846,
          "date": "Sun 11 Dec 2022 16:41",
          "username": "\t\t\t\tShasha1\t\t\t",
          "content": "A<br> AWS Backup is a fully managed service that handles the process of copying backups to a separate Region automatically",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 719599,
          "date": "Wed 16 Nov 2022 13:11",
          "username": "\t\t\t\tbabaxoxo\t\t\t",
          "content": "Ans A with least operational overhead",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 719573,
          "date": "Wed 16 Nov 2022 12:31",
          "username": "\t\t\t\trjam\t\t\t",
          "content": "AWS Backup supports Supports cross-region backups",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 719572,
          "date": "Wed 16 Nov 2022 12:29",
          "username": "\t\t\t\trjam\t\t\t",
          "content": "Option A<br>Aws back up supports , EC2, RDS<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>AWS Backup suports Supports cross-region backups</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 719574,
          "date": "Wed 16 Nov 2022 12:31",
          "username": "\t\t\t\trjam\t\t\t",
          "content": "AWS Backup suports Supports cross-region backups",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#179",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A solutions architect needs to securely store a database user name and password that an application uses to access an Amazon RDS DB instance. The application that accesses the database runs on an Amazon EC2 instance. The solutions architect wants to create a secure parameter in AWS Systems Manager Parameter Store.<br><br>What should the solutions architect do to meet this requirement?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#179",
          "answers": [
            {
              "choice": "<p>A. Create an IAM role that has read access to the Parameter Store parameter. Allow Decrypt access to an AWS Key Management Service (AWS KMS) key that is used to encrypt the parameter. Assign this IAM role to the EC2 instance.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an IAM policy that allows read access to the Parameter Store parameter. Allow Decrypt access to an AWS Key Management Service (AWS KMS) key that is used to encrypt the parameter. Assign this IAM policy to the EC2 instance.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an IAM trust relationship between the Parameter Store parameter and the EC2 instance. Specify Amazon RDS as a principal in the trust policy.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an IAM trust relationship between the DB instance and the EC2 instance. Specify Systems Manager as a principal in the trust policy.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 766003,
          "date": "Wed 04 Jan 2023 19:48",
          "username": "\t\t\t\tHayLLlHuK\t\t\t",
          "content": "There should be the Decrypt access to KMS.<br>\\\"If you choose the SecureString parameter type when you create your parameter, Systems Manager uses AWS KMS to encrypt the parameter value.\\\"<br>https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html<br><br>IAM role - for EC2",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 753667,
          "date": "Thu 22 Dec 2022 22:53",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "CORRECT Option A<br><br>To securely store a database user name and password in AWS Systems Manager Parameter Store and allow an application running on an EC2 instance to access it, the solutions architect should create an IAM role that has read access to the Parameter Store parameter and allow Decrypt access to an AWS KMS key that is used to encrypt the parameter. The solutions architect should then assign this IAM role to the EC2 instance.<br><br>This approach allows the EC2 instance to access the parameter in the Parameter Store and decrypt it using the specified KMS key while enforcing the necessary security controls to ensure that the parameter is only accessible to authorized parties.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option B, would not be sufficient, as IAM policies cannot be directly attached to EC2 instances.<br><br>Option C, would not be a valid solution, as the Parameter Store parameter and the EC2 instance are not entities that can be related through an IAM trust relationship.<br><br>Option D, would not be a valid solution, as the trust policy would not allow the EC2 instance to access the parameter in the Parameter Store or decrypt it using the specified KMS key.</li></ul>",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 753672,
          "date": "Thu 22 Dec 2022 22:56",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option B, would not be sufficient, as IAM policies cannot be directly attached to EC2 instances.<br><br>Option C, would not be a valid solution, as the Parameter Store parameter and the EC2 instance are not entities that can be related through an IAM trust relationship.<br><br>Option D, would not be a valid solution, as the trust policy would not allow the EC2 instance to access the parameter in the Parameter Store or decrypt it using the specified KMS key.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 750477,
          "date": "Tue 20 Dec 2022 05:05",
          "username": "\t\t\t\tBENICE\t\t\t",
          "content": "A -- is correct option",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 748349,
          "date": "Sat 17 Dec 2022 20:44",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option A. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 746084,
          "date": "Thu 15 Dec 2022 13:57",
          "username": "\t\t\t\tk1kavi1\t\t\t",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 741874,
          "date": "Sun 11 Dec 2022 17:08",
          "username": "\t\t\t\tShasha1\t\t\t",
          "content": "Answer A<br>Create an IAM role that has read access to the Parameter Store parameter. Allow Decrypt access to an AWS Key Management Service (AWS KMS) key that is used to encrypt the parameter. Assign this IAM role to the EC2 instance. This solution will allow the application to securely access the database user name and password stored in the parameter store.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 728942,
          "date": "Mon 28 Nov 2022 10:47",
          "username": "\t\t\t\towenrooney11\t\t\t",
          "content": "i think policy<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>can you attach policy to ec2 directly ?</li><li>Access to Parameter Store is enabled by IAM policies and supports resource level permissions for access. An IAM policy that grants permissions to specific parameters or a namespace can be used to limit access to these parameters. CloudTrail logs, if enabled for the service, record any attempt to access a parameter.</li><li>https://aws.amazon.com/blogs/compute/managing-secrets-for-amazon-ecs-applications-using-parameter-store-and-iam-roles-for-tasks/</li><li>This link gives the example \\\"Walkthrough: Securely access Parameter Store resources with IAM roles for tasks\\\" - essentially A above. It doe snot show how this can be done using a policy (B) alone.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 735890,
          "date": "Mon 05 Dec 2022 12:45",
          "username": "\t\t\t\tturalmth\t\t\t",
          "content": "can you attach policy to ec2 directly ?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 728945,
          "date": "Mon 28 Nov 2022 10:48",
          "username": "\t\t\t\towenrooney11\t\t\t",
          "content": "Access to Parameter Store is enabled by IAM policies and supports resource level permissions for access. An IAM policy that grants permissions to specific parameters or a namespace can be used to limit access to these parameters. CloudTrail logs, if enabled for the service, record any attempt to access a parameter.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 728943,
          "date": "Mon 28 Nov 2022 10:47",
          "username": "\t\t\t\towenrooney11\t\t\t",
          "content": "https://aws.amazon.com/blogs/compute/managing-secrets-for-amazon-ecs-applications-using-parameter-store-and-iam-roles-for-tasks/<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>This link gives the example \\\"Walkthrough: Securely access Parameter Store resources with IAM roles for tasks\\\" - essentially A above. It doe snot show how this can be done using a policy (B) alone.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 767708,
          "date": "Fri 06 Jan 2023 14:08",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "This link gives the example \\\"Walkthrough: Securely access Parameter Store resources with IAM roles for tasks\\\" - essentially A above. It doe snot show how this can be done using a policy (B) alone.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 724054,
          "date": "Tue 22 Nov 2022 03:50",
          "username": "\t\t\t\tEKA_CloudGod\t\t\t",
          "content": "A.  Attach IAM role to EC2 Instance<br>https://aws.amazon.com/blogs/security/digital-signing-asymmetric-keys-aws-kms/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 719875,
          "date": "Wed 16 Nov 2022 18:45",
          "username": "\t\t\t\tsdasdawa\t\t\t",
          "content": "Agree with A,IAM role is for services (EC2 for example)<br>IAM policy is more for users and groups",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 719608,
          "date": "Wed 16 Nov 2022 13:26",
          "username": "\t\t\t\tbabaxoxo\t\t\t",
          "content": "Attach IAM role to EC2 Instance profile",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 719280,
          "date": "Wed 16 Nov 2022 03:22",
          "username": "\t\t\t\tgoatbernard\t\t\t",
          "content": "IAM policy",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        }
      ]
    },
    {
      "question_id": "#180",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is designing a cloud communications platform that is driven by APIs. The application is hosted on Amazon EC2 instances behind a Network Load Balancer (NLB). The company uses Amazon API Gateway to provide external users with access to the application through APIs. The company wants to protect the platform against web exploits like SQL injection and also wants to detect and mitigate large, sophisticated DDoS attacks.<br><br>Which combination of solutions provides the MOST protection? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: BC</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#180",
          "answers": [
            {
              "choice": "<p>A. Use AWS WAF to protect the NLB. <br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use AWS Shield Advanced with the NLB. <br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use AWS WAF to protect Amazon API Gateway.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use Amazon GuardDuty with AWS Shield Standard<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Use AWS Shield Standard with Amazon API Gateway.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 719631,
          "date": "Wed 16 Nov 2022 13:46",
          "username": "\t\t\t\tbabaxoxo\t\t\t",
          "content": "Shield - Load Balancer, CF, Route53<br>AWF - CF, ALB, API Gateway<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Thank u U meant WAF* - CloudFormation, right? haha</li></ul>",
          "upvote_count": "25",
          "selected_answers": "Selected Answer: BC"
        },
        {
          "id": 758139,
          "date": "Tue 27 Dec 2022 05:56",
          "username": "\t\t\t\tOuk\t\t\t",
          "content": "Thank u U meant WAF* - CloudFormation, right? haha",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 791624,
          "date": "Sun 29 Jan 2023 13:24",
          "username": "\t\t\t\tkerl\t\t\t",
          "content": "for those who select A, it is wrong, WAF is Layer 7, it only support ABL, APIGateway, CloudFront,COgnito User Pool and AppSync graphQL API (https://docs.aws.amazon.com/waf/latest/developerguide/waf-chapter.html). NLB is NOT supported. Answer is BC",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 784420,
          "date": "Sun 22 Jan 2023 16:08",
          "username": "\t\t\t\tbullrem\t\t\t",
          "content": "A and B are the best options to provide the greatest protection for the platform against web vulnerabilities and large, sophisticated DDoS attacks.<br>Option A: Use AWS WAF to protect the NLB.  This will provide protection against common web vulnerabilities such as SQL injection.<br>Option B: Use AWS Shield Advanced with the NLB.  This will provide additional protection against large and sophisticated DDoS attacks.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>The best protection for the platform would be to use A and C together because it will protect both the NLB and the API Gateway from web vulnerabilities and DDoS attacks.</li><li>A and C are the best options for protecting the platform against web vulnerabilities and detecting and mitigating large and sophisticated DDoS attacks.<br>A: AWS WAF can be used to protect the NLB from web vulnerabilities such as SQL injection.<br>C: AWS WAF can be used to protect Amazon API Gateway and also provide protection against DDoS attacks.<br>B: AWS Shield Advanced is used to protect resources from DDoS attacks, but it is not specific to the NLB and may not provide the same level of protection as using WAF specifically on the NLB. <br>D and E: Amazon GuardDuty and AWS Shield Standard are primarily used for threat detection and may not provide the same level of protection as using WAF and Shield Advanced.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AB"
        },
        {
          "id": 784426,
          "date": "Sun 22 Jan 2023 16:13",
          "username": "\t\t\t\tbullrem\t\t\t",
          "content": "The best protection for the platform would be to use A and C together because it will protect both the NLB and the API Gateway from web vulnerabilities and DDoS attacks.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 784429,
          "date": "Sun 22 Jan 2023 16:16",
          "username": "\t\t\t\tbullrem\t\t\t",
          "content": "A and C are the best options for protecting the platform against web vulnerabilities and detecting and mitigating large and sophisticated DDoS attacks.<br>A: AWS WAF can be used to protect the NLB from web vulnerabilities such as SQL injection.<br>C: AWS WAF can be used to protect Amazon API Gateway and also provide protection against DDoS attacks.<br>B: AWS Shield Advanced is used to protect resources from DDoS attacks, but it is not specific to the NLB and may not provide the same level of protection as using WAF specifically on the NLB. <br>D and E: Amazon GuardDuty and AWS Shield Standard are primarily used for threat detection and may not provide the same level of protection as using WAF and Shield Advanced.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 757775,
          "date": "Mon 26 Dec 2022 20:15",
          "username": "\t\t\t\tdrabi\t\t\t",
          "content": "WS Shield Advanced can help protect your Amazon EC2 instances and Network Load Balancers against infrastructure-layer Distributed Denial of Service (DDoS) attacks. Enable AWS Shield Advanced on an AWS Elastic IP address and attach the address to an internet-facing EC2 instance or Network Load Balancer.https://aws.amazon.com/blogs/security/tag/network-load-balancers/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: BC"
        },
        {
          "id": 754298,
          "date": "Fri 23 Dec 2022 15:38",
          "username": "\t\t\t\tduriselvan\t\t\t",
          "content": "Regional resources<br><br>You can protect regional resources in all Regions where AWS WAF is available. You can see the list at AWS WAF endpoints and quotas in the Amazon Web Services General Reference.<br><br>You can use AWS WAF to protect the following regional resource types:<br><br>Amazon API Gateway REST API<br><br>Application Load Balancer<br><br>AWS AppSync GraphQL API<br><br>Amazon Cognito user pool<br><br>You can only associate a web ACL to an Application Load Balancer that's within AWS Regions. For example, you cannot associate a web ACL to an Application Load Balancer that's on AWS Outposts.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Ans:-a and C</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 754299,
          "date": "Fri 23 Dec 2022 15:39",
          "username": "\t\t\t\tduriselvan\t\t\t",
          "content": "Ans:-a and C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 753681,
          "date": "Thu 22 Dec 2022 23:06",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "***CORRECT***<br>A.  Use AWS WAF to protect the NLB. C.  Use AWS WAF to protect Amazon API Gateway.<br><br>AWS WAF is a web application firewall that helps protect web applications from common web exploits such as SQL injection and cross-site scripting attacks. By using AWS WAF to protect the NLB and Amazon API Gateway, the company can provide an additional layer of protection for its cloud communications platform against these types of web exploits.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Your answer is wrong. <br><br>Sophisticated DDOS = Shield Advanced (DD0S attacks the front!) What happens if your load balances goes down? <br><br>Your API gateway is on the BACK further behind the NLB.  SQL Protect that with the WAF<br><br>B and C are right.</li><li>This guy just copies and pastes from ChatGPT.</li><li>About AWS Shield Advanced and Amazon GuardDuty<br><br>AWS Shield Advanced is a managed DDoS protection service that provides additional protection for Amazon EC2 instances, Amazon RDS DB instances, Amazon Elastic Load Balancers, and Amazon CloudFront distributions. It can help detect and mitigate large, sophisticated DDoS attacks, \\\"but it does not provide protection against web exploits like SQL injection.\\\"<br><br>Amazon GuardDuty is a threat detection service that uses machine learning and other techniques to identify potentially malicious activity in your AWS accounts. It can be used in conjunction with AWS Shield Standard, which provides basic DDoS protection for Amazon EC2 instances, Amazon RDS DB instances, and Amazon Elastic Load Balancers. However, neither Amazon GuardDuty nor AWS Shield Standard provides protection against web exploits like SQL injection.<br><br>Overall, the combination of using AWS WAF to protect the NLB and Amazon API Gateway provides the most protection against web exploits and large, sophisticated DDoS attacks.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AC"
        },
        {
          "id": 757835,
          "date": "Mon 26 Dec 2022 21:44",
          "username": "\t\t\t\tPassNow1234\t\t\t",
          "content": "Your answer is wrong. <br><br>Sophisticated DDOS = Shield Advanced (DD0S attacks the front!) What happens if your load balances goes down? <br><br>Your API gateway is on the BACK further behind the NLB.  SQL Protect that with the WAF<br><br>B and C are right.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>This guy just copies and pastes from ChatGPT.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 789728,
          "date": "Fri 27 Jan 2023 16:18",
          "username": "\t\t\t\tjwu413\t\t\t",
          "content": "This guy just copies and pastes from ChatGPT.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 753684,
          "date": "Thu 22 Dec 2022 23:10",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "About AWS Shield Advanced and Amazon GuardDuty<br><br>AWS Shield Advanced is a managed DDoS protection service that provides additional protection for Amazon EC2 instances, Amazon RDS DB instances, Amazon Elastic Load Balancers, and Amazon CloudFront distributions. It can help detect and mitigate large, sophisticated DDoS attacks, \\\"but it does not provide protection against web exploits like SQL injection.\\\"<br><br>Amazon GuardDuty is a threat detection service that uses machine learning and other techniques to identify potentially malicious activity in your AWS accounts. It can be used in conjunction with AWS Shield Standard, which provides basic DDoS protection for Amazon EC2 instances, Amazon RDS DB instances, and Amazon Elastic Load Balancers. However, neither Amazon GuardDuty nor AWS Shield Standard provides protection against web exploits like SQL injection.<br><br>Overall, the combination of using AWS WAF to protect the NLB and Amazon API Gateway provides the most protection against web exploits and large, sophisticated DDoS attacks.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 750476,
          "date": "Tue 20 Dec 2022 05:05",
          "username": "\t\t\t\tBENICE\t\t\t",
          "content": "Option B and C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 748352,
          "date": "Sat 17 Dec 2022 20:47",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "B and C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BC"
        },
        {
          "id": 742100,
          "date": "Sun 11 Dec 2022 22:09",
          "username": "\t\t\t\ttz1\t\t\t",
          "content": "B & C is the answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 724494,
          "date": "Tue 22 Nov 2022 17:50",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "B and C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 720494,
          "date": "Thu 17 Nov 2022 14:48",
          "username": "\t\t\t\tBENICE\t\t\t",
          "content": "B and C<br>\\\"AWS Shield Advanced\\\" for \\\"sophisticated DDoS attacks\\\"<br>\\\"AWS WAF\\\"for \\\"NLB",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 719611,
          "date": "Wed 16 Nov 2022 13:27",
          "username": "\t\t\t\tNigma\t\t\t",
          "content": "B and C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 719582,
          "date": "Wed 16 Nov 2022 12:45",
          "username": "\t\t\t\trjam\t\t\t",
          "content": "AWS Shield Advanced- DDos attacks<br>AWS WAF to protect Amazon API Gateway, because WAF sits before the API Gateway and then comes NLB. ",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: BC"
        }
      ]
    },
    {
      "question_id": "#181",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has a legacy data processing application that runs on Amazon EC2 instances. Data is processed sequentially, but the order of results does not matter. The application uses a monolithic architecture. The only way that the company can scale the application to meet increased demand is to increase the size of the instances.<br><br>The company's developers have decided to rewrite the application to use a microservices architecture on Amazon Elastic Container Service (Amazon ECS).<br><br>What should a solutions architect recommend for communication between the microservices?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#181",
          "answers": [
            {
              "choice": "<p>A. Create an Amazon Simple Queue Service (Amazon SQS) queue. Add code to the data producers, and send data to the queue. Add code to the data consumers to process data from the queue.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an Amazon Simple Notification Service (Amazon SNS) topic. Add code to the data producers, and publish notifications to the topic. Add code to the data consumers to subscribe to the topic.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an AWS Lambda function to pass messages. Add code to the data producers to call the Lambda function with a data object. Add code to the data consumers to receive a data object that is passed from the Lambda function.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an Amazon DynamoDB table. Enable DynamoDB Streams. Add code to the data producers to insert data into the table. Add code to the data consumers to use the DynamoDB Streams API to detect new table entries and retrieve the data.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 753696,
          "date": "Thu 22 Dec 2022 23:17",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option B, using Amazon Simple Notification Service (SNS), would not be suitable for this use case, as SNS is a pub/sub messaging service that is designed for one-to-many communication, rather than point-to-point communication between specific microservices.<br><br>Option C, using an AWS Lambda function to pass messages, would not be suitable for this use case, as it would require the data producers and data consumers to have a direct connection and invoke the Lambda function, rather than being decoupled through a message queue.<br><br>Option D, using an Amazon DynamoDB table with DynamoDB Streams, would not be suitable for this use case, as it would require the data consumers to continuously poll the DynamoDB Streams API to detect new table entries, rather than being notified of new data through a message queue.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Hence, Option A is the correct answer.<br><br>Create an Amazon Simple Queue Service (Amazon SQS) queue. Add code to the data producers, and send data to the queue. Add code to the data consumers to process data from the queue.</li></ul>",
          "upvote_count": "6",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 753697,
          "date": "Thu 22 Dec 2022 23:18",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Hence, Option A is the correct answer.<br><br>Create an Amazon Simple Queue Service (Amazon SQS) queue. Add code to the data producers, and send data to the queue. Add code to the data consumers to process data from the queue.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 843125,
          "date": "Sat 18 Mar 2023 20:58",
          "username": "\t\t\t\tasoli\t\t\t",
          "content": "The answer is A. <br>B is wrong because SNS cannot send events \\\"directly\\\" to ECS.<br>https://docs.aws.amazon.com/sns/latest/dg/sns-event-destinations.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 821539,
          "date": "Sat 25 Feb 2023 15:02",
          "username": "\t\t\t\tuser_deleted\t\t\t",
          "content": "it deosn;t say it is one-one relationships , SNS is better",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 748246,
          "date": "Sat 17 Dec 2022 18:45",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Best answer is A.  <br>Though C or D is possible it requires additional components and integration and so they are not efficient. Assuming that rate of incoming requests is within limits that SQS can handle A is best option.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 746096,
          "date": "Thu 15 Dec 2022 14:06",
          "username": "\t\t\t\tk1kavi1\t\t\t",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 741917,
          "date": "Sun 11 Dec 2022 18:06",
          "username": "\t\t\t\tShasha1\t\t\t",
          "content": "answer is B.  <br>An Amazon Simple Notification Service (Amazon SNS) topic can be used for communication between the microservices in this scenario. The data producers can be configured to publish notifications to the topic, and the data consumers can be configured to subscribe to the topic and receive notifications as they are published. This allows for asynchronous communication between the microservices, Question here focus on communication between microservices",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 721986,
          "date": "Sat 19 Nov 2022 13:25",
          "username": "\t\t\t\txua81376\t\t\t",
          "content": "We needdecoupling so ok to use SQS",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 720502,
          "date": "Thu 17 Nov 2022 14:57",
          "username": "\t\t\t\tBENICE\t\t\t",
          "content": "Can someone explain it bit more? Not able to understand it.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>As monolithic systems become too large to deal with, many enterprises are drawn to breaking them down into the microservices architectural style by means of decoupling. Amazon Simple Queue Service (Amazon SQS) is a fully managed message queuing service that makes it easy to decouple and scale microservices, distributed systems, and serverless applications</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 724066,
          "date": "Tue 22 Nov 2022 04:08",
          "username": "\t\t\t\tEKA_CloudGod\t\t\t",
          "content": "As monolithic systems become too large to deal with, many enterprises are drawn to breaking them down into the microservices architectural style by means of decoupling. Amazon Simple Queue Service (Amazon SQS) is a fully managed message queuing service that makes it easy to decouple and scale microservices, distributed systems, and serverless applications",
          "upvote_count": "12",
          "selected_answers": ""
        },
        {
          "id": 720235,
          "date": "Thu 17 Nov 2022 06:34",
          "username": "\t\t\t\ttaer\t\t\t",
          "content": "Answer is A",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 719645,
          "date": "Wed 16 Nov 2022 13:54",
          "username": "\t\t\t\tNigma\t\t\t",
          "content": "SQS to decouple.",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#182",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company wants to migrate its MySQL database from on premises to AWS. The company recently experienced a database outage that significantly impacted the business. To ensure this does not happen again, the company wants a reliable database solution on AWS that minimizes data loss and stores every transaction on at least two nodes.<br><br>Which solution meets these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#182",
          "answers": [
            {
              "choice": "<p>A. Create an Amazon RDS DB instance with synchronous replication to three nodes in three Availability Zones.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an Amazon RDS MySQL DB instance with Multi-AZ functionality enabled to synchronously replicate the data.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an Amazon RDS MySQL DB instance and then create a read replica in a separate AWS Region that synchronously replicates the data.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an Amazon EC2 instance with a MySQL engine installed that triggers an AWS Lambda function to synchronously replicate the data to an Amazon RDS MySQL DB instance.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 719590,
          "date": "Wed 16 Nov 2022 12:57",
          "username": "\t\t\t\trjam\t\t\t",
          "content": "Amazon RDS MySQL DB instance with Multi-AZ functionality enabled to synchronously replicate the data<br> Standby DB in Multi-AZ- synchronous replication<br><br>Read Replica always asynchronous. so option C is ignored.",
          "upvote_count": "10",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 778453,
          "date": "Tue 17 Jan 2023 02:56",
          "username": "\t\t\t\tLuckyAro\t\t\t",
          "content": "Multi AZ is not as protected as Multi-Region Read Replica.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 769296,
          "date": "Sun 08 Jan 2023 11:50",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "I curios to know why A isn't right. Is it just that it would take more effort?",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 755119,
          "date": "Sat 24 Dec 2022 21:07",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "B is correct C requires more wokr.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 750475,
          "date": "Tue 20 Dec 2022 05:04",
          "username": "\t\t\t\tBENICE\t\t\t",
          "content": "Option B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 748820,
          "date": "Sun 18 Dec 2022 13:15",
          "username": "\t\t\t\tbammy\t\t\t",
          "content": "Multi-AZ will give at least two nodes as required by the question. The answer is B. <br><br>Amazon RDS provides high availability and failover support for DB instances using Multi-AZ deployments with a single standby DB instance.<br>https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZSingleStandby.html",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 748255,
          "date": "Sat 17 Dec 2022 18:57",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 741929,
          "date": "Sun 11 Dec 2022 18:24",
          "username": "\t\t\t\tShasha1\t\t\t",
          "content": "Option A is the correct answer in this scenario because it meets the requirements specified in the question. It creates an Amazon RDS DB instance with synchronous replication to three nodes in three Availability Zones, which will provide high availability and durability for the database, ensuring that the data is stored on multiple nodes and automatically replicated across Availability Zones.<br><br>Option B is not a correct answer because it creates an Amazon RDS MySQL DB instance with Multi-AZ functionality enabled, which only provides failover capabilities. It does not enable synchronous replication to multiple nodes, which is required in this scenario.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option B is not incorrect: \\\"The primary DB instance is synchronously replicated across Availability Zones to a standby replica to provide data redundancy and minimize latency spikes during system backups\\\" from https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZSingleStandby.html</li><li>I would go with Option B since it meets the company's requirements and is the most suitable solution.<br><br>By creating an Amazon RDS MySQL DB instance with Multi-AZ functionality enabled, the solutions architect will ensure that data is automatically synchronously replicated across multiple AZs within the same Region. This provides high availability and data durability, minimizing the risk of data loss and ensuring that every transaction is stored on at least two nodes.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 769293,
          "date": "Sun 08 Jan 2023 11:45",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "Option B is not incorrect: \\\"The primary DB instance is synchronously replicated across Availability Zones to a standby replica to provide data redundancy and minimize latency spikes during system backups\\\" from https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZSingleStandby.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 753706,
          "date": "Thu 22 Dec 2022 23:33",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "I would go with Option B since it meets the company's requirements and is the most suitable solution.<br><br>By creating an Amazon RDS MySQL DB instance with Multi-AZ functionality enabled, the solutions architect will ensure that data is automatically synchronously replicated across multiple AZs within the same Region. This provides high availability and data durability, minimizing the risk of data loss and ensuring that every transaction is stored on at least two nodes.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 740632,
          "date": "Sat 10 Dec 2022 03:10",
          "username": "\t\t\t\tstepman\t\t\t",
          "content": "Maybe C since Amazon RDC now supports cross region read replica https://aws.amazon.com/about-aws/whats-new/2022/11/amazon-rds-sql-server-cross-region-read-replica/",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 724498,
          "date": "Tue 22 Nov 2022 17:53",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 724069,
          "date": "Tue 22 Nov 2022 04:13",
          "username": "\t\t\t\tEKA_CloudGod\t\t\t",
          "content": "Option B is the correct answer:<br>https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZSingleStandby.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 719650,
          "date": "Wed 16 Nov 2022 13:57",
          "username": "\t\t\t\tNigma\t\t\t",
          "content": "B is the answer",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#183",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is building a new dynamic ordering website. The company wants to minimize server maintenance and patching. The website must be highly available and must scale read and write capacity as quickly as possible to meet changes in user demand.<br><br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#183",
          "answers": [
            {
              "choice": "<p>A. Host static content in Amazon S3. Host dynamic content by using Amazon API Gateway and AWS Lambda. Use Amazon DynamoDB with on-demand capacity for the database. Configure Amazon CloudFront to deliver the website content.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Host static content in Amazon S3. Host dynamic content by using Amazon API Gateway and AWS Lambda. Use Amazon Aurora with Aurora Auto Scaling for the database. Configure Amazon CloudFront to deliver the website content.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Host all the website content on Amazon EC2 instances. Create an Auto Scaling group to scale the EC2 instances. Use an Application Load Balancer to distribute traffic. Use Amazon DynamoDB with provisioned write capacity for the database.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Host all the website content on Amazon EC2 instances. Create an Auto Scaling group to scale the EC2 instances. Use an Application Load Balancer to distribute traffic. Use Amazon Aurora with Aurora Auto Scaling for the database.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 725099,
          "date": "Wed 23 Nov 2022 13:20",
          "username": "\t\t\t\tromko\t\t\t",
          "content": "A - is correct, because Dynamodb on-demand scales write and read capacity<br>B - Aurora auto scaling scales only read replicas",
          "upvote_count": "23",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 722500,
          "date": "Sun 20 Nov 2022 10:23",
          "username": "\t\t\t\tManlikeleke\t\t\t",
          "content": "please is this dump enough to pass the exam?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>You can tell us now ? Going by the date of your post I guess you would have challenged the exam by now ?so how did it go ?</li><li>I HOPE SO</li></ul>",
          "upvote_count": "7",
          "selected_answers": ""
        },
        {
          "id": 798614,
          "date": "Sun 05 Feb 2023 05:45",
          "username": "\t\t\t\tLuckyAro\t\t\t",
          "content": "You can tell us now ? Going by the date of your post I guess you would have challenged the exam by now ?so how did it go ?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 724173,
          "date": "Tue 22 Nov 2022 09:26",
          "username": "\t\t\t\tBobbybash\t\t\t",
          "content": "I HOPE SO",
          "upvote_count": "7",
          "selected_answers": ""
        },
        {
          "id": 771261,
          "date": "Tue 10 Jan 2023 10:47",
          "username": "\t\t\t\tDavidNamy\t\t\t",
          "content": "The correct answer is B. <br><br>The option A would also meet the company's requirements of minimizing server maintenance and patching, and providing high availability and quick scaling for read and write capacity. However, there are a few reasons why option B is a more optimal solution:<br><br>In option A, it uses Amazon DynamoDB with on-demand capacity for the database, which may not provide the same level of scalability and performance as using Amazon Aurora with Aurora Auto Scaling.<br>Amazon Aurora offers additional features such as automatic failover, read replicas, and backups that makes it a more robust and resilient option than DynamoDB.  Additionally, the auto scaling feature is better suited to handle the changes in user demand.<br>Additionally, option B provides a more cost-effective solution, as Amazon Aurora can be more cost-effective for high read and write workloads than Amazon DynamoDB, and also it's providing more features.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>The answer is A. <br>Key phrase in the Question is must scale read and write capacity. Aurora is only for Read.<br>Amazon DynamoDB has two read/write capacity modes for processing reads and writes on your tables:<br>On-demand<br>Provisioned (default, free-tier eligible)<br>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 788782,
          "date": "Thu 26 Jan 2023 15:01",
          "username": "\t\t\t\tJoxtat\t\t\t",
          "content": "The answer is A. <br>Key phrase in the Question is must scale read and write capacity. Aurora is only for Read.<br>Amazon DynamoDB has two read/write capacity modes for processing reads and writes on your tables:<br>On-demand<br>Provisioned (default, free-tier eligible)<br>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 763607,
          "date": "Mon 02 Jan 2023 10:01",
          "username": "\t\t\t\tZerotn3\t\t\t",
          "content": "A for sure ~",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 748259,
          "date": "Sat 17 Dec 2022 19:00",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 742500,
          "date": "Mon 12 Dec 2022 08:19",
          "username": "\t\t\t\tlapaki\t\t\t",
          "content": "A. Looking for serverless to reduce maintenance requirements",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 741937,
          "date": "Sun 11 Dec 2022 18:38",
          "username": "\t\t\t\tShasha1\t\t\t",
          "content": "A<br>Amazon DynamoDB with on-demand capacity for the database. This solution allows the website to automatically scale to meet changes in user demand and minimize the need for server maintenance and patching.B is not a correct answer because it uses Amazon Aurora with Aurora Auto Scaling for the database(While Amazon Aurora is a highly available and scalable database solution); however, it is not a suitable choice for this scenario because it requires server maintenance and patching.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Right answer but wrong reason. B is not suitable because the requirements are \\\"must scale read and write\\\" but Aurora replication is using single-master replication, i.e. Read Replication.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 769313,
          "date": "Sun 08 Jan 2023 12:07",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "Right answer but wrong reason. B is not suitable because the requirements are \\\"must scale read and write\\\" but Aurora replication is using single-master replication, i.e. Read Replication.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 725048,
          "date": "Wed 23 Nov 2022 11:15",
          "username": "\t\t\t\tmabotega\t\t\t",
          "content": "On-demand mode is a good option if any of the following are true:<br><br>You create new tables with unknown workloads.<br><br>You have unpredictable application traffic.<br><br>You prefer the ease of paying for only what you use.<br>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 724499,
          "date": "Tue 22 Nov 2022 17:55",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 720943,
          "date": "Thu 17 Nov 2022 23:54",
          "username": "\t\t\t\tAz900500\t\t\t",
          "content": "Selected Answer A<br><br>\\\"Read write capacity = DynamoDb\\\" Read Replica mostly Aurora ..@nhlegend yes DynampDB has 400KB maximum but in the answer neither Dynamo or Aurora was used as primary storage",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 719877,
          "date": "Wed 16 Nov 2022 18:48",
          "username": "\t\t\t\tsdasdawa\t\t\t",
          "content": "Agree with A, DynamoDB is perfect for storing ordering data (key-values)",
          "upvote_count": "5",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 719651,
          "date": "Wed 16 Nov 2022 14:00",
          "username": "\t\t\t\tNigma\t\t\t",
          "content": "A is the answer",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 719594,
          "date": "Wed 16 Nov 2022 13:01",
          "username": "\t\t\t\trjam\t\t\t",
          "content": "option B . Aurora is better than DynamoDB<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>amazon aurora - highly available, self-healing, auto-scaling</li><li>B is not suitable because the requirements are \\\"must scale read and write\\\" but Aurora replication is using single-master replication, i.e. Read Replication.</li><li>Question states \\\"must scale Read and Write Capacity\\\" which refers to Dynamo, whereas, Aurora is good for scaling read replicas.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 719596,
          "date": "Wed 16 Nov 2022 13:03",
          "username": "\t\t\t\trjam\t\t\t",
          "content": "amazon aurora - highly available, self-healing, auto-scaling<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>B is not suitable because the requirements are \\\"must scale read and write\\\" but Aurora replication is using single-master replication, i.e. Read Replication.</li><li>Question states \\\"must scale Read and Write Capacity\\\" which refers to Dynamo, whereas, Aurora is good for scaling read replicas.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 769314,
          "date": "Sun 08 Jan 2023 12:08",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "B is not suitable because the requirements are \\\"must scale read and write\\\" but Aurora replication is using single-master replication, i.e. Read Replication.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 734052,
          "date": "Fri 02 Dec 2022 21:36",
          "username": "\t\t\t\tAamee\t\t\t",
          "content": "Question states \\\"must scale Read and Write Capacity\\\" which refers to Dynamo, whereas, Aurora is good for scaling read replicas.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 719177,
          "date": "Wed 16 Nov 2022 00:01",
          "username": "\t\t\t\tnhlegend\t\t\t",
          "content": "B is correct, DynampDB has 400KB maximum<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>typo, I mean A is correct</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 719178,
          "date": "Wed 16 Nov 2022 00:03",
          "username": "\t\t\t\tnhlegend\t\t\t",
          "content": "typo, I mean A is correct",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#184",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has an AWS account used for software engineering. The AWS account has access to the company's on-premises data center through a pair of AWS Direct Connect connections. All non-VPC traffic routes to the virtual private gateway.<br><br>A development team recently created an AWS Lambda function through the console. The development team needs to allow the function to access a database that runs in a private subnet in the company's data center.<br><br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#184",
          "answers": [
            {
              "choice": "<p>A. Configure the Lambda function to run in the VPC with the appropriate security group.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Set up a VPN connection from AWS to the data center. Route the traffic from the Lambda function through the VPN.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Update the route tables in the VPC to allow the Lambda function to access the on-premises data center through Direct Connect.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an Elastic IP address. Configure the Lambda function to send traffic through the Elastic IP address without an elastic network interface.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 737760,
          "date": "Wed 07 Dec 2022 12:07",
          "username": "\t\t\t\tjavitech83\t\t\t",
          "content": "it is A.  C is not correct at all as in the question it metions that the VPC already has connectivity with on-premises<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>C says to \\\"update the route table\\\" not create a new connection. C is correct.</li></ul>",
          "upvote_count": "7",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 778458,
          "date": "Tue 17 Jan 2023 03:07",
          "username": "\t\t\t\tLuckyAro\t\t\t",
          "content": "C says to \\\"update the route table\\\" not create a new connection. C is correct.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 732947,
          "date": "Thu 01 Dec 2022 19:35",
          "username": "\t\t\t\tGil80\t\t\t",
          "content": "To configure a VPC for an existing function:<br><br>1. Open the Functions page of the Lambda console.<br>2. Choose a function.<br>3. Choose Configuration and then choose VPC. <br>4. Under VPC, choose Edit.<br>5. Choose a VPC, subnets, and security groups. <-- **That's why I believe the answer is A**.<br><br>Note:<br>If your function needs internet access, use network address translation (NAT). Connecting a function to a public subnet doesn't give it internet access or a public IP address.",
          "upvote_count": "6",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 841752,
          "date": "Fri 17 Mar 2023 08:50",
          "username": "\t\t\t\tDevsin2000\t\t\t",
          "content": "In my opinion this question is flawed. Non of the answers makes any sense to me. However, if I have to choose one I will choose C.  There is no option of associating Security Group with Lambda function.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 816780,
          "date": "Tue 21 Feb 2023 16:57",
          "username": "\t\t\t\tbdp123\t\t\t",
          "content": "https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html#vpc-managing-eni",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 809270,
          "date": "Wed 15 Feb 2023 09:04",
          "username": "\t\t\t\tnickolaj\t\t\t",
          "content": "The best solution to meet the requirements would be option A - Configure the Lambda function to run in the VPC with the appropriate security group.<br><br>By configuring the Lambda function to run in the VPC, the function will have access to the private subnets in the company's data center through the Direct Connect connections. Additionally, security groups can be used to control inbound and outbound traffic to and from the Lambda function, ensuring that only the necessary traffic is allowed.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option B is not ideal as it would require additional configuration and management of a VPN connection between the company's data center and AWS, which may not be necessary for the specific use case.<br><br>Option C is not recommended as updating the route tables to allow the Lambda function to access the on-premises data center through Direct Connect would allow all VPC traffic to route through the data center, which may not be desirable and could potentially create security risks.<br><br>Option D is not a viable solution for accessing resources in the on-premises data center as Elastic IP addresses are only used for outbound internet traffic from an Amazon VPC, and cannot be used to communicate with resources in an on-premises data center.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 809271,
          "date": "Wed 15 Feb 2023 09:04",
          "username": "\t\t\t\tnickolaj\t\t\t",
          "content": "Option B is not ideal as it would require additional configuration and management of a VPN connection between the company's data center and AWS, which may not be necessary for the specific use case.<br><br>Option C is not recommended as updating the route tables to allow the Lambda function to access the on-premises data center through Direct Connect would allow all VPC traffic to route through the data center, which may not be desirable and could potentially create security risks.<br><br>Option D is not a viable solution for accessing resources in the on-premises data center as Elastic IP addresses are only used for outbound internet traffic from an Amazon VPC, and cannot be used to communicate with resources in an on-premises data center.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 807170,
          "date": "Mon 13 Feb 2023 09:08",
          "username": "\t\t\t\tYelizaveta\t\t\t",
          "content": "\\\"All non-VPC traffic routes to the virtual private gateway.\\\" means -> there are already the appropriate routes, so no need for update the route tables.<br>Key phrase: \\\"database that runs in a private subnet in the company's data center.\\\", means: You need the appropriate security group to access the DB. ",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 778459,
          "date": "Tue 17 Jan 2023 03:09",
          "username": "\t\t\t\tLuckyAro\t\t\t",
          "content": "A makes more sense to me.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 763817,
          "date": "Mon 02 Jan 2023 16:45",
          "username": "\t\t\t\tMindvision\t\t\t",
          "content": "A = Answer. <br><br>Note that \\\" All non-VPC traffic routes to the virtual gateway\\\" meaning if traffic not meant for the VPC, it routes to on-prem (C answer invalid). For the Lambda function to access the on-prem database you have to configure the Lambda function in the VPC and use appropriate SG outbound. Phew! did some research on this, was a bit confused with C. <br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Yes Lambda is not connected to an Amazon VPC.  so Answer A</li></ul>",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 803785,
          "date": "Fri 10 Feb 2023 00:34",
          "username": "\t\t\t\tDeepak_k\t\t\t",
          "content": "Yes Lambda is not connected to an Amazon VPC.  so Answer A",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 755601,
          "date": "Sun 25 Dec 2022 12:44",
          "username": "\t\t\t\tNV305\t\t\t",
          "content": "it is C only",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 753708,
          "date": "Thu 22 Dec 2022 23:43",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "To allow an AWS Lambda function to access a database in a private subnet in the company's data center, the correct solution is to update the route tables in the Virtual Private Cloud (VPC) to allow the Lambda function to access the on-premises data center through the AWS Direct Connect connections.<br><br>Option C, updating the route tables in the VPC to allow the Lambda function to access the on-premises data center through Direct Connect, is the correct solution to meet the requirements.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A, configuring the Lambda function to run in the VPC with the appropriate security group, is not the correct solution because it does not allow the Lambda function to access the database in the private subnet in the data center.<br><br>Option B, setting up a VPN connection from AWS to the data center and routing the traffic from the Lambda function through the VPN, is not the correct solution because it would not be the most efficient solution, as the traffic would need to be routed over the public internet, potentially increasing latency.<br><br>Option D, creating an Elastic IP address and configuring the Lambda function to send traffic through the Elastic IP address without an elastic network interface, is not a valid solution because Elastic IP addresses are used to assign a static public IP address to an instance or network interface, and do not provide a direct connection to an on-premises data center.</li><li>Sorry, but like a lot of your responses in this group, your answers are incorrect. I really think you need to study more, unless you are deliberately trying to confuse people. \\\"All non-VPC traffic routes to the virtual private gateway\\\" means that C is not necessary.</li><li>Have noticed the Buru----tuy guy/girl likes giving incorrect answers.</li><li>Most likely Buru----tuy is getting responses from ChatGPT, which is not always right.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 753709,
          "date": "Thu 22 Dec 2022 23:43",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A, configuring the Lambda function to run in the VPC with the appropriate security group, is not the correct solution because it does not allow the Lambda function to access the database in the private subnet in the data center.<br><br>Option B, setting up a VPN connection from AWS to the data center and routing the traffic from the Lambda function through the VPN, is not the correct solution because it would not be the most efficient solution, as the traffic would need to be routed over the public internet, potentially increasing latency.<br><br>Option D, creating an Elastic IP address and configuring the Lambda function to send traffic through the Elastic IP address without an elastic network interface, is not a valid solution because Elastic IP addresses are used to assign a static public IP address to an instance or network interface, and do not provide a direct connection to an on-premises data center.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 769319,
          "date": "Sun 08 Jan 2023 12:13",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "Sorry, but like a lot of your responses in this group, your answers are incorrect. I really think you need to study more, unless you are deliberately trying to confuse people. \\\"All non-VPC traffic routes to the virtual private gateway\\\" means that C is not necessary.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Have noticed the Buru----tuy guy/girl likes giving incorrect answers.</li><li>Most likely Buru----tuy is getting responses from ChatGPT, which is not always right.</li></ul>",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 792486,
          "date": "Mon 30 Jan 2023 07:39",
          "username": "\t\t\t\tProfXsamson\t\t\t",
          "content": "Have noticed the Buru----tuy guy/girl likes giving incorrect answers.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Most likely Buru----tuy is getting responses from ChatGPT, which is not always right.</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 803834,
          "date": "Fri 10 Feb 2023 01:34",
          "username": "\t\t\t\tsuperman917\t\t\t",
          "content": "Most likely Buru----tuy is getting responses from ChatGPT, which is not always right.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 748265,
          "date": "Sat 17 Dec 2022 19:07",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 726231,
          "date": "Thu 24 Nov 2022 22:51",
          "username": "\t\t\t\tNewptone\t\t\t",
          "content": "When you connect a function to a VPC, Lambda assigns your function to a Hyperplane ENI (elastic network interface) for each subnet in your function's VPC configuration. Lambda creates a Hyperplane ENI the first time a unique subnet and security group combination is defined for a VPC-enabled function in an account.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 725111,
          "date": "Wed 23 Nov 2022 13:38",
          "username": "\t\t\t\tromko\t\t\t",
          "content": "lambda by default runs out of vpc, so without A lambda is out of vpc.<br><br>C is incorrect, because don't matter how you change route tables in VPC it doesn't make sense while lambda is out of vpc.<br><br>So the correct answer is A",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 724500,
          "date": "Tue 22 Nov 2022 17:56",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "C is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 720236,
          "date": "Thu 17 Nov 2022 06:35",
          "username": "\t\t\t\ttaer\t\t\t",
          "content": "Answer is C",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 720136,
          "date": "Thu 17 Nov 2022 03:05",
          "username": "\t\t\t\tmricee9\t\t\t",
          "content": "C<br>https://www.examtopics.com/discussions/amazon/view/68069-exam-aws-certified-solutions-architect-associate-saa-c02/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 719075,
          "date": "Tue 15 Nov 2022 20:46",
          "username": "\t\t\t\tOhnet\t\t\t",
          "content": "Its A. Deploy the Lambda Function in the VPC with a security group.<br>https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html#vpc-managing-eni",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#185",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company runs an application using Amazon ECS. The application creates resized versions of an original image and then makes Amazon S3 API calls to store the resized images in Amazon S3.<br><br>How can a solutions architect ensure that the application has permission to access Amazon S3?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#185",
          "answers": [
            {
              "choice": "<p>A. Update the S3 role in AWS IAM to allow read/write access from Amazon ECS, and then relaunch the container.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an IAM role with S3 permissions, and then specify that role as the taskRoleArn in the task definition.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create a security group that allows access from Amazon ECS to Amazon S3, and update the launch configuration used by the ECS cluster.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an IAM user with S3 permissions, and then relaunch the Amazon EC2 instances for the ECS cluster while logged in as this account.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 755632,
          "date": "Sun 25 Dec 2022 13:33",
          "username": "\t\t\t\tk1kavi1\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/27954-exam-aws-certified-solutions-architect-associate-saa-c02/<br><br>https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-ecs-taskdefinition.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 755036,
          "date": "Sat 24 Dec 2022 18:15",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "The short name or full Amazon Resource Name (ARN) of the AWS Identity and Access Management role that grants containers in the task permission to call AWS APIs on your behalf.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 753714,
          "date": "Thu 22 Dec 2022 23:47",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "To ensure that an Amazon Elastic Container Service (ECS) application has permission to access Amazon Simple Storage Service (S3), the correct solution is to create an AWS Identity and Access Management (IAM) role with the necessary S3 permissions and specify that role as the taskRoleArn in the task definition for the ECS application.<br><br>Option B, creating an IAM role with S3 permissions and specifying that role as the taskRoleArn in the task definition, is the correct solution to meet the requirement.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A, updating the S3 role in IAM to allow read/write access from ECS and relaunching the container, is not the correct solution because the S3 role is not associated with the ECS application.<br><br>Option C, creating a security group that allows access from ECS to S3 and updating the launch configuration used by the ECS cluster, is not the correct solution because security groups are used to control inbound and outbound traffic to resources, and do not grant permissions to access resources.<br><br>Option D, creating an IAM user with S3 permissions and relaunching the EC2 instances for the ECS cluster while logged in as this account, is not the correct solution because it is generally considered best practice to use IAM roles rather than IAM users to grant permissions to resources.</li></ul>",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 753715,
          "date": "Thu 22 Dec 2022 23:47",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A, updating the S3 role in IAM to allow read/write access from ECS and relaunching the container, is not the correct solution because the S3 role is not associated with the ECS application.<br><br>Option C, creating a security group that allows access from ECS to S3 and updating the launch configuration used by the ECS cluster, is not the correct solution because security groups are used to control inbound and outbound traffic to resources, and do not grant permissions to access resources.<br><br>Option D, creating an IAM user with S3 permissions and relaunching the EC2 instances for the ECS cluster while logged in as this account, is not the correct solution because it is generally considered best practice to use IAM roles rather than IAM users to grant permissions to resources.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 750473,
          "date": "Tue 20 Dec 2022 05:03",
          "username": "\t\t\t\tBENICE\t\t\t",
          "content": "Option B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 748266,
          "date": "Sat 17 Dec 2022 19:10",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option B. ",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 746105,
          "date": "Thu 15 Dec 2022 14:17",
          "username": "\t\t\t\tk1kavi1\t\t\t",
          "content": "Agreed",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 739128,
          "date": "Thu 08 Dec 2022 14:57",
          "username": "\t\t\t\tlighrz\t\t\t",
          "content": "B is the best answer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 724502,
          "date": "Tue 22 Nov 2022 17:57",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 720230,
          "date": "Thu 17 Nov 2022 06:20",
          "username": "\t\t\t\ttaer\t\t\t",
          "content": "The answer is B. ",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 719665,
          "date": "Wed 16 Nov 2022 14:10",
          "username": "\t\t\t\tNigma\t\t\t",
          "content": "B is the answer",
          "upvote_count": "2",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#186",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has a Windows-based application that must be migrated to AWS. The application requires the use of a shared Windows file system attached to multiple Amazon EC2 Windows instances that are deployed across multiple Availability Zone:<br><br>What should a solutions architect do to meet this requirement?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#186",
          "answers": [
            {
              "choice": "<p>A. Configure AWS Storage Gateway in volume gateway mode. Mount the volume to each Windows instance.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Configure Amazon FSx for Windows File Server. Mount the Amazon FSx file system to each Windows instance.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Configure a file system by using Amazon Elastic File System (Amazon EFS). Mount the EFS file system to each Windows instance.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure an Amazon Elastic Block Store (Amazon EBS) volume with the required size. Attach each EC2 instance to the volume. Mount the file system within the volume to each Windows instance.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 719669,
          "date": "Wed 16 Nov 2022 14:14",
          "username": "\t\t\t\tNigma\t\t\t",
          "content": "Correct is B<br>FSx --> shared Windows file systemSMB<br>EFS --> Linux NFS",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 755636,
          "date": "Sun 25 Dec 2022 13:36",
          "username": "\t\t\t\tk1kavi1\t\t\t",
          "content": "References : <br>https://www.examtopics.com/discussions/amazon/view/28006-exam-aws-certified-solutions-architect-associate-saa-c02/<br><br>https://docs.aws.amazon.com/AmazonECS/latest/developerguide/wfsx-volumes.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 755030,
          "date": "Sat 24 Dec 2022 17:55",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "EFS is not compatible with Windows.<br>https://pilotcoresystems.com/insights/ebs-efs-fsx-s3-how-these-storage-options-differ/#:~:text=EFS%20works%20with%20Linux%20and,with%20all%20Window%20Server%20platforms.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 753821,
          "date": "Fri 23 Dec 2022 04:17",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "A.  Configure AWS Storage Gateway in volume gateway mode. Mount the volume to each Windows instance.<br><br>This option is incorrect because AWS Storage Gateway is not a file storage service. It is a hybrid storage service that allows you to store data in the cloud while maintaining low-latency access to frequently accessed data. It is designed to integrate with on-premises storage systems, not to provide file storage for Amazon EC2 instances.<br>B.  Configure Amazon FSx for Windows File Server. Mount the Amazon FSx file system to each Windows instance.<br><br>This is the correct answer. Amazon FSx for Windows File Server is a fully managed file storage service that provides a native Windows file system that can be accessed over the SMB protocol. It is specifically designed for use with Windows-based applications, and it can be easily integrated with existing applications by mounting the file system to each EC2 instance.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>C.  Configure a file system by using Amazon Elastic File System (Amazon EFS). Mount the EFS file system to each Windows instance.<br><br>This option is incorrect because Amazon EFS is a file storage service that is designed for use with Linux-based applications. It is not compatible with Windows-based applications, and it cannot be accessed over the SMB protocol.<br>D.  Configure an Amazon Elastic Block Store (Amazon EBS) volume with the required size. Attach each EC2 instance to the volume. Mount the file system within the volume to each Windows instance.<br><br>This option is incorrect because Amazon EBS is a block storage service, not a file storage service. It is designed for storing raw block-level data that can be accessed by a single EC2 instance at a time. It is not designed for use as a shared file system that can be accessed by multiple instances.</li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 753822,
          "date": "Fri 23 Dec 2022 04:18",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "C.  Configure a file system by using Amazon Elastic File System (Amazon EFS). Mount the EFS file system to each Windows instance.<br><br>This option is incorrect because Amazon EFS is a file storage service that is designed for use with Linux-based applications. It is not compatible with Windows-based applications, and it cannot be accessed over the SMB protocol.<br>D.  Configure an Amazon Elastic Block Store (Amazon EBS) volume with the required size. Attach each EC2 instance to the volume. Mount the file system within the volume to each Windows instance.<br><br>This option is incorrect because Amazon EBS is a block storage service, not a file storage service. It is designed for storing raw block-level data that can be accessed by a single EC2 instance at a time. It is not designed for use as a shared file system that can be accessed by multiple instances.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 750474,
          "date": "Tue 20 Dec 2022 05:03",
          "username": "\t\t\t\tBENICE\t\t\t",
          "content": "B - is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 748268,
          "date": "Sat 17 Dec 2022 19:12",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 724503,
          "date": "Tue 22 Nov 2022 17:57",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 722600,
          "date": "Sun 20 Nov 2022 13:39",
          "username": "\t\t\t\txua81376\t\t\t",
          "content": "B FSx for windows",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 721242,
          "date": "Fri 18 Nov 2022 12:10",
          "username": "\t\t\t\tBENICE\t\t\t",
          "content": "B is correct option",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 719713,
          "date": "Wed 16 Nov 2022 15:04",
          "username": "\t\t\t\trjam\t\t\t",
          "content": "Amazon FSx for Windows File Server",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        }
      ]
    },
    {
      "question_id": "#187",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is developing an ecommerce application that will consist of a load-balanced front end, a container-based application, and a relational database. A solutions architect needs to create a highly available solution that operates with as little manual intervention as possible.<br><br>Which solutions meet these requirements? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: AD</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#187",
          "answers": [
            {
              "choice": "<p>A. Create an Amazon RDS DB instance in Multi-AZ mode.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create an Amazon RDS DB instance and one or more replicas in another Availability Zone.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an Amazon EC2 instance-based Docker cluster to handle the dynamic application load.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an Amazon Elastic Container Service (Amazon ECS) cluster with a Fargate launch type to handle the dynamic application load.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. Create an Amazon Elastic Container Service (Amazon ECS) cluster with an Amazon EC2 launch type to handle the dynamic application load.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 755029,
          "date": "Sat 24 Dec 2022 17:52",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "https://containersonaws.com/introduction/ec2-or-aws-fargate/A. (O) multi-az <= 'little intervention'B. (X) read replica <= Promoting a read replica to be a standalone DB instance<br>You can promote a read replica into a standalone DB instance. When you promote a read replica, the DB instance is rebooted before it becomes available.<br>https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.htmlC. (X) use Amazon ECS instead of EC2-based docker for little human interventionD. (O) Amazon ECS on AWS Fargate : AWS Fargate is a technology that you can use with Amazon ECS to run containers without having to manage servers or clusters of Amazon EC2 instances.E. (X) EC2 launch type<br>The EC2 launch type can be used to run your containerized applications on Amazon EC2 instances that you register to your Amazon ECS cluster and manage yourself.",
          "upvote_count": "9",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 748269,
          "date": "Sat 17 Dec 2022 19:13",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option A&D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 746107,
          "date": "Thu 15 Dec 2022 14:20",
          "username": "\t\t\t\tk1kavi1\t\t\t",
          "content": "A and D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 727703,
          "date": "Sat 26 Nov 2022 18:07",
          "username": "\t\t\t\tGabs90\t\t\t",
          "content": "A and D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 726208,
          "date": "Thu 24 Nov 2022 22:02",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "A and D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 721246,
          "date": "Fri 18 Nov 2022 12:29",
          "username": "\t\t\t\tBENICE\t\t\t",
          "content": "A and D are the options",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 720022,
          "date": "Wed 16 Nov 2022 22:29",
          "username": "\t\t\t\tDanny23132412141_2312\t\t\t",
          "content": "AD for sure<br>Link: https://www.examtopics.com/discussions/amazon/view/43729-exam-aws-certified-solutions-architect-associate-saa-c02/",
          "upvote_count": "3",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#188",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company uses Amazon S3 as its data lake. The company has a new partner that must use SFTP to upload data files. A solutions architect needs to implement a highly available SFTP solution that minimizes operational overhead.<br><br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#188",
          "answers": [
            {
              "choice": "<p>A. Use AWS Transfer Family to configure an SFTP-enabled server with a publicly accessible endpoint. Choose the S3 data lake as the destination.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use Amazon S3 File Gateway as an SFTP server. Expose the S3 File Gateway endpoint URL to the new partner. Share the S3 File Gateway endpoint with the new partner.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Launch an Amazon EC2 instance in a private subnet in a VPInstruct the new partner to upload files to the EC2 instance by using a VPN. Run a cron job script, on the EC2 instance to upload files to the S3 data lake.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Launch Amazon EC2 instances in a private subnet in a VPC.  Place a Network Load Balancer (NLB) in front of the EC2 instances. Create an SFTP listener port for the NLB.  Share the NLB hostname with the new partner. Run a cron job script on the EC2 instances to upload files to the S3 data lake.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 755190,
          "date": "Sat 24 Dec 2022 22:55",
          "username": "\t\t\t\tChirantan\t\t\t",
          "content": "Answer is A <br>AWS Transfer Family securely scales your recurring business-to-business file transfers to AWS Storage services using SFTP, FTPS, FTP, and AS2 protocols.<br>https://aws.amazon.com/aws-transfer-family/",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 750472,
          "date": "Tue 20 Dec 2022 05:02",
          "username": "\t\t\t\tBENICE\t\t\t",
          "content": "A -- is the option",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 748275,
          "date": "Sat 17 Dec 2022 19:20",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option A",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 731968,
          "date": "Wed 30 Nov 2022 21:47",
          "username": "\t\t\t\tmj98\t\t\t",
          "content": "AWS Transfer Family - SFTP",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 724203,
          "date": "Tue 22 Nov 2022 10:10",
          "username": "\t\t\t\tBobbybash\t\t\t",
          "content": "AAAAAAAA<br>AWS Transfer for SFTP, a fully-managed, highly-available SFTP service. You simply create a server, set up user accounts, and associate the server with one or more Amazon Simple Storage Service (Amazon S3) buckets",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 724202,
          "date": "Tue 22 Nov 2022 10:09",
          "username": "\t\t\t\tBobbybash\t\t\t",
          "content": "AAAAAAAA<br>AWS Transfer for SFTP, a fully-managed, highly-available SFTP service. You simply create a server, set up user accounts, and associate the server with one or more Amazon Simple Storage Service (Amazon S3) buckets.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 719912,
          "date": "Wed 16 Nov 2022 19:39",
          "username": "\t\t\t\tmabotega\t\t\t",
          "content": "A is the answer - https://docs.aws.amazon.com/transfer/latest/userguide/create-server-sftp.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 719672,
          "date": "Wed 16 Nov 2022 14:17",
          "username": "\t\t\t\tNigma\t\t\t",
          "content": "A is the answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 719576,
          "date": "Wed 16 Nov 2022 12:36",
          "username": "\t\t\t\tLeGloupier\t\t\t",
          "content": "answer is A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 719153,
          "date": "Tue 15 Nov 2022 22:49",
          "username": "\t\t\t\tmabotega\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/83197-exam-aws-certified-solutions-architect-associate-saa-c02/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        }
      ]
    },
    {
      "question_id": "#189",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company needs to store contract documents. A contract lasts for 5 years. During the 5-year period, the company must ensure that the documents cannot be overwritten or deleted. The company needs to encrypt the documents at rest and rotate the encryption keys automatically every year.<br><br>Which combination of steps should a solutions architect take to meet these requirements with the LEAST operational overhead? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: BD</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#189",
          "answers": [
            {
              "choice": "<p>A. Store the documents in Amazon S3. Use S3 Object Lock in governance mode.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Store the documents in Amazon S3. Use S3 Object Lock in compliance mode.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Configure key rotation.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use server-side encryption with AWS Key Management Service (AWS KMS) customer managed keys. Configure key rotation.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>E. Use server-side encryption with AWS Key Management Service (AWS KMS) customer provided (imported) keys. Configure key rotation.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 729062,
          "date": "Mon 28 Nov 2022 13:08",
          "username": "\t\t\t\towenrooney11\t\t\t",
          "content": "Originally answered B and C due to least operational overhead. after research its bugging me that the s3 key rotation is determined based on AWS master Key rotation which cannot guarantee the key is rotated with in a 365 day period. stated as \\\"varies\\\" in the documentation. also its impossible to configure this in the console. <br>KMS-C is a tick box in the console to turn on annual key rotation but requires more operational overhead than SSE-S3. <br>C - will not guarantee the questions objectives but requires little overhead.<br>D - will guarantee the questions objective with more overhead.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Id have to disagree on that. It states here that aws managed keys are rotated every year which is what the question asks: https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html so C would be correct.<br>However, it also states that you cannot enable or disable rotation for aws managed keys which would again point towards D</li></ul>",
          "upvote_count": "12",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 749168,
          "date": "Sun 18 Dec 2022 21:36",
          "username": "\t\t\t\tvadiminski_a\t\t\t",
          "content": "Id have to disagree on that. It states here that aws managed keys are rotated every year which is what the question asks: https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html so C would be correct.<br>However, it also states that you cannot enable or disable rotation for aws managed keys which would again point towards D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 843158,
          "date": "Sat 18 Mar 2023 21:49",
          "username": "\t\t\t\tasoli\t\t\t",
          "content": "The answer is B and D<br>C is not correct. with SSe-S3 encryption, you do not have control over the key rotation.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 836396,
          "date": "Sat 11 Mar 2023 19:31",
          "username": "\t\t\t\tfkie4\t\t\t",
          "content": "C is wrong. see this:<br>https://stackoverflow.com/questions/63478626/which-aws-s3-encryption-technique-provides-rotation-policy-for-encryption-keys#:~:text=This%20uses%20your%20own%20key,automatically%20rotated%20every%201%20year.<br>it said \\\"SSE-S3 - is free and uses AWS owned CMKs (CMK = Customer Master Key). The encryption key is owned and managed by AWS, and is shared among many accounts. Its rotation is automatic with time that varies as shown in the table here. The time is not explicitly defined.\\\" . <br>So SSE-S3 does have key rotation, but user cannot configure rotation frequency. It vaires and managed by AWS, NOT by user.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 818970,
          "date": "Thu 23 Feb 2023 09:20",
          "username": "\t\t\t\tjennyka76\t\t\t",
          "content": "2 QUESTION ASK FORl - The company needs to encrypt the documents at rest and rotate the encryption keys automatically every year.<br>READ:https://docs.aws.amazon.com/kms/latest/developerguide/overview.html<br>ANSWER - D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 818947,
          "date": "Thu 23 Feb 2023 08:57",
          "username": "\t\t\t\tjennyka76\t\t\t",
          "content": "1. QUESTION ASK THE FOLLOWING:During the 5-year period, the company must ensure that the documents cannot be overwritten or deleted. ?<br> SEE: https://jayendrapatil.com/tag/s3-object-lock-in-governance-mode/<br>ANSWER: B<br>AM GOING RESEARCH ON SECOND PART OF QUESTION.<br>JESUS IS GOOD. .",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 807108,
          "date": "Mon 13 Feb 2023 07:20",
          "username": "\t\t\t\tYelizaveta\t\t\t",
          "content": "C or D -> Trick question:<br>C is wrong because the keys are rotated automatically by the S3 service in (SSE-S3) option.<br>You are correct that the question says \\\"rotate the encryption keys automatically every year.\\\"<br>But the Answer C says: \\\"Configure key rotation\\\" and that you can not do with (SSE-S3), because it rotates automatically ;)",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 798337,
          "date": "Sat 04 Feb 2023 21:09",
          "username": "\t\t\t\tremand\t\t\t",
          "content": "compliance mode is unnecessary here.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>the company must ensure that the documents cannot be overwritten or deleted.<br><br>This is the definition of compliance mode, it is absolutely needed here.</li><li>totally agree.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: AD"
        },
        {
          "id": 806161,
          "date": "Sun 12 Feb 2023 10:37",
          "username": "\t\t\t\tUnluckyDucky\t\t\t",
          "content": "the company must ensure that the documents cannot be overwritten or deleted.<br><br>This is the definition of compliance mode, it is absolutely needed here.",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 799927,
          "date": "Mon 06 Feb 2023 16:52",
          "username": "\t\t\t\tocbn3wby\t\t\t",
          "content": "totally agree.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 788450,
          "date": "Thu 26 Jan 2023 07:54",
          "username": "\t\t\t\tjohn626\t\t\t",
          "content": "Ans C mention - Configure Key rotation. but SSE-S3 does not have key rotation configuration.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>it does not have that configuration because it is built in to it. A and C are correct</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 796708,
          "date": "Fri 03 Feb 2023 05:13",
          "username": "\t\t\t\tremand\t\t\t",
          "content": "it does not have that configuration because it is built in to it. A and C are correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 778596,
          "date": "Tue 17 Jan 2023 06:36",
          "username": "\t\t\t\tLuckyAro\t\t\t",
          "content": "What part of the question required customer intervention of annual key rotation ? I don't get why automatic rotation is so difficult to grasp, SS3-S3 rotates the key automatically annually as the question required.<br>https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 778592,
          "date": "Tue 17 Jan 2023 06:28",
          "username": "\t\t\t\tLuckyAro\t\t\t",
          "content": "SSE-S3 AWS managed keys are rotated every year. The question did not request for user intervention that's why the said \\\"Rotated Automatically\\\".",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: BC"
        },
        {
          "id": 777557,
          "date": "Mon 16 Jan 2023 11:35",
          "username": "\t\t\t\twmp7039\t\t\t",
          "content": "Amazon S3 managed encryption keys (SSE-S3) doesn't allow customer to configure key rotation. Keys are rotated automatically by the S3 service in (SSE-S3) option<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>The question did not request for user intervention that's why the said \\\"Rotated Automatically\\\".</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 778594,
          "date": "Tue 17 Jan 2023 06:29",
          "username": "\t\t\t\tLuckyAro\t\t\t",
          "content": "The question did not request for user intervention that's why the said \\\"Rotated Automatically\\\".",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 766395,
          "date": "Thu 05 Jan 2023 09:35",
          "username": "\t\t\t\tHayLLlHuK\t\t\t",
          "content": "I haven't found a clear description for S3-SSE key rotation period. Only this:<br>\\\"Server-side encryption protects data at rest. Amazon S3 encrypts each object with a unique key. As an additional safeguard, it encrypts the key itself with a key that it rotates **regularly**\\\". <br>https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingServerSideEncryption.html<br>So I don't go with C. <br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>For D the rotation period is clear.<br>\\\"Customer managed keys<br>Automatic key rotation is disabled by default on customer managed keys but authorized users can enable and disable it. When you enable (or re-enable) automatic key rotation, AWS KMS automatically rotates the KMS key one year (approximately 365 days) after the enable date and every year thereafter.\\\"<br>https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html#rotate-customer-keys</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 766396,
          "date": "Thu 05 Jan 2023 09:35",
          "username": "\t\t\t\tHayLLlHuK\t\t\t",
          "content": "For D the rotation period is clear.<br>\\\"Customer managed keys<br>Automatic key rotation is disabled by default on customer managed keys but authorized users can enable and disable it. When you enable (or re-enable) automatic key rotation, AWS KMS automatically rotates the KMS key one year (approximately 365 days) after the enable date and every year thereafter.\\\"<br>https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html#rotate-customer-keys",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 764054,
          "date": "Mon 02 Jan 2023 22:58",
          "username": "\t\t\t\tZerotn3\t\t\t",
          "content": "That's correct !<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>and when you choose B \\\"Store the documents in Amazon S3. Use S3 Object Lock in compliance mode.\\\" =&gt; key encrypt can not store in S3</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 764057,
          "date": "Mon 02 Jan 2023 23:00",
          "username": "\t\t\t\tZerotn3\t\t\t",
          "content": "and when you choose B \\\"Store the documents in Amazon S3. Use S3 Object Lock in compliance mode.\\\" => key encrypt can not store in S3",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 758256,
          "date": "Tue 27 Dec 2022 08:58",
          "username": "\t\t\t\tSoluAWS\t\t\t",
          "content": "LEAST operational overhead - AWS Managed Key. I would go with BC",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BC"
        },
        {
          "id": 755009,
          "date": "Sat 24 Dec 2022 17:25",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "B due to compliance mode no usercan delete files<br>C-doesn't rotate after an year.<br>E-add more operational overhead.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Why B https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 755011,
          "date": "Sat 24 Dec 2022 17:27",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "Why B https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 748316,
          "date": "Sat 17 Dec 2022 20:11",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "B&D is right option as SSE-S3 does not provide guarantee for the exact duration (1 year in this case) for key rotation.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: BD"
        },
        {
          "id": 744279,
          "date": "Tue 13 Dec 2022 17:40",
          "username": "\t\t\t\tNavneet90\t\t\t",
          "content": "B and D <br>KMS rotate passwords automate",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#190",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has a web application that is based on Java and PHP. The company plans to move the application from on premises to AWS. The company needs the ability to test new site features frequently. The company also needs a highly available and managed solution that requires minimum operational overhead.<br><br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#190",
          "answers": [
            {
              "choice": "<p>A. Create an Amazon S3 bucket. Enable static web hosting on the S3 bucket. Upload the static content to the S3 bucket. Use AWS Lambda to process all dynamic content.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Deploy the web application to an AWS Elastic Beanstalk environment. Use URL swapping to switch between multiple Elastic Beanstalk environments for feature testing.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Deploy the web application to Amazon EC2 instances that are configured with Java and PHP. Use Auto Scaling groups and an Application Load Balancer to manage the website's availability.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Containerize the web application. Deploy the web application to Amazon EC2 instances. Use the AWS Load Balancer Controller to dynamically route traffic between containers that contain the new site features for testing.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 742067,
          "date": "Sun 11 Dec 2022 21:23",
          "username": "\t\t\t\tShasha1\t\t\t",
          "content": "B<br> Elastic Beanstalk is a fully managed service that makes it easy to deploy and run applications in the AWS; To enable frequent testing of new site features, you can use URL swapping to switch between multiple Elastic Beanstalk environments.",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 808547,
          "date": "Tue 14 Feb 2023 16:58",
          "username": "\t\t\t\tkerin\t\t\t",
          "content": "Option B as it has the minimum operational overhead",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 806459,
          "date": "Sun 12 Feb 2023 15:59",
          "username": "\t\t\t\tmaciekmaciek\t\t\t",
          "content": "Blue/Green deployments https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 805019,
          "date": "Sat 11 Feb 2023 06:47",
          "username": "\t\t\t\tnaxer82\t\t\t",
          "content": "is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 763967,
          "date": "Mon 02 Jan 2023 21:34",
          "username": "\t\t\t\tgustavtd\t\t\t",
          "content": "As I was told, Elastic Beanstalk is an expensive service, isn't it?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>so what? The question doesn't require the most cost-effective solution</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 766422,
          "date": "Thu 05 Jan 2023 10:06",
          "username": "\t\t\t\tHayLLlHuK\t\t\t",
          "content": "so what? The question doesn't require the most cost-effective solution",
          "upvote_count": "8",
          "selected_answers": ""
        },
        {
          "id": 754980,
          "date": "Sat 24 Dec 2022 16:55",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "D includes additional overhead of installing.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 750471,
          "date": "Tue 20 Dec 2022 05:01",
          "username": "\t\t\t\tBENICE\t\t\t",
          "content": "B --is correct answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 748319,
          "date": "Sat 17 Dec 2022 20:13",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option B as it has the minimum operational overhead",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 746127,
          "date": "Thu 15 Dec 2022 14:36",
          "username": "\t\t\t\tk1kavi1\t\t\t",
          "content": "B looks correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 736796,
          "date": "Tue 06 Dec 2022 13:28",
          "username": "\t\t\t\thpipit\t\t\t",
          "content": "B is the correct. 100%. i have confirmation",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 731973,
          "date": "Wed 30 Nov 2022 21:50",
          "username": "\t\t\t\tmj98\t\t\t",
          "content": "Answer B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 731932,
          "date": "Wed 30 Nov 2022 20:56",
          "username": "\t\t\t\tStuden15\t\t\t",
          "content": "for containers, you need source image. Beanstalk is configurable runtime environment - you can choose stack (java, php, ..) and its version. Much more easier to deploy and use compared to containers.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 725131,
          "date": "Wed 23 Nov 2022 13:55",
          "username": "\t\t\t\tromko\t\t\t",
          "content": "wow, so many votes for B. <br><br>B will be correct if application requires one of runtime java or php, elastic Beanstallk allows to specify only one runtime. In requirement is \\\"web application that is based on Java and PHP\\\"<br>so B is out.<br><br>D allows to setup own container and there you may install as many as system needs<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>You can't set up a containerized application on ec2.</li><li>You are right, Beanstalk allows Java or PHP, but not both. I think there could be an error in the question text, as it also mentions that it needs to be a managed service and also able to test new features frequently, so url swappingis great for this. I would choose B</li><li>D can also be done by Elastic Beanstalk. Answer is B, as it using beanstalk removes the overhead<br><br>AWS Elastic Beanstalk is the fastest way to get web applications up and running on AWS. You can simply upload your application code, and the service automatically handles details such as resource provisioning, load balancing, auto scaling, and monitoring. Elastic Beanstalk is ideal if you have a PHP, Java, Python, Ruby, Node.js, .NET, Go, or Docker web application. Elastic Beanstalk uses core AWS services such as Amazon Elastic Compute Cloud (EC2), Amazon Elastic Container Service (ECS), AWS Auto Scaling, and Elastic Load Balancing (ELB) to easily support applications that need to scale to serve millions of users.</li><li>But Elastic Beanstalk configs only support one runtime at once, so you cannot automatically have Java and PHP,unless you go to EC2 directly and install another runtime.</li><li>Don't get your point here... how can you justify Option D for a 'High Available' and 'managed' solution when you're containorizing your apps and deploying your containers on EC2s w/o any Auto-scaling groups involved??...the need in the question is about removing the overhead of managing different layers of computation involved.</li><li>Yeah, agree that D doesn't look as correct I had read EC2 as ECS first time, so ECS and containers are good fit.<br><br>I don't think it's D as well I don't think it's B, because by default ElasticBeanstalk doesn't allow to have PHP and JAVA in the same time.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 744580,
          "date": "Wed 14 Dec 2022 01:09",
          "username": "\t\t\t\tmikey2000\t\t\t",
          "content": "You can't set up a containerized application on ec2.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 737794,
          "date": "Wed 07 Dec 2022 12:40",
          "username": "\t\t\t\tjavitech83\t\t\t",
          "content": "You are right, Beanstalk allows Java or PHP, but not both. I think there could be an error in the question text, as it also mentions that it needs to be a managed service and also able to test new features frequently, so url swappingis great for this. I would choose B",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 727630,
          "date": "Sat 26 Nov 2022 16:50",
          "username": "\t\t\t\tCizzla7049\t\t\t",
          "content": "D can also be done by Elastic Beanstalk. Answer is B, as it using beanstalk removes the overhead<br><br>AWS Elastic Beanstalk is the fastest way to get web applications up and running on AWS. You can simply upload your application code, and the service automatically handles details such as resource provisioning, load balancing, auto scaling, and monitoring. Elastic Beanstalk is ideal if you have a PHP, Java, Python, Ruby, Node.js, .NET, Go, or Docker web application. Elastic Beanstalk uses core AWS services such as Amazon Elastic Compute Cloud (EC2), Amazon Elastic Container Service (ECS), AWS Auto Scaling, and Elastic Load Balancing (ELB) to easily support applications that need to scale to serve millions of users.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>But Elastic Beanstalk configs only support one runtime at once, so you cannot automatically have Java and PHP,unless you go to EC2 directly and install another runtime.</li><li>Don't get your point here... how can you justify Option D for a 'High Available' and 'managed' solution when you're containorizing your apps and deploying your containers on EC2s w/o any Auto-scaling groups involved??...the need in the question is about removing the overhead of managing different layers of computation involved.</li><li>Yeah, agree that D doesn't look as correct I had read EC2 as ECS first time, so ECS and containers are good fit.<br><br>I don't think it's D as well I don't think it's B, because by default ElasticBeanstalk doesn't allow to have PHP and JAVA in the same time.</li></ul>",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 731525,
          "date": "Wed 30 Nov 2022 14:34",
          "username": "\t\t\t\tromko\t\t\t",
          "content": "But Elastic Beanstalk configs only support one runtime at once, so you cannot automatically have Java and PHP,unless you go to EC2 directly and install another runtime.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Don't get your point here... how can you justify Option D for a 'High Available' and 'managed' solution when you're containorizing your apps and deploying your containers on EC2s w/o any Auto-scaling groups involved??...the need in the question is about removing the overhead of managing different layers of computation involved.</li><li>Yeah, agree that D doesn't look as correct I had read EC2 as ECS first time, so ECS and containers are good fit.<br><br>I don't think it's D as well I don't think it's B, because by default ElasticBeanstalk doesn't allow to have PHP and JAVA in the same time.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 734162,
          "date": "Sat 03 Dec 2022 03:14",
          "username": "\t\t\t\tAamee\t\t\t",
          "content": "Don't get your point here... how can you justify Option D for a 'High Available' and 'managed' solution when you're containorizing your apps and deploying your containers on EC2s w/o any Auto-scaling groups involved??...the need in the question is about removing the overhead of managing different layers of computation involved.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Yeah, agree that D doesn't look as correct I had read EC2 as ECS first time, so ECS and containers are good fit.<br><br>I don't think it's D as well I don't think it's B, because by default ElasticBeanstalk doesn't allow to have PHP and JAVA in the same time.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 749694,
          "date": "Mon 19 Dec 2022 10:52",
          "username": "\t\t\t\tromko\t\t\t",
          "content": "Yeah, agree that D doesn't look as correct I had read EC2 as ECS first time, so ECS and containers are good fit.<br><br>I don't think it's D as well I don't think it's B, because by default ElasticBeanstalk doesn't allow to have PHP and JAVA in the same time.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 724508,
          "date": "Tue 22 Nov 2022 18:03",
          "username": "\t\t\t\tWpcorgan\t\t\t",
          "content": "B is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 719731,
          "date": "Wed 16 Nov 2022 15:23",
          "username": "\t\t\t\trjam\t\t\t",
          "content": "Swapping URL : ElasticBeanStalk<br>https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 719687,
          "date": "Wed 16 Nov 2022 14:34",
          "username": "\t\t\t\tNigma\t\t\t",
          "content": "B is the answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 719544,
          "date": "Wed 16 Nov 2022 11:25",
          "username": "\t\t\t\tLeGloupier\t\t\t",
          "content": "isn't it B ?",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#191",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has an ordering application that stores customer information in Amazon RDS for MySQL. During regular business hours, employees run one-time queries for reporting purposes. Timeouts are occurring during order processing because the reporting queries are taking a long time to run. The company needs to eliminate the timeouts without preventing employees from performing queries.<br><br>What should a solutions architect do to meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#191",
          "answers": [
            {
              "choice": "<p>A. Create a read replica. Move reporting queries to the read replica.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Create a read replica. Distribute the ordering application to the primary DB instance and the read replica.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Migrate the ordering application to Amazon DynamoDB with on-demand capacity.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Schedule the reporting queries for non-peak hours.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 750470,
          "date": "Tue 20 Dec 2022 05:00",
          "username": "\t\t\t\tBENICE\t\t\t",
          "content": "A is correct answer. This was in my exam<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Did these questions help with your exam?</li></ul>",
          "upvote_count": "10",
          "selected_answers": ""
        },
        {
          "id": 844354,
          "date": "Mon 20 Mar 2023 01:03",
          "username": "\t\t\t\tGrace83\t\t\t",
          "content": "Did these questions help with your exam?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 850676,
          "date": "Sun 26 Mar 2023 04:44",
          "username": "\t\t\t\tk33\t\t\t",
          "content": "Answer : A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 848680,
          "date": "Thu 23 Mar 2023 22:15",
          "username": "\t\t\t\tPUCKER\t\t\t",
          "content": "SUMMA SUMMA KICK ERUDHAE ! ULUKULAE NALA BHODHA ERUDHAE !",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 840211,
          "date": "Wed 15 Mar 2023 20:04",
          "username": "\t\t\t\tidriskameni\t\t\t",
          "content": "A is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 754966,
          "date": "Sat 24 Dec 2022 16:33",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "we cant distribute write load to s read replica",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 747848,
          "date": "Sat 17 Dec 2022 08:34",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option A is right answer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 746943,
          "date": "Fri 16 Dec 2022 09:09",
          "username": "\t\t\t\tromko\t\t\t",
          "content": "A - is correct because reporting is OK to run on replicated data with some delay in replication.<br>B - is incorrect because main app cannot pointed to read replicate to handle write operation (it's not allowed on read replica) and there is nothing mentioned that only read operations will be performed there.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 729532,
          "date": "Mon 28 Nov 2022 19:54",
          "username": "\t\t\t\tKapello10\t\t\t",
          "content": "A is the correct ans",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 729370,
          "date": "Mon 28 Nov 2022 17:20",
          "username": "\t\t\t\tGabs90\t\t\t",
          "content": "It's A from an old question: https://www.examtopics.com/discussions/amazon/view/81535-exam-aws-certified-solutions-architect-associate-saa-c02/",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 729316,
          "date": "Mon 28 Nov 2022 16:26",
          "username": "\t\t\t\tleonnnn\t\t\t",
          "content": "Timeout occurs because of the query. So use read replica for query is correct answer.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 729201,
          "date": "Mon 28 Nov 2022 15:10",
          "username": "\t\t\t\tJayanKuruwita\t\t\t",
          "content": "It should be read load to read replica",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 729179,
          "date": "Mon 28 Nov 2022 14:53",
          "username": "\t\t\t\tNigma\t\t\t",
          "content": "Answer : A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        }
      ]
    },
    {
      "question_id": "#192",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A hospital wants to create digital copies for its large collection of historical written records. The hospital will continue to add hundreds of new documents each day. The hospital's data team will scan the documents and will upload the documents to the AWS Cloud.<br><br>A solutions architect must implement a solution to analyze the documents, extract the medical information, and store the documents so that an application can run SQL queries on the data. The solution must maximize scalability and operational efficiency.<br><br>Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: BE</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#192",
          "answers": [
            {
              "choice": "<p>A. Write the document information to an Amazon EC2 instance that runs a MySQL database.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Write the document information to an Amazon S3 bucket. Use Amazon Athena to query the data.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Create an Auto Scaling group of Amazon EC2 instances to run a custom application that processes the scanned files and extracts the medical information.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an AWS Lambda function that runs when new documents are uploaded. Use Amazon Rekognition to convert the documents to raw text. Use Amazon Transcribe Medical to detect and extract relevant medical information from the text.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Create an AWS Lambda function that runs when new documents are uploaded. Use Amazon Textract to convert the documents to raw text. Use Amazon Comprehend Medical to detect and extract relevant medical information from the text.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 729908,
          "date": "Tue 29 Nov 2022 05:48",
          "username": "\t\t\t\tKADSM\t\t\t",
          "content": "B and E are correct. Textract to extract text from files. Rekognition can also be used for text detection but after Rekognition - it's mentioned that Transcribe is used. Transcribe is used for Speech to Text. So that option D may not be valid.",
          "upvote_count": "6",
          "selected_answers": ""
        },
        {
          "id": 850677,
          "date": "Sun 26 Mar 2023 04:45",
          "username": "\t\t\t\tk33\t\t\t",
          "content": "Answer : BE",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BE"
        },
        {
          "id": 840212,
          "date": "Wed 15 Mar 2023 20:05",
          "username": "\t\t\t\tidriskameni\t\t\t",
          "content": "B and E are correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BE"
        },
        {
          "id": 797004,
          "date": "Fri 03 Feb 2023 13:03",
          "username": "\t\t\t\taakashkumar1999\t\t\t",
          "content": "Lambda, Textract and S3 Athena perfect combination",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: BE"
        },
        {
          "id": 779069,
          "date": "Tue 17 Jan 2023 16:38",
          "username": "\t\t\t\tLuckyAro\t\t\t",
          "content": "Correct answers are B & E",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BE"
        },
        {
          "id": 754963,
          "date": "Sat 24 Dec 2022 16:23",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "BE-Sql query on S3 and textract ot extract text and compregend to analyze.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: BE"
        },
        {
          "id": 746950,
          "date": "Fri 16 Dec 2022 09:15",
          "username": "\t\t\t\tromko\t\t\t",
          "content": "Usually documents it can be few pages with text,so storing large text in Mysql is not very sufficient + deploy it on EC2 required operation overhead, so A is out. <br><br>Only Textract is used for converting documents to text and Comprehend Medical to parse medical phrases. So E is correct.<br><br>Correct are BE",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: BE"
        },
        {
          "id": 737536,
          "date": "Wed 07 Dec 2022 08:42",
          "username": "\t\t\t\talexleely\t\t\t",
          "content": "Can someone help me, should'nt it be AE? As document information is Text, is it to be stored in a relationship db instead of S3?",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 731978,
          "date": "Wed 30 Nov 2022 21:54",
          "username": "\t\t\t\tmj98\t\t\t",
          "content": "answer BE",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: BE"
        },
        {
          "id": 730983,
          "date": "Wed 30 Nov 2022 02:25",
          "username": "\t\t\t\tTonyghostR05\t\t\t",
          "content": "BE of course",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 730488,
          "date": "Tue 29 Nov 2022 15:11",
          "username": "\t\t\t\tEkie\t\t\t",
          "content": "Answer: BE",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 730487,
          "date": "Tue 29 Nov 2022 15:11",
          "username": "\t\t\t\tvqhuy\t\t\t",
          "content": "B and E for Sure",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 730060,
          "date": "Tue 29 Nov 2022 08:43",
          "username": "\t\t\t\tlearner2023\t\t\t",
          "content": "B,E is correct",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: BE"
        },
        {
          "id": 729674,
          "date": "Mon 28 Nov 2022 23:41",
          "username": "\t\t\t\tTMM369\t\t\t",
          "content": "B - Store S3 Bucket<br>E - Amazon Textstract",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 729534,
          "date": "Mon 28 Nov 2022 19:58",
          "username": "\t\t\t\tKapello10\t\t\t",
          "content": "B and E is the correct ans<br><br>B > Store documents on S3 an use Athena to query ><br>E > Use Textract to extract text from files and not Rekognition. N.B Rekognition is for image identififcation",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 729319,
          "date": "Mon 28 Nov 2022 16:29",
          "username": "\t\t\t\tleonnnn\t\t\t",
          "content": "B E meets the requirements.",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: BE"
        }
      ]
    },
    {
      "question_id": "#193",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company is running a batch application on Amazon EC2 instances. The application consists of a backend with multiple Amazon RDS databases. The application is causing a high number of reads on the databases. A solutions architect must reduce the number of database reads while ensuring high availability.<br><br>What should the solutions architect do to meet this requirement?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#193",
          "answers": [
            {
              "choice": "<p>A. Add Amazon RDS read replicas.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use Amazon ElastiCache for Redis.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use Amazon Route 53 DNS caching<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use Amazon ElastiCache for Memcached.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 729329,
          "date": "Mon 28 Nov 2022 16:34",
          "username": "\t\t\t\tleonnnn\t\t\t",
          "content": "Use ElastiCache to reduce reading and choose redis to ensure high availability.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Where is the high availability when the database fails and the cache time runs out?<br>The answer is a.</li></ul>",
          "upvote_count": "17",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 814542,
          "date": "Sun 19 Feb 2023 22:01",
          "username": "\t\t\t\tLalo\t\t\t",
          "content": "Where is the high availability when the database fails and the cache time runs out?<br>The answer is a.",
          "upvote_count": "8",
          "selected_answers": ""
        },
        {
          "id": 850679,
          "date": "Sun 26 Mar 2023 04:47",
          "username": "\t\t\t\tk33\t\t\t",
          "content": "Answer : B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 850665,
          "date": "Sun 26 Mar 2023 04:02",
          "username": "\t\t\t\tfrenzoid\t\t\t",
          "content": "A.  ElasticCache would be a good solution, if it wasn't that it mentios Redis, and Redis is not Relational. So it can't be applied to RDS Databases.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 849421,
          "date": "Fri 24 Mar 2023 16:13",
          "username": "\t\t\t\tvolkan4242\t\t\t",
          "content": "A.  Add Amazon RDS read replicas.<br><br>Adding Amazon RDS read replicas is the recommended solution for reducing the number of database reads while ensuring high availability. Read replicas are copies of the primary database that can be used to offload read traffic from the primary database. This can reduce the load on the primary database and improve performance. Read replicas can also be used to improve availability, as they can be promoted to become the primary database in case of a failure.<br>According to chatgpt",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 830037,
          "date": "Sun 05 Mar 2023 16:38",
          "username": "\t\t\t\tSteve_4542636\t\t\t",
          "content": "Caching will reduce database reads",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 822896,
          "date": "Sun 26 Feb 2023 20:49",
          "username": "\t\t\t\tJa13\t\t\t",
          "content": "Asks to reduce the number of reads, not to improve the performance, so elasticache is the option",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 807125,
          "date": "Mon 13 Feb 2023 07:48",
          "username": "\t\t\t\tYelizaveta\t\t\t",
          "content": "\\\"A solutions architect must reduce the number of database reads while ensuring high availability.\\\"!!!!",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 806378,
          "date": "Sun 12 Feb 2023 14:41",
          "username": "\t\t\t\tUnluckyDucky\t\t\t",
          "content": "The key to this question is reducing the database read operations which can be achieved with ElastiCache as reads are also saved to ElastiCache, therefore future read quests will often get a response from cache hits, resulting in less database read operations.<br><br>As for the ElastiCache options - Redis vs Memcached:<br>The question states high availability which Memcached does not support.<br>Redis supports Multi-AZ and therefore - ensures high availability.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 802541,
          "date": "Wed 08 Feb 2023 21:46",
          "username": "\t\t\t\tbdp123\t\t\t",
          "content": "Can be used with RDS will reduce reads and has HA<br>https://aws.amazon.com/elasticache/redis/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 800790,
          "date": "Tue 07 Feb 2023 12:15",
          "username": "\t\t\t\tCaoMengde09\t\t\t",
          "content": "Elasticache is useful when all users are accessing the same content of the database. So to improve reads we cache that common accessed content in Elasticache. At the end Elasticache is not a durable storage it's IN-MEMORY yes guaranteehigh Available YES but not a durable storage as the RDS REPLICA.  So A is the most optimal solution from Performance / High Availibility (Cost also even if it's not a criteria for the question)",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 797006,
          "date": "Fri 03 Feb 2023 13:05",
          "username": "\t\t\t\taakashkumar1999\t\t\t",
          "content": "RDS reads means Read Replicas",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 796806,
          "date": "Fri 03 Feb 2023 07:36",
          "username": "\t\t\t\tLuckyAro\t\t\t",
          "content": "Makes more sense<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Amazon ElastiCache for Redis is a blazing fast in-memory data store that provides sub-millisecond latency to power internet-scale real-time applications. Built on open-source Redis and compatible with the Redis APIs, ElastiCache for Redis works with your Redis clients and uses the open Redis data format to store your data. Your self-managed Redis applications can work seamlessly with ElastiCache for Redis without any code changes. ElastiCache for Redis combines the speed, simplicity, and versatility of open-source Redis with manageability, security, and scalability from Amazon to power the most demanding real-time applications in Gaming, Ad-Tech, E-Commerce, Healthcare, Financial Services, and IoT.</li><li>Amazon RDS Read Replicas provide enhanced performance and durability for Amazon RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. You can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput. Read replicas can also be promoted when needed to become standalone DB instances. Read replicas are available in Amazon RDS for MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server as well as Amazon Aurora.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 799439,
          "date": "Mon 06 Feb 2023 06:22",
          "username": "\t\t\t\tLuckyAro\t\t\t",
          "content": "Amazon ElastiCache for Redis is a blazing fast in-memory data store that provides sub-millisecond latency to power internet-scale real-time applications. Built on open-source Redis and compatible with the Redis APIs, ElastiCache for Redis works with your Redis clients and uses the open Redis data format to store your data. Your self-managed Redis applications can work seamlessly with ElastiCache for Redis without any code changes. ElastiCache for Redis combines the speed, simplicity, and versatility of open-source Redis with manageability, security, and scalability from Amazon to power the most demanding real-time applications in Gaming, Ad-Tech, E-Commerce, Healthcare, Financial Services, and IoT.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Amazon RDS Read Replicas provide enhanced performance and durability for Amazon RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. You can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput. Read replicas can also be promoted when needed to become standalone DB instances. Read replicas are available in Amazon RDS for MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server as well as Amazon Aurora.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 799441,
          "date": "Mon 06 Feb 2023 06:25",
          "username": "\t\t\t\tLuckyAro\t\t\t",
          "content": "Amazon RDS Read Replicas provide enhanced performance and durability for Amazon RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. You can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput. Read replicas can also be promoted when needed to become standalone DB instances. Read replicas are available in Amazon RDS for MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server as well as Amazon Aurora.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 793748,
          "date": "Tue 31 Jan 2023 08:04",
          "username": "\t\t\t\tuchiken\t\t\t",
          "content": "My answer is A!!",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 791532,
          "date": "Sun 29 Jan 2023 10:59",
          "username": "\t\t\t\tmhmt4438\t\t\t",
          "content": "Definitely A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 790157,
          "date": "Sat 28 Jan 2023 02:41",
          "username": "\t\t\t\tdevonwho\t\t\t",
          "content": "https://aws.amazon.com/getting-started/hands-on/boosting-mysql-database-performance-with-amazon-elasticache-for-redis/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 788831,
          "date": "Thu 26 Jan 2023 16:01",
          "username": "\t\t\t\tJoxtat\t\t\t",
          "content": "https://amangoeliitb.medium.com/improving-database-performance-with-redis-dbd38fdf3cb",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 788262,
          "date": "Thu 26 Jan 2023 02:19",
          "username": "\t\t\t\tspilyi\t\t\t",
          "content": "IF the answer is B, why D is not the possible answer?<br>From the question, to reduce 'high number of reads on the databases' seems like D (Memcached) is more simple way to apply compare to B (Redis).<br><br>I agree all the comments that 'read-replica' doesn't reduce reads on 'database' but from the selection I don't get why D (Memcached) is eliminated from the option.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Memcached is a very limited in comparison with Redis. Look at the sentence \\\"The application consists of a backend with multiple Amazon RDS databases\\\" so you should think on advanced and complex data structures and queries so -&gt; Redis.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 848053,
          "date": "Thu 23 Mar 2023 11:21",
          "username": "\t\t\t\tMssP\t\t\t",
          "content": "Memcached is a very limited in comparison with Redis. Look at the sentence \\\"The application consists of a backend with multiple Amazon RDS databases\\\" so you should think on advanced and complex data structures and queries so -> Redis.",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#194",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company needs to run a critical application on AWS. The company needs to use Amazon EC2 for the application's database. The database must be highly available and must fail over automatically if a disruptive event occurs.<br><br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: A</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#194",
          "answers": [
            {
              "choice": "<p>A. Launch two EC2 instances, each in a different Availability Zone in the same AWS Region. Install the database on both EC2 instances. Configure the EC2 instances as a cluster. Set up database replication.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>B. Launch an EC2 instance in an Availability Zone. Install the database on the EC2 instance. Use an Amazon Machine Image (AMI) to back up the data. Use AWS CloudFormation to automate provisioning of the EC2 instance if a disruptive event occurs.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Launch two EC2 instances, each in a different AWS Region. Install the database on both EC2 instances. Set up database replication. Fail over the database to a second Region.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Launch an EC2 instance in an Availability Zone. Install the database on the EC2 instance. Use an Amazon Machine Image (AMI) to back up the data. Use EC2 automatic recovery to recover the instance if a disruptive event occurs.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 730227,
          "date": "Tue 29 Nov 2022 11:21",
          "username": "\t\t\t\tGil80\t\t\t",
          "content": "The question states that it is a critical app and it has to be HA.  A could be the answer, but it's in the same AZ, so if the entire region fails, it doesn't cater for the HA requirement. <br><br>However, the likelihood of a failure in two different regions at the same time is 0. Therefore, to me it seems that C is the better option to cater for HA requirement.<br><br>In addition, C does state like A that the DB app is installed on an EC2 instance.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>The question doesn't ask which option is the most HA. It asks what meets the requirements.</li><li>but for C you need communication between the two VPC, which increase the complexity. With a should be enough for HA</li></ul>",
          "upvote_count": "15",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 824211,
          "date": "Mon 27 Feb 2023 23:44",
          "username": "\t\t\t\tSteve_4542636\t\t\t",
          "content": "The question doesn't ask which option is the most HA. It asks what meets the requirements.",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 737806,
          "date": "Wed 07 Dec 2022 12:51",
          "username": "\t\t\t\tjavitech83\t\t\t",
          "content": "but for C you need communication between the two VPC, which increase the complexity. With a should be enough for HA",
          "upvote_count": "4",
          "selected_answers": ""
        },
        {
          "id": 734986,
          "date": "Sun 04 Dec 2022 11:17",
          "username": "\t\t\t\tGil80\t\t\t",
          "content": "Changing my vote to A.  After reviewing a Udemy course of SAA-C03, it seems that A (multi-AZ and Clusters) is sufficient for HA. <br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>what number of class ?</li></ul>",
          "upvote_count": "15",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 755449,
          "date": "Sun 25 Dec 2022 07:47",
          "username": "\t\t\t\tberks\t\t\t",
          "content": "what number of class ?",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 850682,
          "date": "Sun 26 Mar 2023 04:50",
          "username": "\t\t\t\tk33\t\t\t",
          "content": "Answer : A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 846663,
          "date": "Wed 22 Mar 2023 05:17",
          "username": "\t\t\t\tbgsanata\t\t\t",
          "content": "For the once wondering between A and C. <br>\\\"..Configure the EC2 instances as a cluster\\\" > this give you the automatic failover to the second DB.  C point to manual failover making the answer incorrect.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 838532,
          "date": "Tue 14 Mar 2023 04:47",
          "username": "\t\t\t\tsamu010203\t\t\t",
          "content": "looks like A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 824207,
          "date": "Mon 27 Feb 2023 23:40",
          "username": "\t\t\t\tSteve_4542636\t\t\t",
          "content": "Where should the database be stored?It should be stored on an EBS which doesn't support multi-region failover.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 819726,
          "date": "Thu 23 Feb 2023 21:42",
          "username": "\t\t\t\tLonojack\t\t\t",
          "content": "High availability = Availability Zone<br>Disaster Recovery = Multi-Region<br>DISRUPTIVE DOES NOT suggest DISASTER!",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 817173,
          "date": "Tue 21 Feb 2023 22:11",
          "username": "\t\t\t\tMichal_L_95\t\t\t",
          "content": "Voted for A after some consulatio with more experienced AWS architect... Clue over here is that region failover must be done automatically",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 811084,
          "date": "Thu 16 Feb 2023 21:40",
          "username": "\t\t\t\tbdp123\t\t\t",
          "content": "ECS Spread placement strategy<br>ECS groups available capacity used to place Tasks into ECS Clusters with ECS Tasks being launched into an ECS Cluster. An ECS Clusters configured to use EC2 will have EC2 Instances registered with it and each EC2 instance resides in a single Availability Zone. You should be ensuring that you have EC2 instances registered with your Cluster from multiple Availability Zones. <br>https://aws.amazon.com/blogs/containers/amazon-ecs-availability-best-practices/#:~:text=An%20ECS%20Clusters%20configured%20to,Cluster%20from%20multiple%20Availability%20Zones.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 810231,
          "date": "Thu 16 Feb 2023 04:57",
          "username": "\t\t\t\tKZM\t\t\t",
          "content": "It is \\\"A\\\".<br>Multi-AZ in the same region is enough with the requirements for HA and failover.<br>It is not \\\"C\\\". The cross regions may have higher latency.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 809822,
          "date": "Wed 15 Feb 2023 18:58",
          "username": "\t\t\t\tOuk\t\t\t",
          "content": "Failover so multiple region C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 807210,
          "date": "Mon 13 Feb 2023 09:54",
          "username": "\t\t\t\tYelizaveta\t\t\t",
          "content": "High availability means: multi-AZ.<br>DR (Disaster Recovery) means, it could it would be multi-Regions as it talks about disruptive events.<br>But because the keyword is \\\"High Availability\\\" and you have a multi-region for the database this will not be highly available as there will be additional latency issues and data consistency issues as databases are in the different regions.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 797008,
          "date": "Fri 03 Feb 2023 13:07",
          "username": "\t\t\t\taakashkumar1999\t\t\t",
          "content": "Answer is A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 791534,
          "date": "Sun 29 Jan 2023 11:01",
          "username": "\t\t\t\tmhmt4438\t\t\t",
          "content": "Definitely A",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 783898,
          "date": "Sun 22 Jan 2023 03:45",
          "username": "\t\t\t\tbullrem\t\t\t",
          "content": "Option C does not fully meet the requirement of automatic failover in case of a disruptive event. While it does have the database replicated in two regions, there is no mention of automatic failover in the event of a disruption. Additionally, it would also have additional latency and data consistency issues as the databases are in different regions. Option A and D are better solutions as they have automatic failover mechanisms in place in case of disruptive events.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 777933,
          "date": "Mon 16 Jan 2023 17:34",
          "username": "\t\t\t\timisioluwa\t\t\t",
          "content": "The correct answer is A.  <br>(Configure the EC2 instances as a cluster) Cluster consist of one or more DB instances and a cluster volume that manages the data for those DB instances. Cluster Volume is a VIRTUAL DATABASE storage volume that spans multiple Availability Zones, with each Availability Zone having a copy of the DB cluster data.https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.html<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>That's good for HA but not for DR which is the ask here. Correct answer is C. </li></ul>",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 784960,
          "date": "Mon 23 Jan 2023 05:08",
          "username": "\t\t\t\tjainparag1\t\t\t",
          "content": "That's good for HA but not for DR which is the ask here. Correct answer is C. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 772182,
          "date": "Wed 11 Jan 2023 08:33",
          "username": "\t\t\t\tDavidNamy\t\t\t",
          "content": "A.  Launch two EC2 instances, each in a different Availability Zone in the same AWS Region. Install the database on both EC2 instances. Configure the EC2 instances as a cluster. Set up database replication.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        }
      ]
    },
    {
      "question_id": "#195",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company's order system sends requests from clients to Amazon EC2 instances. The EC2 instances process the orders and then store the orders in a database on Amazon RDS. Users report that they must reprocess orders when the system fails. The company wants a resilient solution that can process orders automatically if a system outage occurs.<br><br>What should a solutions architect do to meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: C</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#195",
          "answers": [
            {
              "choice": "<p>A. Move the EC2 instances into an Auto Scaling group. Create an Amazon EventBridge (Amazon CloudWatch Events) rule to target an Amazon Elastic Container Service (Amazon ECS) task.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Move the EC2 instances into an Auto Scaling group behind an Application Load Balancer (ALB). Update the order system to send messages to the ALB endpoint.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Move the EC2 instances into an Auto Scaling group. Configure the order system to send messages to an Amazon Simple Queue Service (Amazon SQS) queue. Configure the EC2 instances to consume messages from the queue.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>D. Create an Amazon Simple Notification Service (Amazon SNS) topic. Create an AWS Lambda function, and subscribe the function to the SNS topic. Configure the order system to send messages to the SNS topic. Send a command to the EC2 instances to process the messages by using AWS Systems Manager Run Command.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 850684,
          "date": "Sun 26 Mar 2023 04:52",
          "username": "\t\t\t\tk33\t\t\t",
          "content": "Answer : C",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 809837,
          "date": "Wed 15 Feb 2023 19:18",
          "username": "\t\t\t\tnickolaj\t\t\t",
          "content": "C.  Move the EC2 instances into an Auto Scaling group. Configure the order system to send messages to an Amazon Simple Queue Service (Amazon SQS) queue. Configure the EC2 instances to consume messages from the queue.<br><br>To meet the requirements of the company, a solutions architect should ensure that the system is resilient and can process orders automatically in the event of a system outage. To achieve this, moving the EC2 instances into an Auto Scaling group is a good first step. This will enable the system to automatically add or remove instances based on demand and availability.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>However, it's also necessary to ensure that orders are not lost if a system outage occurs. To achieve this, the order system can be configured to send messages to an Amazon Simple Queue Service (Amazon SQS) queue. SQS is a highly available and durable messaging service that can help ensure that messages are not lost if the system fails.<br><br>Finally, the EC2 instances can be configured to consume messages from the queue, process the orders and then store them in the database on Amazon RDS. This approach ensures that orders are not lost and can be processed automatically if a system outage occurs. Therefore, option C is the correct answer.</li><li>Option A is incorrect because it suggests creating an Amazon EventBridge rule to target an Amazon Elastic Container Service (ECS) task. While this may be a valid solution in some cases, it is not necessary in this scenario.<br><br>Option B is incorrect because it suggests moving the EC2 instances into an Auto Scaling group behind an Application Load Balancer (ALB) and updating the order system to send messages to the ALB endpoint. While this approach can provide resilience and scalability, it does not address the issue of order processing and the need to ensure that orders are not lost if a system outage occurs.<br><br>Option D is incorrect because it suggests using Amazon Simple Notification Service (SNS) to send messages to an AWS Lambda function, which will then send a command to the EC2 instances to process the messages by using AWS Systems Manager Run Command. While this approach may work, it is more complex than necessary and does not take advantage of the durability and availability of SQS.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 809839,
          "date": "Wed 15 Feb 2023 19:19",
          "username": "\t\t\t\tnickolaj\t\t\t",
          "content": "However, it's also necessary to ensure that orders are not lost if a system outage occurs. To achieve this, the order system can be configured to send messages to an Amazon Simple Queue Service (Amazon SQS) queue. SQS is a highly available and durable messaging service that can help ensure that messages are not lost if the system fails.<br><br>Finally, the EC2 instances can be configured to consume messages from the queue, process the orders and then store them in the database on Amazon RDS. This approach ensures that orders are not lost and can be processed automatically if a system outage occurs. Therefore, option C is the correct answer.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A is incorrect because it suggests creating an Amazon EventBridge rule to target an Amazon Elastic Container Service (ECS) task. While this may be a valid solution in some cases, it is not necessary in this scenario.<br><br>Option B is incorrect because it suggests moving the EC2 instances into an Auto Scaling group behind an Application Load Balancer (ALB) and updating the order system to send messages to the ALB endpoint. While this approach can provide resilience and scalability, it does not address the issue of order processing and the need to ensure that orders are not lost if a system outage occurs.<br><br>Option D is incorrect because it suggests using Amazon Simple Notification Service (SNS) to send messages to an AWS Lambda function, which will then send a command to the EC2 instances to process the messages by using AWS Systems Manager Run Command. While this approach may work, it is more complex than necessary and does not take advantage of the durability and availability of SQS.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 809840,
          "date": "Wed 15 Feb 2023 19:19",
          "username": "\t\t\t\tnickolaj\t\t\t",
          "content": "Option A is incorrect because it suggests creating an Amazon EventBridge rule to target an Amazon Elastic Container Service (ECS) task. While this may be a valid solution in some cases, it is not necessary in this scenario.<br><br>Option B is incorrect because it suggests moving the EC2 instances into an Auto Scaling group behind an Application Load Balancer (ALB) and updating the order system to send messages to the ALB endpoint. While this approach can provide resilience and scalability, it does not address the issue of order processing and the need to ensure that orders are not lost if a system outage occurs.<br><br>Option D is incorrect because it suggests using Amazon Simple Notification Service (SNS) to send messages to an AWS Lambda function, which will then send a command to the EC2 instances to process the messages by using AWS Systems Manager Run Command. While this approach may work, it is more complex than necessary and does not take advantage of the durability and availability of SQS.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 779230,
          "date": "Tue 17 Jan 2023 19:52",
          "username": "\t\t\t\tLuckyAro\t\t\t",
          "content": "My question is; can orders be sent directly into an SQS queue ? How about the protocol for management of the messages from the queue ? can EC2 instances be programmed to process them like Lambda ?",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 755451,
          "date": "Sun 25 Dec 2022 07:55",
          "username": "\t\t\t\tberks\t\t\t",
          "content": "I choose D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 754535,
          "date": "Fri 23 Dec 2022 21:51",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "To meet the requirements of the company, a solution should be implemented that can automatically process orders if a system outage occurs. Option C meets these requirements by using an Auto Scaling group and Amazon Simple Queue Service (SQS) to ensure that orders can be processed even if a system outage occurs.<br><br>In this solution, the EC2 instances are placed in an Auto Scaling group, which ensures that the number of instances can be automatically scaled up or down based on demand. The ordering system is configured to send messages to an SQS queue, which acts as a buffer and stores the messages until they can be processed by the EC2 instances. The EC2 instances are configured to consume messages from the queue and process them. If a system outage occurs, the messages in the queue will remain available and can be processed once the system is restored.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 754457,
          "date": "Fri 23 Dec 2022 20:24",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "c is right",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 748294,
          "date": "Sat 17 Dec 2022 19:41",
          "username": "\t\t\t\tNikaCZ\t\t\t",
          "content": "C.  Move the EC2 instances into an Auto Scaling group. Configure the order system to send messages to an Amazon Simple Queue Service (Amazon SQS) queue. Configure the EC2 instances to consume messages from the queue.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 746960,
          "date": "Fri 16 Dec 2022 09:32",
          "username": "\t\t\t\tromko\t\t\t",
          "content": "C, decouple applications and functionality, give ability to reprocess message if failed due to networking issue or overloaded other systems",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 742566,
          "date": "Mon 12 Dec 2022 09:51",
          "username": "\t\t\t\tShasha1\t\t\t",
          "content": "C<br>Configuring the EC2 instances to consume messages from the SQS queue will ensure that the instances can process orders automatically, even if a system outage occurs.<br>e.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 730985,
          "date": "Wed 30 Nov 2022 02:30",
          "username": "\t\t\t\tTonyghostR05\t\t\t",
          "content": "SQS order",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 730233,
          "date": "Tue 29 Nov 2022 11:25",
          "username": "\t\t\t\tGil80\t\t\t",
          "content": "C.  SQS meets this requirement.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 730067,
          "date": "Tue 29 Nov 2022 08:57",
          "username": "\t\t\t\tlearner2023\t\t\t",
          "content": "C is the right answer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 729875,
          "date": "Tue 29 Nov 2022 04:13",
          "username": "\t\t\t\tkvsomu\t\t\t",
          "content": "C is the answer",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 729869,
          "date": "Tue 29 Nov 2022 03:55",
          "username": "\t\t\t\tNigma\t\t\t",
          "content": "Answer : C",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 729790,
          "date": "Tue 29 Nov 2022 02:04",
          "username": "\t\t\t\tMee6\t\t\t",
          "content": "Answer: C due to SQS",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 729686,
          "date": "Mon 28 Nov 2022 23:53",
          "username": "\t\t\t\tTMM369\t\t\t",
          "content": "C - system to send messages to an Amazon Simple Queue Service (Amazon SQS)",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 729427,
          "date": "Mon 28 Nov 2022 17:33",
          "username": "\t\t\t\tGabs90\t\t\t",
          "content": "C - https://www.examtopics.com/discussions/amazon/view/81087-exam-aws-certified-solutions-architect-associate-saa-c02/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: C"
        }
      ]
    },
    {
      "question_id": "#196",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company runs an application on a large fleet of Amazon EC2 instances. The application reads and writes entries into an Amazon DynamoDB table. The size of the DynamoDB table continuously grows, but the application needs only data from the last 30 days. The company needs a solution that minimizes cost and development effort.<br><br>Which solution meets these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#196",
          "answers": [
            {
              "choice": "<p>A. Use an AWS CloudFormation template to deploy the complete solution. Redeploy the CloudFormation stack every 30 days, and delete the original stack.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use an EC2 instance that runs a monitoring application from AWS Marketplace. Configure the monitoring application to use Amazon DynamoDB Streams to store the timestamp when a new item is created in the table. Use a script that runs on the EC2 instance to delete items that have a timestamp that is older than 30 days.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Configure Amazon DynamoDB Streams to invoke an AWS Lambda function when a new item is created in the table. Configure the Lambda function to delete items in the table that are older than 30 days.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Extend the application to add an attribute that has a value of the current timestamp plus 30 days to each new item that is created in the table. Configure DynamoDB to use the attribute as the TTL attribute.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 733004,
          "date": "Thu 01 Dec 2022 20:18",
          "username": "\t\t\t\tGil80\t\t\t",
          "content": "changing my answer to D after researching a bit.<br><br>The DynamoDB TTL feature allows you to define a per-item timestamp to determine when an item is no longer needed. Shortly after the date and time of the specified timestamp, DynamoDB deletes the item from your table without consuming any write throughput.",
          "upvote_count": "18",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 779243,
          "date": "Tue 17 Jan 2023 20:11",
          "username": "\t\t\t\tLuckyAro\t\t\t",
          "content": "Amazon DynamoDB Time to Live (TTL) allows you to define a per-item timestamp to determine when an item is no longer needed. Shortly after the date and time of the specified timestamp, DynamoDB deletes the item from your table without consuming any write throughput. TTL is provided at no extra cost as a means to reduce stored data volumes by retaining only the items that remain current for your workload's needs.<br><br>TTL is useful if you store items that lose relevance after a specific time.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 772255,
          "date": "Wed 11 Jan 2023 09:55",
          "username": "\t\t\t\tDavidNamy\t\t\t",
          "content": "D: This solution is more efficient and cost-effective than alternatives that would require additional resources and maintenance.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 758940,
          "date": "Tue 27 Dec 2022 19:53",
          "username": "\t\t\t\tanonymouscloudguy\t\t\t",
          "content": "D DyanmoDB TTL will expire the items<br><br>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 754528,
          "date": "Fri 23 Dec 2022 21:42",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "To minimize cost and development effort, a solution that requires minimal changes to the existing application and infrastructure would be the most appropriate. Option D meets these requirements by using DynamoDB's Time-To-Live (TTL) feature, which allows you to specify an attribute on each item in a table that has a timestamp indicating when the item should expire.<br><br>In this solution, the application is extended to add an attribute that has a value of the current timestamp plus 30 days to each new item that is created in the table. DynamoDB is then configured to use this attribute as the TTL attribute, which causes items to be automatically deleted from the table when their TTL value is reached. This solution requires minimal changes to the existing application and infrastructure and does not require any additional resources or a complex setup.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Option A involves using AWS CloudFormation to redeploy the solution every 30 days, but this would require significant development effort and could cause downtime for the application. <br><br>Option B involves using an EC2 instance and a monitoring application to delete items that are older than 30 days, but this requires additional infrastructure and maintenance effort. <br><br>Option C involves using DynamoDB Streams and a Lambda function to delete items that are older than 30 days, but this requires additional infrastructure and maintenance effort.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 754529,
          "date": "Fri 23 Dec 2022 21:43",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "Option A involves using AWS CloudFormation to redeploy the solution every 30 days, but this would require significant development effort and could cause downtime for the application. <br><br>Option B involves using an EC2 instance and a monitoring application to delete items that are older than 30 days, but this requires additional infrastructure and maintenance effort. <br><br>Option C involves using DynamoDB Streams and a Lambda function to delete items that are older than 30 days, but this requires additional infrastructure and maintenance effort.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 754427,
          "date": "Fri 23 Dec 2022 19:45",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "TTL does the trick",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 753455,
          "date": "Thu 22 Dec 2022 16:59",
          "username": "\t\t\t\tkvenikoduru\t\t\t",
          "content": "Amazon DynamoDB Time to Live (TTL) allows you to define a per-item timestamp to determine when an item is no longer needed. Shortly after the date and time of the specified timestamp, DynamoDB deletes the item from your table without consuming any write throughput. -check this link https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 748729,
          "date": "Sun 18 Dec 2022 11:09",
          "username": "\t\t\t\tprethesh\t\t\t",
          "content": "https://aws.amazon.com/about-aws/whats-new/2017/02/amazon-dynamodb-now-supports-automatic-item-expiration-with-time-to-live-ttl/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 747860,
          "date": "Sat 17 Dec 2022 09:07",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option D - Right answer",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 746109,
          "date": "Thu 15 Dec 2022 14:22",
          "username": "\t\t\t\tBaba_Eni\t\t\t",
          "content": "DynamoDB has the TTL (Time to Live) functionality that gives you the option to set the duration you want a particular data to persist in the table.<br><br>https://aws.amazon.com/premiumsupport/knowledge-center/ttl-dynamodb/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 742581,
          "date": "Mon 12 Dec 2022 10:03",
          "username": "\t\t\t\tShasha1\t\t\t",
          "content": "C <br> Amazon DynamoDB Streams to invoke an AWS Lambda function when a new item is created in the table. The Lambda function can then be configured to delete items in the table that are older than 30 days. This solution minimizes cost and development effort because it uses existing AWS services and does not require any additional infrastructure or code development. <br>Option D is not correct for me, it is because,DynamoDB Time-to-Live (TTL) is not the most effective solution for minimizing cost and development effort. While DynamoDB TTL can be used to automatically delete items in a table after a certain amount of time, it requires manual configuration of the TTL attribute for each item in the table. This solution would require additional development effort to add the TTL attribute to the application, and it may not be feasible if the application is already running.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>This is inefficient:<br>The function would run every time an item was added, would generate costs each time it ran, and typically would not need to delete an item, since the first execution of the day would delete the items over 30 days old.<br>It would also require development effort to create the lambda function.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 771108,
          "date": "Tue 10 Jan 2023 07:11",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "This is inefficient:<br>The function would run every time an item was added, would generate costs each time it ran, and typically would not need to delete an item, since the first execution of the day would delete the items over 30 days old.<br>It would also require development effort to create the lambda function.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 741966,
          "date": "Sun 11 Dec 2022 19:11",
          "username": "\t\t\t\tbmofo\t\t\t",
          "content": "\\\"AWS Lambda is charging its users by the number of requests for their functions and by the duration, which is the time the code needs to execute.\\\" As the questions notes \\\"A LARGE FLEET OF EC2\\\", could rack up lots of money from using lambda calls to delete from tables. TTL is \\\"FREE\\\" to use and it also removes data from the table. so \\\"D\\\" would be the best solution.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 741279,
          "date": "Sat 10 Dec 2022 23:30",
          "username": "\t\t\t\tUhrien\t\t\t",
          "content": "This answer seems to be D. <br>https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 737822,
          "date": "Wed 07 Dec 2022 13:04",
          "username": "\t\t\t\tjavitech83\t\t\t",
          "content": "D is correct. For C I think developing a lambda has more effort than including an attribute, that would be 2 lines code. And of course cheaper than invoking a lambda for each single entry, which has no sense.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 737233,
          "date": "Tue 06 Dec 2022 22:35",
          "username": "\t\t\t\temohar01\t\t\t",
          "content": "\\\"Amazon DynamoDB Time to Live (TTL) allows you to define a per-item timestamp to determine when an item is no longer needed. Shortly after the date and time of the specified timestamp, DynamoDB deletes the item from your table without consuming any write throughput. TTL is provided at no extra cost as a means to reduce stored data volumes by retaining only the items that remain current for your workload's needs.\\\"",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 736872,
          "date": "Tue 06 Dec 2022 14:55",
          "username": "\t\t\t\tinvaderfr\t\t\t",
          "content": "C because even if TTL should be ok, the goal is to reduce cost, so if you reduce DB size you'll reduce the cost.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>the goal is also to \\\"minimize development effort\\\" and lambda functions are development effort. So it's D. </li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 761565,
          "date": "Fri 30 Dec 2022 00:50",
          "username": "\t\t\t\tFNJ1111\t\t\t",
          "content": "the goal is also to \\\"minimize development effort\\\" and lambda functions are development effort. So it's D. ",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 731989,
          "date": "Wed 30 Nov 2022 22:13",
          "username": "\t\t\t\tmj98\t\t\t",
          "content": "Ans is C",
          "upvote_count": "1",
          "selected_answers": ""
        }
      ]
    },
    {
      "question_id": "#197",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company has a Microsoft .NET application that runs on an on-premises Windows Server. The application stores data by using an Oracle Database Standard Edition server. The company is planning a migration to AWS and wants to minimize development changes while moving the application. The AWS application environment should be highly available.<br><br>Which combination of actions should the company take to meet these requirements? (Choose two.)<br><br></p>",
      "mark": 1,
      "is_partially_correct": true,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: BE</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#197",
          "answers": [
            {
              "choice": "<p>A. Refactor the application as serverless with AWS Lambda functions running .NET Core.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Rehost the application in AWS Elastic Beanstalk with the .NET platform in a Multi-AZ deployment.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Replatform the application to run on Amazon EC2 with the Amazon Linux Amazon Machine Image (AMI).<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use AWS Database Migration Service (AWS DMS) to migrate from the Oracle database to Amazon DynamoDB in a Multi-AZ deployment.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>E. Use AWS Database Migration Service (AWS DMS) to migrate from the Oracle database to Oracle on Amazon RDS in a Multi-AZ deployment.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 772259,
          "date": "Wed 11 Jan 2023 10:00",
          "username": "\t\t\t\tDavidNamy\t\t\t",
          "content": "B.  Rehost the application in AWS Elastic Beanstalk with the .NET platform in a Multi-AZ deployment.E.  Use AWS Database Migration Service (AWS DMS) to migrate from the Oracle database to Oracle on Amazon RDS in a Multi-AZ deployment.<br><br>Rehosting the application in Elastic Beanstalk with the .NET platform can minimize development changes. Multi-AZ deployment of Elastic Beanstalk will increase the availability of application, so it meets the requirement of high availability.<br><br>Using AWS Database Migration Service (DMS) to migrate the database to Amazon RDS Oracle will ensure compatibility, so the application can continue to use the same database technology, and the development team can use their existing skills. It also migrates to a managed service, which will handle the availability, so the team do not have to worry about it. Multi-AZ deployment will increase the availability of the database.",
          "upvote_count": "6",
          "selected_answers": "Selected Answer: BE"
        },
        {
          "id": 850687,
          "date": "Sun 26 Mar 2023 04:54",
          "username": "\t\t\t\tk33\t\t\t",
          "content": "Answer : BE",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BE"
        },
        {
          "id": 763662,
          "date": "Mon 02 Jan 2023 12:20",
          "username": "\t\t\t\twaiyiu9981\t\t\t",
          "content": "Why A is wrong?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Because that needs some development,</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 763987,
          "date": "Mon 02 Jan 2023 21:57",
          "username": "\t\t\t\tgustavtd\t\t\t",
          "content": "Because that needs some development,",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 754518,
          "date": "Fri 23 Dec 2022 21:25",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "B.  Rehost the application in AWS Elastic Beanstalk with the .NET platform in a Multi-AZ deployment.E.  Use AWS Database Migration Service (AWS DMS) to migrate from the Oracle database to Oracle on Amazon RDS in a Multi-AZ deployment.<br><br>To minimize development changes while moving the application to AWS and to ensure a high level of availability, the company can rehost the application in AWS Elastic Beanstalk with the .NET platform in a Multi-AZ deployment. This will allow the application to run in a highly available environment without requiring any changes to the application code.<br><br>The company can also use AWS Database Migration Service (AWS DMS) to migrate the Oracle database to Oracle on Amazon RDS in a Multi-AZ deployment. This will allow the company to maintain the existing database platform while still achieving a high level of availability.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: BE"
        },
        {
          "id": 754418,
          "date": "Fri 23 Dec 2022 19:28",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "B&E Option ,because D is for No-Sql<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>And requires additional development effort</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BE"
        },
        {
          "id": 771111,
          "date": "Tue 10 Jan 2023 07:18",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "And requires additional development effort",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 747861,
          "date": "Sat 17 Dec 2022 09:10",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "B&E Option",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 732200,
          "date": "Thu 01 Dec 2022 04:08",
          "username": "\t\t\t\tdcyberguy\t\t\t",
          "content": "B- According to the AWS documentation, the simplest way to migrate .NET applications to AWS is to repost the applications using either AWS Elastic Beanstalk or Amazon EC2.<br>E - RDS with Oracle is a no-brainer",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 730772,
          "date": "Tue 29 Nov 2022 20:18",
          "username": "\t\t\t\towenrooney11\t\t\t",
          "content": "same as everyone else",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: BE"
        },
        {
          "id": 729928,
          "date": "Tue 29 Nov 2022 06:12",
          "username": "\t\t\t\tKADSM\t\t\t",
          "content": "B E should be correct. Question says \\\"Minimize development changes\\\" - so should go for same oracle DB",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 729788,
          "date": "Tue 29 Nov 2022 02:01",
          "username": "\t\t\t\tMee6\t\t\t",
          "content": "B for Minimal Development(Elastic BeanStalk)<br>E for RDS with Oracle",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BE"
        },
        {
          "id": 729429,
          "date": "Mon 28 Nov 2022 17:37",
          "username": "\t\t\t\tGabs90\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/67840-exam-aws-certified-solutions-architect-associate-saa-c02/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: BE"
        },
        {
          "id": 729350,
          "date": "Mon 28 Nov 2022 16:56",
          "username": "\t\t\t\tleonnnn\t\t\t",
          "content": "B E is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BE"
        },
        {
          "id": 729200,
          "date": "Mon 28 Nov 2022 15:10",
          "username": "\t\t\t\tNigma\t\t\t",
          "content": "B and E <br>Oracle to RDS",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: BE"
        },
        {
          "id": 729090,
          "date": "Mon 28 Nov 2022 13:42",
          "username": "\t\t\t\tasthman\t\t\t",
          "content": "migrate to oracle on RDS is easy compare DynamoDB",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: BE"
        }
      ]
    },
    {
      "question_id": "#198",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company runs a containerized application on a Kubernetes cluster in an on-premises data center. The company is using a MongoDB database for data storage. The company wants to migrate some of these environments to AWS, but no code changes or deployment method changes are possible at this time. The company needs a solution that minimizes operational overhead.<br><br>Which solution meets these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#198",
          "answers": [
            {
              "choice": "<p>A. Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 worker nodes for compute and MongoDB on EC2 for data storage.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate for compute and Amazon DynamoDB for data storage<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use Amazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 worker nodes for compute and Amazon DynamoDB for data storage.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate for compute and Amazon DocumentDB (with MongoDB compatibility) for data storage.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 743697,
          "date": "Tue 13 Dec 2022 08:13",
          "username": "\t\t\t\tMarge_Simpson\t\t\t",
          "content": "If you see MongoDB, just go ahead and look for the answer that says DocumentDB. ",
          "upvote_count": "9",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 754508,
          "date": "Fri 23 Dec 2022 21:21",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "To minimize operational overhead and avoid making any code or deployment method changes, the company can use Amazon Elastic Kubernetes Service (EKS) with AWS Fargate for computing and Amazon DocumentDB (with MongoDB compatibility) for data storage. This solution allows the company to run the containerized application on EKS without having to manage the underlying infrastructure or make any changes to the application code.<br><br>AWS Fargate is a fully-managed container execution environment that allows you to run containerized applications without the need to manage the underlying EC2 instances.<br><br>Amazon DocumentDB is a fully-managed document database service that supports MongoDB workloads, allowing the company to use the same database platform as in their on-premises environment without having to make any code changes.",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 754411,
          "date": "Fri 23 Dec 2022 19:17",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "Reason A &B Elimnated as its Kubernates<br>why D read here https://containersonaws.com/introduction/ec2-or-aws-fargate/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 747863,
          "date": "Sat 17 Dec 2022 09:11",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option D",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 732195,
          "date": "Thu 01 Dec 2022 03:58",
          "username": "\t\t\t\tdcyberguy\t\t\t",
          "content": "DDDDDDD",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 729430,
          "date": "Mon 28 Nov 2022 17:39",
          "username": "\t\t\t\tGabs90\t\t\t",
          "content": "https://www.examtopics.com/discussions/amazon/view/67897-exam-aws-certified-solutions-architect-associate-saa-c02/",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 729351,
          "date": "Mon 28 Nov 2022 16:57",
          "username": "\t\t\t\tleonnnn\t\t\t",
          "content": "D meets the requirements",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 729203,
          "date": "Mon 28 Nov 2022 15:14",
          "username": "\t\t\t\tNigma\t\t\t",
          "content": "D<br>EKS because of Kubernetes so A and B are eliminated<br>not C because of MongoDB and Fargate is more expensive",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        }
      ]
    },
    {
      "question_id": "#199",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A telemarketing company is designing its customer call center functionality on AWS. The company needs a solution that provides multiple speaker recognition and generates transcript files. The company wants to query the transcript files to analyze the business patterns. The transcript files must be stored for 7 years for auditing purposes.<br><br>Which solution will meet these requirements?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: B</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#199",
          "answers": [
            {
              "choice": "<p>A. Use Amazon Rekognition for multiple speaker recognition. Store the transcript files in Amazon S3. Use machine learning models for transcript file analysis.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. Use Amazon Transcribe for multiple speaker recognition. Use Amazon Athena for transcript file analysis.<br></p>",
              "correct": true,
              "feedback": ""
            },
            {
              "choice": "<p>C. Use Amazon Translate for multiple speaker recognition. Store the transcript files in Amazon Redshift. Use SQL queries for transcript file analysis.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Use Amazon Rekognition for multiple speaker recognition. Store the transcript files in Amazon S3. Use Amazon Textract for transcript file analysis.<br></p>",
              "correct": false,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 754495,
          "date": "Fri 23 Dec 2022 21:10",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "The correct answer is B: Use Amazon Transcribe for multiple speaker recognition. Use Amazon Athena for transcript file analysis.<br><br>Amazon Transcribe is a service that automatically transcribes spoken language into written text. It can handle multiple speakers and can generate transcript files in real-time or asynchronously. These transcript files can be stored in Amazon S3 for long-term storage.<br><br>Amazon Athena is a query service that allows you to analyze data stored in Amazon S3 using SQL. You can use it to analyze the transcript files and identify patterns in the data.<br><br>Option A is incorrect because Amazon Rekognition is a service for analyzing images and videos, not transcribing spoken language.<br><br>Option C is incorrect because Amazon Translate is a service for translating text from one language to another, not transcribing spoken language.<br><br>Option D is incorrect because Amazon Textract is a service for extracting text and data from documents and images, not transcribing spoken language.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>The correct answer is C. <br>https://docs.aws.amazon.com/transcribe/latest/dg/what-is.html<br>You can transcribe streaming media in real time or you can upload and transcribe media files. To see which languages are supported for each type of transcription, refer to the Supported languages and language-specific features table.</li><li>Disregard.I meant B</li><li>https://aws.amazon.com/about-aws/whats-new/2022/06/amazon-transcribe-supports-automatic-language-identification-multi-lingual-audio/<br>Amazon Translate is a service for multi-language identification, which identifies all languages spoken in the audio file and creates transcript using each identified language.</li><li>Disregard.I meant Amazon Transcribe</li></ul>",
          "upvote_count": "9",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 786646,
          "date": "Tue 24 Jan 2023 16:28",
          "username": "\t\t\t\tenzomv\t\t\t",
          "content": "The correct answer is C. <br>https://docs.aws.amazon.com/transcribe/latest/dg/what-is.html<br>You can transcribe streaming media in real time or you can upload and transcribe media files. To see which languages are supported for each type of transcription, refer to the Supported languages and language-specific features table.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Disregard.I meant B</li><li>https://aws.amazon.com/about-aws/whats-new/2022/06/amazon-transcribe-supports-automatic-language-identification-multi-lingual-audio/<br>Amazon Translate is a service for multi-language identification, which identifies all languages spoken in the audio file and creates transcript using each identified language.</li><li>Disregard.I meant Amazon Transcribe</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 788548,
          "date": "Thu 26 Jan 2023 09:53",
          "username": "\t\t\t\tenzomv\t\t\t",
          "content": "Disregard.I meant B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 788541,
          "date": "Thu 26 Jan 2023 09:46",
          "username": "\t\t\t\tenzomv\t\t\t",
          "content": "https://aws.amazon.com/about-aws/whats-new/2022/06/amazon-transcribe-supports-automatic-language-identification-multi-lingual-audio/<br>Amazon Translate is a service for multi-language identification, which identifies all languages spoken in the audio file and creates transcript using each identified language.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Disregard.I meant Amazon Transcribe</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 788552,
          "date": "Thu 26 Jan 2023 09:54",
          "username": "\t\t\t\tenzomv\t\t\t",
          "content": "Disregard.I meant Amazon Transcribe",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 850688,
          "date": "Sun 26 Mar 2023 04:55",
          "username": "\t\t\t\tk33\t\t\t",
          "content": "Answer : B",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 786651,
          "date": "Tue 24 Jan 2023 16:31",
          "username": "\t\t\t\tenzomv\t\t\t",
          "content": "https://docs.aws.amazon.com/transcribe/latest/dg/what-is.html",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: C"
        },
        {
          "id": 768288,
          "date": "Sat 07 Jan 2023 08:04",
          "username": "\t\t\t\tmaster1004\t\t\t",
          "content": "The correct answer is C. <br>Wouldn't it be the right answer to save and analyze using Amazon Redshift, which can be used to analyze big data on data warhousing?",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 755195,
          "date": "Sat 24 Dec 2022 23:15",
          "username": "\t\t\t\tChirantan\t\t\t",
          "content": "B<br><br>https://aws.amazon.com/transcribe/<br>Amazon Transcribe<br>Automatically convert speech to text",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 753795,
          "date": "Fri 23 Dec 2022 02:45",
          "username": "\t\t\t\ttechhb\t\t\t",
          "content": "Only B<br>ashttps://www.examtopics.com/exams/amazon/aws-certified-solutions-architect-associate-saa-c03/view/7/#<br>Rekognition - Image and Video Analysis<br>Transcribe - Text to speech<br>Translate - Translate a text-based file from a language to another language",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 753446,
          "date": "Thu 22 Dec 2022 16:51",
          "username": "\t\t\t\tkvenikoduru\t\t\t",
          "content": "Rekognition - Image and Video Analysis<br>Transcribe - Text to speech <br>Translate - Translate a text based file from a language to another language<br>So by logical deduction is it B",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 748242,
          "date": "Sat 17 Dec 2022 18:35",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "B is the right answer. You can specify the S3 bucket with transcribe to store the data for 7 years and use Athena for Analytics later. Transcribe also supports Multiple speaker recognition.",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 733483,
          "date": "Fri 02 Dec 2022 07:08",
          "username": "\t\t\t\tjustsaysid\t\t\t",
          "content": "Answer is B - pretty straightforward.",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 732194,
          "date": "Thu 01 Dec 2022 03:55",
          "username": "\t\t\t\tdcyberguy\t\t\t",
          "content": "Answer is B. ",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 732082,
          "date": "Thu 01 Dec 2022 00:18",
          "username": "\t\t\t\tTelaO\t\t\t",
          "content": "Why is it not C?<br>\\\"Amazon Translate is a text translation service that uses advanced machine learning technologies to provide high-quality translation on demand. You can use Amazon Translate to translate unstructured text documents or to build applications that work in multiple languages.\\\"<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Disregard.I meant B</li></ul>",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 732083,
          "date": "Thu 01 Dec 2022 00:18",
          "username": "\t\t\t\tTelaO\t\t\t",
          "content": "Disregard.I meant B",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 731045,
          "date": "Wed 30 Nov 2022 04:22",
          "username": "\t\t\t\tkmaneith\t\t\t",
          "content": "Why it is B instead of C? The question didn't mention to use S3 to store the data, so it cannot be athena ?<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>\\\"The transcript files must be stored for 7 years for auditing purposes\\\" which implied S3 storage. C is text translation (text from language 1 to language 2), you are asked for audio transcription (audio to text), which are completely different things.</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 771117,
          "date": "Tue 10 Jan 2023 07:36",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "\\\"The transcript files must be stored for 7 years for auditing purposes\\\" which implied S3 storage. C is text translation (text from language 1 to language 2), you are asked for audio transcription (audio to text), which are completely different things.",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 730999,
          "date": "Wed 30 Nov 2022 02:38",
          "username": "\t\t\t\tTonyghostR05\t\t\t",
          "content": "B Transcribe",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 730774,
          "date": "Tue 29 Nov 2022 20:21",
          "username": "\t\t\t\towenrooney11\t\t\t",
          "content": "Amazon Transcribe now supports speaker labeling for streaming transcription. Amazon Transcribe is an automatic speech recognition (ASR) service that makes it easy for you to convert speech-to-text. In live audio transcription, each stream of audio may contain multiple speakers. Now you can conveniently turn on the ability to label speakers, thus helping to identify who is saying what in the output transcript.<br><br>https://aws.amazon.com/about-aws/whats-new/2020/08/amazon-transcribe-supports-speaker-labeling-streaming-transcription/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 730711,
          "date": "Tue 29 Nov 2022 19:15",
          "username": "\t\t\t\tManlikeleke\t\t\t",
          "content": "It cannot be B because it leaves out the storage part of the question.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>D identifies images and video, so completely irrelevant</li><li>\\\"Use Amazon Athena for transcript file analysis\\\" -&gt; this implies that the data has to reside on S3 so it does take care of the storage question as well.</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 771118,
          "date": "Tue 10 Jan 2023 07:37",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "D identifies images and video, so completely irrelevant",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 731983,
          "date": "Wed 30 Nov 2022 22:00",
          "username": "\t\t\t\ttdkcumberland\t\t\t",
          "content": "\\\"Use Amazon Athena for transcript file analysis\\\" -> this implies that the data has to reside on S3 so it does take care of the storage question as well.",
          "upvote_count": "3",
          "selected_answers": ""
        },
        {
          "id": 730419,
          "date": "Tue 29 Nov 2022 14:13",
          "username": "\t\t\t\tHeyang\t\t\t",
          "content": "Amazon transcribe convert speech to text and Athena for query",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: B"
        },
        {
          "id": 730411,
          "date": "Tue 29 Nov 2022 14:05",
          "username": "\t\t\t\tGil80\t\t\t",
          "content": "Cannot be Rekognition, because it's for:<br>  Find objects, people, text, scenes in images and videos using ML<br>  Facial analysis and facial search to do user verification, people counting<br>  Create a database of \\\"familiar faces\\\" or compare against celebrities<br><br>Transcribe is for:<br>  Automatically convert speech to text<br>  Uses a deep learning process called automatic speech recognition (ASR) to convert speech to text quickly and accurately<br>  Automatically remove PII using reduction<br>  Use cases:<br> Transcribe customer service calls<br> Automate closed captioning and subtitling<br> Generate metadata for media assets to create a fully searchable archive",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: B"
        }
      ]
    },
    {
      "question_id": "#200",
      "topic_id": 1,
      "course_id": 1,
      "case_study_id": null,
      "lab_id": 0,
      "question_text": "<p>A company hosts its application on AWS. The company uses Amazon Cognito to manage users. When users log in to the application, the application fetches required data from Amazon DynamoDB by using a REST API that is hosted in Amazon API Gateway. The company wants an AWS managed solution that will control access to the REST API to reduce development efforts.<br><br>Which solution will meet these requirements with the LEAST operational overhead?<br><br></p>",
      "mark": 1,
      "is_partially_correct": false,
      "question_type": "1",
      "difficulty_level": "0",
      "general_feedback": "<p>Correct Answer: D</p>",
      "is_active": true,
      "answer_list": [
        {
          "question_answer_id": 1,
          "question_id": "#200",
          "answers": [
            {
              "choice": "<p>A. Configure an AWS Lambda function to be an authorizer in API Gateway to validate which user made the request.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>B. For each user, create and assign an API key that must be sent with each request. Validate the key by using an AWS Lambda function.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>C. Send the user's email address in the header with every request. Invoke an AWS Lambda function to validate that the user with that email address has proper access.<br></p>",
              "correct": false,
              "feedback": ""
            },
            {
              "choice": "<p>D. Configure an Amazon Cognito user pool authorizer in API Gateway to allow Amazon Cognito to validate each request.<br></p>",
              "correct": true,
              "feedback": ""
            }
          ]
        }
      ],
      "topic_name": "",
      "discusstion": [
        {
          "id": 850689,
          "date": "Sun 26 Mar 2023 04:55",
          "username": "\t\t\t\tk33\t\t\t",
          "content": "Answer : D",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 847131,
          "date": "Wed 22 Mar 2023 15:03",
          "username": "\t\t\t\tHello4me\t\t\t",
          "content": "D is correct",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 770673,
          "date": "Mon 09 Jan 2023 18:07",
          "username": "\t\t\t\tMahadeva\t\t\t",
          "content": "There is a difference between \\\"Grant Access\\\" (Authentication done by Cognito user pool), and \\\"Control Access\\\" to APIs (Authorization using IAM policy, custom Authorizer, Federated Identity Pool). The question very specifically asks about *Control access to REST APIs* which is a clear case of Authorization and not Authentication. So custom Authorizer using Lambda in API Gateway is the solution.<br><br>Pls refer to this blog: https://aws.amazon.com/blogs/security/building-fine-grained-authorization-using-amazon-cognito-api-gateway-and-iam/<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>This answer looks to be entirely wrong <br><br>This article explains how to do what you claim cannot be done: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html<br><br>It starts \\\"As an alternative to using IAM roles and policies or Lambda authorizers (formerly known as custom authorizers), you can use an Amazon Cognito user pool to control who can access your API in Amazon API Gateway.\\\"<br><br>This suggests that Amazon Cognito user pools CAN be used for Authorization, which you say above cannot be done.<br><br>Further, it explains how to do this...<br><br>\\\"To use an Amazon Cognito user pool with your API, you must first create an authorizer of the COGNITO_USER_POOLS type and then configure an API method to use that authorizer\\\"<br><br>So whilst A is a valid approach, it looks like D achieves the same with \\\"the LEAST operational overhead\\\".</li><li>Control access to a REST API using Amazon Cognito user pools as authorizer<br>https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html</li><li>Option D: there is nothing called Cognito user pool authorizer. We only have custom Authorizer function through Lambda.</li><li>Oh yes there is :)</li></ul>",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: A"
        },
        {
          "id": 771125,
          "date": "Tue 10 Jan 2023 07:45",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "This answer looks to be entirely wrong <br><br>This article explains how to do what you claim cannot be done: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html<br><br>It starts \\\"As an alternative to using IAM roles and policies or Lambda authorizers (formerly known as custom authorizers), you can use an Amazon Cognito user pool to control who can access your API in Amazon API Gateway.\\\"<br><br>This suggests that Amazon Cognito user pools CAN be used for Authorization, which you say above cannot be done.<br><br>Further, it explains how to do this...<br><br>\\\"To use an Amazon Cognito user pool with your API, you must first create an authorizer of the COGNITO_USER_POOLS type and then configure an API method to use that authorizer\\\"<br><br>So whilst A is a valid approach, it looks like D achieves the same with \\\"the LEAST operational overhead\\\".",
          "upvote_count": "5",
          "selected_answers": ""
        },
        {
          "id": 832036,
          "date": "Tue 07 Mar 2023 16:00",
          "username": "\t\t\t\tTungPham\t\t\t",
          "content": "Control access to a REST API using Amazon Cognito user pools as authorizer<br>https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 770678,
          "date": "Mon 09 Jan 2023 18:13",
          "username": "\t\t\t\tMahadeva\t\t\t",
          "content": "Option D: there is nothing called Cognito user pool authorizer. We only have custom Authorizer function through Lambda.<br> <div> Replies:</div> <ul style='list-style-type: disclosure-closed;'><li>Oh yes there is :)</li></ul>",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 771126,
          "date": "Tue 10 Jan 2023 07:45",
          "username": "\t\t\t\tJayBee65\t\t\t",
          "content": "Oh yes there is :)",
          "upvote_count": "2",
          "selected_answers": ""
        },
        {
          "id": 755661,
          "date": "Sun 25 Dec 2022 14:14",
          "username": "\t\t\t\tk1kavi1\t\t\t",
          "content": "https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html",
          "upvote_count": "3",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 754489,
          "date": "Fri 23 Dec 2022 21:02",
          "username": "\t\t\t\tBuruguduystunstugudunstuy\t\t\t",
          "content": "KEYWORD: LEAST operational overhead<br><br>To control access to the REST API and reduce development efforts, the company can use an Amazon Cognito user pool authorizer in API Gateway. This will allow Amazon Cognito to validate each request and ensure that only authenticated users can access the API. This solution has the LEAST operational overhead, as it does not require the company to develop and maintain any additional infrastructure or code.<br><br>Therefore, Option D is the correct answer.<br><br>Option D.  Configure an Amazon Cognito user pool authorizer in API Gateway to allow Amazon Cognito to validate each request.",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 747745,
          "date": "Sat 17 Dec 2022 04:50",
          "username": "\t\t\t\tcareer360guru\t\t\t",
          "content": "Option D - As company already has all the users authentication information in Cognito",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 746213,
          "date": "Thu 15 Dec 2022 15:51",
          "username": "\t\t\t\tk1kavi1\t\t\t",
          "content": "D is correct",
          "upvote_count": "2",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 731990,
          "date": "Wed 30 Nov 2022 22:20",
          "username": "\t\t\t\tmj98\t\t\t",
          "content": "API + Cognito integration - Answer D",
          "upvote_count": "1",
          "selected_answers": ""
        },
        {
          "id": 729877,
          "date": "Tue 29 Nov 2022 04:17",
          "username": "\t\t\t\tNigma\t\t\t",
          "content": "Answer : D<br>Check Gabs90 link<br><br>Use the Amazon Cognito console, CLI/SDK, or API to create a user poolor use one that's owned by another AWS account",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 729694,
          "date": "Tue 29 Nov 2022 00:05",
          "username": "\t\t\t\tTMM369\t\t\t",
          "content": "D - https://aws.amazon.com/premiumsupport/knowledge-center/api-gateway-cognito-user-pool-authorizer/",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 729435,
          "date": "Mon 28 Nov 2022 17:45",
          "username": "\t\t\t\tGabs90\t\t\t",
          "content": "seems to be D to me:https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html",
          "upvote_count": "4",
          "selected_answers": "Selected Answer: D"
        },
        {
          "id": 729355,
          "date": "Mon 28 Nov 2022 17:05",
          "username": "\t\t\t\tleonnnn\t\t\t",
          "content": "D is correct",
          "upvote_count": "1",
          "selected_answers": "Selected Answer: D"
        }
      ]
    }
  ]
}